\newchap{Methodology}
\label{chap:3_method}
In this chapter, I will introduce the composite model of TrOCR and CharBERT by leveraging the ideas in Candidate Fusion mentioned in \autoref{chap:2_related_work}. First, the design concept behind the model will be outlined. Following this, I will elaborate on data collection and preprocessing, and the architectures of the models developed in this study. Finally, I will delve into the details of the training process, including the utilisation of GPU resources, the optimiser and the loss function.
\TODO{Rewrite this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                       Design Concept                                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design Concept}
\label{sec:3_design_concept}
CharBERT mitigates the problems of incomplete modelling and fragile representation by including the character encoding in addition to the subword level information. Furthermore, having NLM as the pre-training task makes CharBERT effective at correcting character level typos, which is a desired feature for OCR correcting. 

On the other hand, Candidate Fusion claims that having an interaction between the recogniser and the LM can enhance the performance of OCR. Thus, combining TrOCR and CharBERT is expected have an improvement on the OCR accuracy. 
\TODO{Rewrite this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     Model Architecture                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Architecture}
\label{sec:3_composite_model_architecture}
The end-to-end composite model is composed of TrOCR and CharBERT. TrOCR does the text recognition task, while CharBERT serves as the corrector. The decoder input of TrOCR will go through CharBERT for correction before entering the decoder of TrOCR. By doing this, the TrOCR decoder will decode the input embedding corrected by CharBERT. In this section, I will first elaborate on the architectures of TrOCR and CharBERT, and how I fuse them together.

\subsection{Notations}
\label{subsec:3_Notations}
In this report, we denote a one-dimensional tensor (vector) using boldface lowercase letters, while a multi-dimensional tensor appears as boldface uppercase letters. The TrOCR sequence IDs are denoted by $\mathbf{W}$ = \{$\mathbf{w}_1, ..., \mathbf{w}_i, ..., \mathbf{w}_D$\} where each $\mathbf{w}_i$ stands for a token tokenised by TrOCR tokeniser and $D$ signifies the length of the sequence. 

\subsection{Recogniser - TrOCR}
\label{subsec:3_recogniser_trocr}
TrOCR is composed of an image encoder and a text decoder. The image encoder takes pixel values of an image as the input, and the encoder output will then be fed into the decoder for text generation. This study involves adapting the decoder to integrate with CharBERT.  Therefore, it will be crucial to focus on and provide a detailed explanation of its architecture. The following contents will first focus on the inputs of the decoder and the label that is used for loss calculation. Then, I will elaborate on how the inputs go through the decoder.

\paragraph*{Decoder Inputs and Labels}
\label{par:3_decoder_inputs_and_labels}
The text decoder takes encoder output, decoder input ids and padding attention mask\myfootnote{Note that the padding attention mask should not be confused with the casual attention mask. The padding attention mask prevents the model from attending to the paddings, while the casual attention mask ensures that the prediction for a specific token in the sequence can only be influenced by previously generated tokens.} as input, and output the generated text. The decoder output will be compared to the label tensor for loss calculation. The loss will then be used for updating the gradients of the parameters during training.

The encoder output is a set of features extracted from the pixel values. These features are the transformed representation of the input image, not only capturing the patterns within the image, but also understanding the arrangements and relations of each elements in the image.

The encoder's output represents a set of high-level features extracted from the input image. These features are essentially a transformed representation of the input image, capturing various aspects and patterns that are relevant for recognizing the text within the image.

Decoder input ids is a tensor of token ids converted from the label texts. The token ids includes three special tokens, which are $<$bos$>$, $<$eos$>$ and padding. The id, $<$bos$>$, is represented by the number 0 and indicates the start of the text. The id, $<$eos$>$, is denoted by the number 2 and represents the end of the text. As for padding, it is a series of number 1 appended after <eos> to make sure that all the input ids have the same length. The decoder input ids can be generated automatically by the TrOCR tokeniser. By default, the generated decoder input ids have $<$bos$>$ in the beginning, and $<$eos$>$, paddings at the end. This can be used during training to simulate the process of inference\myfootnote{During inference, the decoder output from the previous step will be the new decoder input. However, the decoder has no other context at the first step since no output sequence has been generated yet. Thus, the very first step of the decoder input is just this $<$bos$>$ token. The decoder processes this initial input and generates a decoder outpu, which is being used to update the decoder input.}. 

The other input of the decoder, padding attention mask, controls the information that should be ignored by TrOCR. The padding attention mask consists of 0s and 1s. The 0s covers the padding parts of the decoder input ids to prevent the model from focusing on them.

The label tensor for calculating the loss is decoder input ids without the $<$bos$>$ token. And since the TrOCR model is inherited from VisionEncoderDecoderModel, the class requires the label paddings to be -100. Hence, the padding token of the label tensor will be replaced by -100 instead of the 1s.

\paragraph*{Inside the Decoder}
\label{par:3_inside_the_decoder}
Let's first talk about the original decoder before adapting it to fit CharBERT. The decoder receives the decoder input ids, its padding attention mask and the encoder output. First, the decoder input ids converted into decoder embeddings. Next, the decoder embeddings is added with the positional encoding to help keep the spatial information before going through decoder stacks, including masked multi-head attention, multi-head attention and the FFNN. The padding attention mask will be applied to both masked multi-head attention and multi-head attention to prevent TrOCR from attending the padding tokens. Finally, the output of the decoder stacks will go through a linear layer and a softmax layer to generate an output sequence. 

\subsection{Corrector - CharBERT}
\label{subsec:3_corrector_charbert}
CharBERT has a dual-channel architecture, which are the token and the character channels. The token channel has the same structure as BERT or RoBERTa, depending on the initialisation. In this study, we will focus on the CharBERT\textsubscript{RoBERTa}. From this point forward in the document, any mention of "CharBERT" will specifically refer to CharBERT\textsubscript{RoBERTa}, unless explicitly stated otherwise. Similar to BERT and RoBERTa, CharbERT takes texts as input, but instead of outputting a single embedding, CharBERT outputs a token level embeddings and a character level embeddings. Since the model architecture of the token channel is identical to BERT/RoBERTa , the subsequent contents will focus on the character channel and heterogeneous interaction.

\paragraph*{Character Channel \& Heterogeneous Interaction}
\label{par:3_character_channel_heterogeneous_interaction}
The character channel first splits the input text into characters, convert them into IDs by looking up the dictionary. Next, it embeds the IDs, and then apply the bidirectional GRU (Bi-GRU) layer to generate the output embeddings. The output embeddings from the character and token level will then go through the transformer and the heterogeneous interaction. The heterogeneous interaction is consists of two parts, fusion and divide. The fusion allows both embeddings to enrich each other by FFNN and CNN, while the divide part ensures that both embeddings keep their unique features by a FFNN and a residual connection. The residual connection helps retain the respective information from the two embeddings. CharBERT then repeat the transformer and heterogeneous interaction to capture more features and information underlying the input texts.

\subsection{Composite Model}
\label{subsec:3_composite_model}
The composite model is designed to combine TrOCR and CharBERT. The idea is that, during the inference phase, the decoder output is recycled as its input in a feedback loop. Before this recycled input is fed back into the decoder, it has to undergo correction and refinement by CharBERT. Thus, CharBERT will be integrated between the decoder input and the decoder stacks, ensuring that the input to the decoder is optimised on each iteration of the process. The problems with this integration are: 1) TrOCR decoder takes token IDs as input but CharBERT outputs are embeddings; 2) CharBERT takes texts as input but the TrOCR decoder input is a tensor; 3) TrOCR embedding representations do not match CharBERT embedding representations; 4) TrOCR decoder input is a single tensor but CharBERT has dual-channel outputs. The following contents will be discussing the details and approaches to address these two problems.

\paragraph*{Adapted TrOCR}
\label{par:3_adapted_trocr}
Before delving into the potential solution to the first problem, it is essential to thoroughly examine the issue. The TrOCR decoder is designed to take token IDs as input, which it then maps to embeddings. These embeddings are then added with the positional encoding before being passed into the Transformer decoder. However, the challenge arises due to the outputs from CharBERT after the correction being embeddings, not token IDs. This presents a compatibility issue. If we were to solve this problem by converting the CharBERT representations into token IDs before inputting TrOCR decoder, another issue emerges. Employing token IDs as intermediaries between model components is problematic, as token IDs are inherently integers. During the training process, the model weights are updated as floating-point numbers. Constraining these weights to integer values is not feasible. While rounding up the updated weights can technically convert floats to integers, but the underlying meaning of rounding it will be questionable. The rounding would likely distort the model's learning process and could be meaningless for the task. Therefore, the only reasonable way to solve this issue is to modify TrOCR decoder so that it takes embeddings as input. 

There is a straightforward solution. By repositioning the embedding layer from the TrOCR decoder to precede CharBERT, token IDs will be initially converted to TrOCR embeddings, which will then be input into CharBERT for correction. Consequently, the adapted TrOCR can now accept embeddings directly instead of token IDs. This modification ensures that the outputs from CharBERT integrate with the TrOCR decoder and that there will be no IDs between model components.

\paragraph*{Adapted CharBERT}
\label{par:3_adapted_charbert}
The second problem involves the mismatching data types of the input and the output. The TrOCR decoder input is an embedding according to the modification made in \hyperref[par:3_adapted_trocr]{Adapted TrOCR}. This embedding should be fed into CharBERT for correction. Unfortunately, CharBERT takes only texts as input. Thus, creating an adapted CharBERT model is necessary. The adapted CharBERT is modified so that it takes token and character embeddings as inputs. These two embeddings will then go through the token and character channels. To be specific, the adapted CharBERT will no longer take texts as input, convert them into IDs and embed them. The embeddings will be provided as the input directly.

\paragraph*{Tensor Transform}
\label{par:3_tensor_transform}
The third problem is more complex. The TrOCR decoder input is based on embeddings unique to TrOCR. Even for the same text, the embedding representations from TrOCR and CharBERT are markedly different, not to mention CharBERT has dual-channel embeddings. Not only do both models have different representations for the same text, the embedding dimensions are also incompatible. Therefore, even though CharBERT can take embeddings as inputs, the TrOCR decoder input still cannot be fed into CharBERT directly. To address this problem, an architecture consists of CNN and FFNN is employed. This approach serves to adjust the dimension of TrOCR decoder input to align with dimensions of CharBERT token and character embeddings. Additionally, this dimensional transformation allows the model to match the representations between TrOCR and CharBERT effectively. To better illustrate the architecture of the tensor transform, the dimensions will be noted after each input/output between brackets, e.g., (batch size, embedding size).

The goal of the tensor transform is to convert the TrOCR decoder input size (batch size, 512, 1024) into the CharBERT token embeddings size (batch size, 510, 768) and CharBERT character embeddings size (batch size, 3060, 256), where the second dimension is the sequence length and the third dimension is the embedding size. In this tensor transform architecture, the transformation is separated into two stages. In the first stage, the decoder input first goes through a series of CNN layers interspersed with LeakyReLU and batch normalisation steps to adjust the sequence dimension (dim=1). Then, for the second stage, the output from the first stage goes through FFNN layers with LeakyReLU in between to modify the embedding dimension (dim=2).
\begin{equation} \label{eq:3_tensor_transform_cnn}
    \begin{split}
        &\mathbf{t}_{1, j} = \text{LeakyReLU}(\mathbf{b}_1 + \sum_{k=1}^{3}\mathbf{W}_{1,k}\cdot \mathbf{e}_{i+k-1})\quad;\quad \mathbf{t}'_1 = \text{Batch\_Norm}(\mathbf{t}_1)\\
        &\mathbf{t}_{2, l} = \text{LeakyReLU}(\mathbf{b}_2 + \sum_{k=1}^{3}\mathbf{W}_{2,k}\cdot \mathbf{t}'_{1,j+k-1})\quad;\quad \mathbf{t}'_2 = \text{Batch\_Norm}(\mathbf{t}_2)\\
        &\mathbf{t}_{3, p} = \text{LeakyReLU}(\mathbf{b}_3 + \sum_{k=1}^{3}\mathbf{W}_{3,k}\cdot \mathbf{t}'_{2,l+k-1})\quad\\
    \end{split}
\end{equation}
After applying the CNN layers, the dimension of the decoder input becomes (batch size, 510, 1024). The sequence dimension (dim=1) has become the desired dimension for CharBERT token embedding. The purpose of the CNN layers is to either expand or contract the sequence length, offering the advantage of preserving spatial information. This is the primary reason for selecting CNN instead of FFNN for adjusting the sequence dimensions. Moreover, it is worth noting that there are batch normalisation in between the convolutional layers. The batch normalisations are there to stabilise the deep model and to help maintaining healthier gradients with the presense of the activation functions\footnote{With the presense of activation functions in deep networks, gradient can easily explode or vanish. Batch normalisation helps in maintaining a healthier gradient flow in the network, which can improve the efficiency of backpropagation and thence the learning process.}. Next, we enter the second stage of the tensor transformation, which utilise the FFNN layers.
\begin{equation} \label{eq:3_tensor_transform_ffnn}
    \begin{split}
        &\mathbf{T}_{4} = \text{LeakyReLU}(\mathbf{b}_4 + \mathbf{W}_4\cdot \mathbf{T}_{3})\\
        &\mathbf{T}_{5} = \text{LeakyReLU}(\mathbf{b}_5 + \mathbf{W}_5\cdot \mathbf{T}_{4})\\
        &\mathbf{T}_{n} = \text{LeakyReLU}(\mathbf{b}_6 + \mathbf{W}_6\cdot \mathbf{T}_{5})\\
    \end{split}
\end{equation}
The resulting tensor is the CharBERT token embedding $\mathbf{T} = \{\mathbf{t}_1, ..., \mathbf{t}_n, ..., \mathbf{t}_N\}$, where $N$ is the token sequence length with tensor size (batch size, 510, 768).

Same operations are applied to the TrOCR decoder input but with different dimension expansion to generate the CharBERT character embedding $\mathbf{C} = \{\mathbf{c}_1, ..., \mathbf{c}_m, ..., \mathbf{c}_M\}$, where $M$ is the character sequence length. It is established that the length $M$ is six times that of $N$, based on the assumption that the average word contains six characters.

The Tensor Transform layer aims to convert the TrOCR decoder input into CharBERT token embedding and character embedding. The CNN and FFNN layers not only match the dimensions between the tensors, but also learn to map the contextual information in TrOCR embedding to CharBERT embeddings.

\paragraph*{Tensor Combine}
\label{par:3_tensor_combine}
The fourth problem arises due to the fact that CharBERT produces two tensors - token and character representations, whereas TrOCR decoder input is a single tensor. Consequently, the solution would be to combine the two output tensors from CharBERT into a single tensor. In addition to the outputs from CharBERT, we can also add a residual connection from the original TrOCR decoder embedding to stabilise the deep model. The residual connection here can encourage the reuse of features from the original TrOCR decoder embedding and prevent gradient vanishing.

Among the three tensors, two of them comes from CharBERT. Therefore, the representation is not understood by TrOCR decoder and the dimensions do not match. The two of them should first undergo \hyperref[par:3_tensor_transform]{Tensor Transform} before the combination. 
\begin{equation} \label{eq:3_tensor_combine_transform}
    \begin{split}
        &\mathbf{T}\in \mathbb{R}^{d_N}\to \mathbf{T}' = \text{Tensor\_Transform}(\mathbf{T})\in \mathbb{R}^{d_D}\\
        &\mathbf{C}\in \mathbb{R}^{d_M}\to \mathbf{C}' = \text{Tensor\_Transform}(\mathbf{C})\in \mathbb{R}^{d_D}\\
    \end{split}
\end{equation}
where $d_D$ = (batch size, 512, 1024); $d_N$ = (batch size, 510, 768); $d_M$ = (batch size, 3060, 256).

After performing the tensor transform module, three tensors will all be of size (batch size, 512, 1024), which is the same as the original TrOCR decoder input size. They can then be combined and fed into the TrOCR decoder stack. In this study, we examine four tensor combine module architectures: 1) simply adding all of them; 2) mean pooling; 3) using linear layers as the attention net; 4) using convolutional layers as the attention net.

\subparagraph*{Tensor Combine 1: Adding}
\label{subpar:3_adding}
The first tensor transform module is simply aggregating three tensors into a single tensor, operates by performing an element-wise addition of the three input tensors. Given the TrOCR decoder embeddings $\mathbf{E}$, the CharBERT transformed token representation $\mathbf{T'}$ and the CharBERT transformed character representation $\mathbf{C'}$, each contributing equally to the formation of the combined tensor (each input tensor's information is weighted equally), the operation can be mathematically expressed as:
\begin{equation} \label{eq:3_tensor_combine_adding}
    \begin{split}
        \mathbf{E}'_{1, ijk} = \mathbf{E}_{ijk} + \mathbf{T}'_{ijk} + \mathbf{C}'_{ijk}
    \end{split}
\end{equation}
where $i = 1, 2, ..., \text{batch size}; j = 1, 2, ..., 512; k = 1, 2, ..., 1024$.
\subparagraph*{Tensor Combine 2: Mean Pooling}
\label{subpar:3_mean_pooling}
The second tensor combine module is designed to dynamically allocate attention weights to each word across the three input tensors. The weights are obtained by performing max pooling along the embedding axis of three tensors stacked together. After obtaining the weights, it will be applied to the stacked embeddings. Note that the three tensors are all feature-wise normalised before stacking. The feature-wise normalising helps the model converge faster and achieve better generalisation. 
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{E''}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{E}')\\
        &\mathbf{S} = \text{Stack}(\mathbf{E}', \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
        &\mathbf{P} = \frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}_{a,b,c,j}\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\\
        &\mathbf{E}'' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}
\subparagraph*{Tensor Combine 3: Linear Layers as Attention Net}
\label{subpar:3_linear_layers_as_attention_net}
The third tensor combine module further extend the second module by leveraging linear layers as the attention net. Instead of simply perfoming mean pooling, it uses linear layers with activation functions in between to capture more relevant information upon deciding the weights. The process can be expressed as:
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{P} = \text{Stack}(\mathbf{E}, \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
    \end{split}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_2}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\quad\:\:\:\: \text{\small \textcolor{RoyalBlue}{Conv Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_1 = \text{LeakyReLU}(\mathbf{b}_7 + \mathbf{W}_7\cdot \mathbf{S}')\\
        &\qquad\mathbf{S}'_2 = \text{LeakyReLU}(\mathbf{b}_8 + \mathbf{W}_8\cdot \mathbf{S}'_1)\\
        &\qquad\mathbf{S}'_3 = \mathbf{b}_9 + \mathbf{W}_9* \mathbf{S}'_2\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
        % &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{\small \textcolor{Cerulean}{Attention Net}}
    \end{split}}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_3}
    \begin{split}
        &\mathbf{P} = [\mathbf{P}_1; \mathbf{P}_2; ...; \mathbf{P}_{512}]\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\qquad\qquad\qquad\qquad\\
        &\mathbf{E}' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}
\subparagraph*{Tensor Combine 4: Convolutional Layers as Attention Net}
\label{subpar:3_convolutional_layers_as_attention_net}
The fourth tensor combine module replaces the linear layers with convolutional layers. The equations below shows the attention net, which are different from the one in \hyperref[subpar:3_linear_layers_as_attention_net]{Tensor Transform - Linear Layers as Attention Net}.
\begin{equation} \label{eq:3_tensor_combine_conv}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\quad\:\:\: \text{\small \textcolor{RoyalBlue}{Linear Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_1 = \text{LeakyReLU}(\mathbf{b}_7 + \mathbf{W}_7\cdot \mathbf{S}')\\
        &\qquad\mathbf{S}'_2 = \text{LeakyReLU}(\mathbf{b}_8 + \mathbf{W}_8\cdot \mathbf{S}'_1)\\
        &\qquad\mathbf{S}'_3 = \mathbf{b}_9 + \mathbf{W}_9* \mathbf{S}'_2\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
    \end{split}}
\end{equation}
\TODO{Change to conv version}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                    Glyph Incorporation                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Glyph Incorporation}
\label{sec:3_glyph_incorporation}
In our methodology, we enhance the training process by specifically targeting commonly misrecognized characters, such as "." and "," or "O" and "o", with the aim of reducing the likelihood of these errors in future recognitions. To achieve this, we commence by determining the probability $\mathcal{P}_{ij}$, where $i$ represents the correct character that has been erroneously recognized as character $j$. This strategy is a deviation from the CharBERT NLM training approach, which incorporates character-level errors into the text at random.

By leveraging $\mathcal{P}_{ij}$, we refine our training methodology to introduce errors in a more systematic manner, based on the observed probabilities of specific misrecognitions. This targeted approach allows us to focus the model's learning on correcting these particular errors, enhancing its accuracy and reliability in distinguishing between characters that are commonly confused.

\subsection{Get $\mathcal{P}_{ij}$}
\label{subsec:3_get_pij}
To obtain $\mathcal{P}_{ij}$, it is essential first to calculate the frequency of each character misrecognised by the recognizer (TrOCR), termed as \emph{misrecognised frequency}. This process involves initially using the plain TrOCR model to process the GW and IAM datasets. Upon obtaining the text outputs generated by TrOCR, a comparison is made with the corresponding text labels to obtain the frequencies of the characters that were misrecognised.

To match the generated outputs from the labels, we use the "Bio" package in Python, refering to Biopython. It is a set of tools for biological computation. Biopython is primarily designed to work with data from bioinformatics, such as sequences of DNA, RNA, and proteins. It provides capabilities for DNA and protein sequence analysis and alignments.

While Biopython is not directly designed for processing natural language texts, its tools and principles can still be indirectly applied to text data. Biopython's capabilities for sequence matching and regular expressions can inspire similar applications in text analysis. For example, in our scenario, Biopython's framework can be leveraged to perform character-by-character matching within text strings, identifying and annotating any discrepancies encountered. \FIG{add example}This approach exemplifies how Biopython's core functions can be creatively repurposed beyond their intended biological applications, offering valuable insights and techniques for handling and analyzing text data.

\subsection{Training CharBERT$_{\mathcal{P}_{ij}}$}
\label{subsec:3_training_charbert_pij}
\newchap{Methodology}
\label{chap:3_method}
In this chapter, I will introduce the composite model of TrOCR and CharBERT by leveraging the ideas in Candidate Fusion mentioned in \autoref{chap:2_related_work}. First, the design concept behind the model will be outlined. Following this, I will elaborate on data collection and preprocessing, and the architectures of the models developed in this study. Finally, I will delve into the details of the training process, including the utilisation of GPU resources, the optimiser and the loss function.
\TODO{Rewrite this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                       Design Concept                                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design Concept}
\label{sec:3_design_concept}
CharBERT mitigates the problems of incomplete modelling and fragile representation by including the character encoding in addition to the subword level information. Furthermore, having NLM as the pre-training task makes CharBERT effective at correcting character level typos, which is a desired feature for OCR correcting. 

On the other hand, Candidate Fusion claims that having an interaction between the recogniser and the LM can enhance the performance of OCR. Thus, combining TrOCR and CharBERT is expected have an improvement on the OCR accuracy. 
\TODO{Rewrite this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Data Collection and processing                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Collection and Processing}
\label{sec:3_data_collection_and_processing}
This section is dedicated to detailing the data collection process for OCR task training and CharBERT$_{\text{SMALL}}$ training. We will outline the types and sources of data harnessed for this study, emphasizing the diversity and volume of the datasets to ensure comprehensive learning. Following the data collection overview, we will delve into the processing techniques applied to the collected data.
\subsection{Data for OCR} 
\label{subsec:3_data_for_ocr}
We focus on the performances of handwritten datasets on the composite model. The data used in this study is George Washington (GW) handwritten dataset and IAM handwritten dataset. They serve as a valuable benchmark for developing and evaluating handwriting recognition systems. Note that to ensure comparability with existing studies, this research adheres to the established train-validation-test splits of the GW and IAM datasets. 
\TODO{Add data downloading link}

The GW dataset is a collection of historical letters and dairies handwritten by George Washington and this secretaries in 1755. The dataset is often used in research focused on recognising historical handwriting, which poses unique challenges due to the use of archaic words and phrases, and the degradation of materials over time.

The IAM dataset is a more comtemporary collection of English handwriting samples. It contains forms written by hundreds of writers, and is thus renowned for its diversity in handwriting styles, the variability of written content. 

While the George Washington Handwritten Dataset provides a niche focus on historical documents, making it ideal for projects related to historical document analysis and preservation, the IAM Handwriting Database offers a broad spectrum of modern handwriting samples, making it suitable for a wide range of handwriting recognition applications. Both datasets have played crucial roles in advancing the field of handwriting recognition. 
\FIG{Add data sample here}
\paragraph*{Transcription Ground Truth Processing}
\label{par:3_transcription_gound_truth_processing}
The transcription ground truths of the IAM dataset are stored in XML files, where parsing these files is sufficient to retrieve the transcription texts. On the other hand, the transcription ground truths of the GW dataset follow a more complex format. In this dataset, individual characters within words are separated by hyphens ("-"), and words themselves are separated by vertical bars ("\textbar"). Additionally, punctuation marks are represented by special characters, with a specific table detailing the replacements for each punctuation mark. The table for the punctuation replacement can be found in this \href{https://github.com/Yung-Hsin-Chen/master_thesis/blob/src/model/config/punctuation_list.json}{link}. 
\TODO{Add text example}
For data processing purposes, we modify the transcription texts from the GW dataset by removing the hyphens, replacing vertical bars and special characters representing punctuations with spaces and their respective punctuation marks. This process ensures that the transcription texts are standardized and easily readable. 
\subsection{Data for Training CharBERT$_{\text{SMALL}}$}
\label{subsec:3_data_for_training_charbert}
In the original study, the authors trained CharBERT using a 12GB dataset from Wikipedia. This training process spanned 5 days, utilizing the computational power of two NVIDIA Tesla V100 GPUs. Given constraints in time and computational resources, our approach involves testing a scaled-down version of CharBERT, which we have designated as CharBERT$_{\text{SMALL}}$. This variant was trained on a significantly smaller dataset, specifically 1.13GB of English Wikipedia data, from which sentences were randomly sampled. This adaptation allows us to evaluate the performance of CharBERT under more restricted conditions, ensuring our experiments are feasible within our available resources.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     Model Architecture                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Architecture}
\label{sec:3_composite_model_architecture}
The end-to-end composite model is composed of TrOCR and CharBERT. TrOCR does the text recognition task, while CharBERT serves as the corrector. The decoder input of TrOCR will go through CharBERT for correction before entering the decoder of TrOCR. By doing this, the TrOCR decoder will decode the input embedding corrected by CharBERT. In this section, I will first elaborate on the architectures of TrOCR and CharBERT, and how I fuse them together.

\subsection{Notations}
\label{subsec:3_Notations}
In this report, we denote a one-dimensional tensor (vector) using boldface lowercase letters, while a multi-dimensional tensor appears as boldface uppercase letters. The TrOCR sequence IDs are denoted by $\mathbf{W}$ = \{$\mathbf{w}_1, ..., \mathbf{w}_i, ..., \mathbf{w}_D$\} where each $\mathbf{w}_i$ stands for a token tokenised by TrOCR tokeniser and $D$ signifies the length of the sequence. 

\subsection{Recogniser - TrOCR}
\label{subsec:3_recogniser_trocr}
TrOCR is composed of an image encoder and a text decoder. The image encoder takes pixel values of an image as the input, and the encoder output will then be fed into the decoder for text generation. This study involves adapting the decoder to integrate with CharBERT.  Therefore, it will be crucial to focus on and provide a detailed explanation of its architecture. The following contents will first focus on the inputs of the decoder and the label that is used for loss calculation. Then, I will elaborate on how the inputs go through the decoder.

\paragraph*{Decoder Inputs and Labels}
\label{par:3_decoder_inputs_and_labels}
The text decoder takes encoder output, decoder input ids and padding attention mask\myfootnote{Note that the padding attention mask should not be confused with the casual attention mask. The padding attention mask prevents the model from attending to the paddings, while the casual attention mask ensures that the prediction for a specific token in the sequence can only be influenced by previously generated tokens.} as input, and output the generated text. The decoder output will be compared to the label tensor for loss calculation. The loss will then be used for updating the gradients of the parameters during training.

The encoder output is a set of features extracted from the pixel values. These features are the transformed representation of the input image, not only capturing the patterns within the image, but also understanding the arrangements and relations of each elements in the image.

The encoder's output represents a set of high-level features extracted from the input image. These features are essentially a transformed representation of the input image, capturing various aspects and patterns that are relevant for recognizing the text within the image.

Decoder input ids is a tensor of token ids converted from the label texts. The token ids includes three special tokens, which are $<$bos$>$, $<$eos$>$ and padding. The id, $<$bos$>$, is represented by the number 0 and indicates the start of the text. The id, $<$eos$>$, is denoted by the number 2 and represents the end of the text. As for padding, it is a series of number 1 appended after <eos> to make sure that all the input ids have the same length. The decoder input ids can be generated automatically by the TrOCR tokeniser. By default, the generated decoder input ids have $<$bos$>$ in the beginning, and $<$eos$>$, paddings at the end. This can be used during training to simulate the process of inference\myfootnote{During inference, the decoder output from the previous step will be the new decoder input. However, the decoder has no other context at the first step since no output sequence has been generated yet. Thus, the very first step of the decoder input is just this $<$bos$>$ token. The decoder processes this initial input and generates a decoder outpu, which is being used to update the decoder input.}. 

The other input of the decoder, padding attention mask, controls the information that should be ignored by TrOCR. The padding attention mask consists of 0s and 1s. The 0s covers the padding parts of the decoder input ids to prevent the model from focusing on them.

The label tensor for calculating the loss is decoder input ids without the $<$bos$>$ token. And since the TrOCR model is inherited from VisionEncoderDecoderModel, the class requires the label paddings to be -100. Hence, the padding token of the label tensor will be replaced by -100 instead of the 1s.

\paragraph*{Inside the Decoder}
\label{par:3_inside_the_decoder}
Let's first talk about the original decoder before adapting it to fit CharBERT. The decoder receives the decoder input ids, its padding attention mask and the encoder output. First, the decoder input ids converted into decoder embeddings. Next, the decoder embeddings is added with the positional encoding to help keep the spatial information before going through decoder stacks, including masked multi-head attention, multi-head attention and the FFNN. The padding attention mask will be applied to both masked multi-head attention and multi-head attention to prevent TrOCR from attending the padding tokens. Finally, the output of the decoder stacks will go through a linear layer and a softmax layer to generate an output sequence. 

\subsection{Corrector - CharBERT}
\label{subsec:3_corrector_charbert}
CharBERT has a dual-channel architecture, which are the token and the character channels. The token channel has the same structure as BERT or RoBERTa, depending on the initialisation. In this study, we will focus on the CharBERT\textsubscript{RoBERTa}. From this point forward in the document, any mention of "CharBERT" will specifically refer to CharBERT\textsubscript{RoBERTa}, unless explicitly stated otherwise. Similar to BERT and RoBERTa, CharbERT takes texts as input, but instead of outputting a single embedding, CharBERT outputs a token level embeddings and a character level embeddings. Since the model architecture of the token channel is identical to BERT/RoBERTa , the subsequent contents will focus on the character channel and heterogeneous interaction.

\paragraph*{Character Channel \& Heterogeneous Interaction}
\label{par:3_character_channel_heterogeneous_interaction}
The character channel first splits the input text into characters, convert them into IDs by looking up the dictionary. Next, it embeds the IDs, and then apply the bidirectional GRU (Bi-GRU) layer to generate the output embeddings. The output embeddings from the character and token level will then go through the transformer and the heterogeneous interaction. The heterogeneous interaction is consists of two parts, fusion and divide. The fusion allows both embeddings to enrich each other by FFNN and CNN, while the divide part ensures that both embeddings keep their unique features by a FFNN and a residual connection. The residual connection helps retain the respective information from the two embeddings. CharBERT then repeat the transformer and heterogeneous interaction to capture more features and information underlying the input texts.

\subsection{Composite Model}
\label{subsec:3_composite_model}
The composite model is designed to combine TrOCR and CharBERT. The idea is that, during the inference phase, the decoder output is recycled as its input in a feedback loop. Before this recycled input is fed back into the decoder, it has to undergo correction and refinement by CharBERT. Thus, CharBERT will be integrated between the decoder input and the decoder stacks, ensuring that the input to the decoder is optimised on each iteration of the process. The problems with this integration are: 1) TrOCR decoder takes token IDs as input but CharBERT outputs are embeddings; 2) CharBERT takes texts as input but the TrOCR decoder input is a tensor; 3) TrOCR embedding representations do not match CharBERT embedding representations; 4) TrOCR decoder input is a single tensor but CharBERT has dual-channel outputs. The following contents will be discussing the details and approaches to address these two problems.

\paragraph*{Adapted TrOCR}
\label{par:3_adapted_trocr}
Before delving into the potential solution to the first problem, it is essential to thoroughly examine the issue. The TrOCR decoder is designed to take token IDs as input, which it then maps to embeddings. These embeddings are then added with the positional encoding before being passed into the Transformer decoder. However, the challenge arises due to the outputs from CharBERT after the correction being embeddings, not token IDs. This presents a compatibility issue. If we were to solve this problem by converting the CharBERT representations into token IDs before inputting TrOCR decoder, another issue emerges. Employing token IDs as intermediaries between model components is problematic, as token IDs are inherently integers. During the training process, the model weights are updated as floating-point numbers. Constraining these weights to integer values is not feasible. While rounding up the updated weights can technically convert floats to integers, but the underlying meaning of rounding it will be questionable. The rounding would likely distort the model's learning process and could be meaningless for the task. Therefore, the only reasonable way to solve this issue is to modify TrOCR decoder so that it takes embeddings as input. 

There is a straightforward solution. By repositioning the embedding layer from the TrOCR decoder to precede CharBERT, token IDs will be initially converted to TrOCR embeddings, which will then be input into CharBERT for correction. Consequently, the adapted TrOCR can now accept embeddings directly instead of token IDs. This modification ensures that the outputs from CharBERT integrate with the TrOCR decoder and that there will be no IDs between model components.

\paragraph*{Adapted CharBERT}
\label{par:3_adapted_charbert}
The second problem involves the mismatching data types of the input and the output. The TrOCR decoder input is an embedding according to the modification made in \hyperref[par:3_adapted_trocr]{Adapted TrOCR}. This embedding should be fed into CharBERT for correction. Unfortunately, CharBERT takes only texts as input. Thus, creating an adapted CharBERT model is necessary. The adapted CharBERT is modified so that it takes token and character embeddings as inputs. These two embeddings will then go through the token and character channels. To be specific, the adapted CharBERT will no longer take texts as input, convert them into IDs and embed them. The embeddings will be provided as the input directly.

\paragraph*{Tensor Transform}
\label{par:3_tensor_transform}
The third problem is more complex. The TrOCR decoder input is based on embeddings unique to TrOCR. Even for the same text, the embedding representations from TrOCR and CharBERT are markedly different, not to mention CharBERT has dual-channel embeddings. Not only do both models have different representations for the same text, the embedding dimensions are also incompatible. Therefore, even though CharBERT can take embeddings as inputs, the TrOCR decoder input still cannot be fed into CharBERT directly. To address this problem, an architecture consists of CNN and FFNN is employed. This approach serves to adjust the dimension of TrOCR decoder input to align with dimensions of CharBERT token and character embeddings. Additionally, this dimensional transformation allows the model to match the representations between TrOCR and CharBERT effectively. To better illustrate the architecture of the tensor transform, the dimensions will be noted after each input/output between brackets, e.g., (batch size, embedding size).

The goal of the tensor transform is to convert the TrOCR decoder input size (batch size, 512, 1024) into the CharBERT token embeddings size (batch size, 510, 768) and CharBERT character embeddings size (batch size, 3060, 256), where the second dimension is the sequence length and the third dimension is the embedding size. In this tensor transform architecture, the transformation is separated into two stages. In the first stage, the decoder input first goes through a series of CNN layers interspersed with LeakyReLU and batch normalisation steps to adjust the sequence dimension (dim=1). Then, for the second stage, the output from the first stage goes through FFNN layers with LeakyReLU in between to modify the embedding dimension (dim=2).
\begin{equation} \label{eq:3_tensor_transform_cnn}
    \begin{split}
        &\mathbf{t}_{1, j} = \text{LeakyReLU}(\mathbf{b}_1 + \sum_{k=1}^{3}\mathbf{W}_{1,k}\cdot \mathbf{e}_{i+k-1})\quad;\quad \mathbf{t}'_1 = \text{Batch\_Norm}(\mathbf{t}_1)\\
        &\mathbf{t}_{2, l} = \text{LeakyReLU}(\mathbf{b}_2 + \sum_{k=1}^{3}\mathbf{W}_{2,k}\cdot \mathbf{t}'_{1,j+k-1})\quad;\quad \mathbf{t}'_2 = \text{Batch\_Norm}(\mathbf{t}_2)\\
        &\mathbf{t}_{3, p} = \text{LeakyReLU}(\mathbf{b}_3 + \sum_{k=1}^{3}\mathbf{W}_{3,k}\cdot \mathbf{t}'_{2,l+k-1})\quad\\
    \end{split}
\end{equation}
After applying the CNN layers, the dimension of the decoder input becomes (batch size, 510, 1024). The sequence dimension (dim=1) has become the desired dimension for CharBERT token embedding. The purpose of the CNN layers is to either expand or contract the sequence length, offering the advantage of preserving spatial information. This is the primary reason for selecting CNN instead of FFNN for adjusting the sequence dimensions. Moreover, it is worth noting that there are batch normalisation in between the convolutional layers. The batch normalisations are there to stabilise the deep model and to help maintaining healthier gradients with the presense of the activation functions\footnote{With the presense of activation functions in deep networks, gradient can easily explode or vanish. Batch normalisation helps in maintaining a healthier gradient flow in the network, which can improve the efficiency of backpropagation and thence the learning process.}. Next, we enter the second stage of the tensor transformation, which utilise the FFNN layers.
\begin{equation} \label{eq:3_tensor_transform_ffnn}
    \begin{split}
        &\mathbf{T}_{4} = \text{LeakyReLU}(\mathbf{b}_4 + \mathbf{W}_4\cdot \mathbf{T}_{3})\\
        &\mathbf{T}_{5} = \text{LeakyReLU}(\mathbf{b}_5 + \mathbf{W}_5\cdot \mathbf{T}_{4})\\
        &\mathbf{T}_{n} = \text{LeakyReLU}(\mathbf{b}_6 + \mathbf{W}_6\cdot \mathbf{T}_{5})\\
    \end{split}
\end{equation}
The resulting tensor is the CharBERT token embedding $\mathbf{T} = \{\mathbf{t}_1, ..., \mathbf{t}_n, ..., \mathbf{t}_N\}$, where $N$ is the token sequence length with tensor size (batch size, 510, 768).

Same operations are applied to the TrOCR decoder input but with different dimension expansion to generate the CharBERT character embedding $\mathbf{C} = \{\mathbf{c}_1, ..., \mathbf{c}_m, ..., \mathbf{c}_M\}$, where $M$ is the character sequence length. It is established that the length $M$ is six times that of $N$, based on the assumption that the average word contains six characters.

The Tensor Transform layer aims to convert the TrOCR decoder input into CharBERT token embedding and character embedding. The CNN and FFNN layers not only match the dimensions between the tensors, but also learn to map the contextual information in TrOCR embedding to CharBERT embeddings.

\paragraph*{Tensor Combine}
\label{par:3_tensor_combine}
The fourth problem arises due to the fact that CharBERT produces two tensors - token and character representations, whereas TrOCR decoder input is a single tensor. Consequently, the solution would be to combine the two output tensors from CharBERT into a single tensor. In addition to the outputs from CharBERT, we can also add a residual connection from the original TrOCR decoder embedding to stabilise the deep model. The residual connection here can encourage the reuse of features from the original TrOCR decoder embedding and prevent gradient vanishing.

Among the three tensors, two of them comes from CharBERT. Therefore, the representation is not understood by TrOCR decoder and the dimensions do not match. The two of them should first undergo \hyperref[par:3_tensor_transform]{Tensor Transform} before the combination. 
\begin{equation} \label{eq:3_tensor_combine_transform}
    \begin{split}
        &\mathbf{T}\in \mathbb{R}^{d_N}\to \mathbf{T}' = \text{Tensor\_Transform}(\mathbf{T})\in \mathbb{R}^{d_D}\\
        &\mathbf{C}\in \mathbb{R}^{d_M}\to \mathbf{C}' = \text{Tensor\_Transform}(\mathbf{C})\in \mathbb{R}^{d_D}\\
    \end{split}
\end{equation}
where $d_D$ = (batch size, 512, 1024); $d_N$ = (batch size, 510, 768); $d_M$ = (batch size, 3060, 256).

After performing the tensor transform module, three tensors will all be of size (batch size, 512, 1024), which is the same as the original TrOCR decoder input size. They can then be combined and fed into the TrOCR decoder stack. In this study, we examine four tensor combine module architectures: 1) simply adding all of them; 2) mean pooling; 3) using linear layers as the attention net; 4) using convolutional layers as the attention net.

\subparagraph*{Tensor Combine 1: Adding}
\label{subpar:3_adding}
The first tensor transform module is simply aggregating three tensors into a single tensor, operates by performing an element-wise addition of the three input tensors. Given the TrOCR decoder embeddings $\mathbf{E}$, the CharBERT transformed token representation $\mathbf{T'}$ and the CharBERT transformed character representation $\mathbf{C'}$, each contributing equally to the formation of the combined tensor (each input tensor's information is weighted equally), the operation can be mathematically expressed as:
\begin{equation} \label{eq:3_tensor_combine_adding}
    \begin{split}
        \mathbf{E}'_{1, ijk} = \mathbf{E}_{ijk} + \mathbf{T}'_{ijk} + \mathbf{C}'_{ijk}
    \end{split}
\end{equation}
where $i = 1, 2, ..., \text{batch size}; j = 1, 2, ..., 512; k = 1, 2, ..., 1024$.
\subparagraph*{Tensor Combine 2: Mean Pooling}
\label{subpar:3_mean_pooling}
The second tensor combine module is designed to dynamically allocate attention weights to each word across the three input tensors. The weights are obtained by performing max pooling along the embedding axis of three tensors stacked together. After obtaining the weights, it will be applied to the stacked embeddings. Note that the three tensors are all feature-wise normalised before stacking. The feature-wise normalising helps the model converge faster and achieve better generalisation. 
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{E''}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{E}')\\
        &\mathbf{S} = \text{Stack}(\mathbf{E}', \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
        &\mathbf{P} = \frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}_{a,b,c,j}\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\\
        &\mathbf{E}'' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}
\subparagraph*{Tensor Combine 3: Linear Layers as Attention Net}
\label{subpar:3_linear_layers_as_attention_net}
The third tensor combine module further extend the second module by leveraging linear layers as the attention net. Instead of simply perfoming mean pooling, it uses linear layers with activation functions in between to capture more relevant information upon deciding the weights. The process can be expressed as:
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{P} = \text{Stack}(\mathbf{E}, \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
    \end{split}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_2}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\quad\:\:\:\: \text{\small \textcolor{RoyalBlue}{Conv Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_1 = \text{LeakyReLU}(\mathbf{b}_7 + \mathbf{W}_7\cdot \mathbf{S}')\\
        &\qquad\mathbf{S}'_2 = \text{LeakyReLU}(\mathbf{b}_8 + \mathbf{W}_8\cdot \mathbf{S}'_1)\\
        &\qquad\mathbf{S}'_3 = \mathbf{b}_9 + \mathbf{W}_9* \mathbf{S}'_2\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
        % &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{\small \textcolor{Cerulean}{Attention Net}}
    \end{split}}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_3}
    \begin{split}
        &\mathbf{P} = [\mathbf{P}_1; \mathbf{P}_2; ...; \mathbf{P}_{512}]\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\qquad\qquad\qquad\qquad\\
        &\mathbf{E}' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}
\subparagraph*{Tensor Combine 4: Convolutional Layers as Attention Net}
\label{subpar:3_convolutional_layers_as_attention_net}
The fourth tensor combine module replaces the linear layers with convolutional layers. The equations below shows the attention net, which are different from the one in \hyperref[subpar:3_linear_layers_as_attention_net]{Tensor Transform - Linear Layers as Attention Net}.
\begin{equation} \label{eq:3_tensor_combine_conv}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\quad\:\:\: \text{\small \textcolor{RoyalBlue}{Linear Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_1 = \text{LeakyReLU}(\mathbf{b}_7 + \mathbf{W}_7\cdot \mathbf{S}')\\
        &\qquad\mathbf{S}'_2 = \text{LeakyReLU}(\mathbf{b}_8 + \mathbf{W}_8\cdot \mathbf{S}'_1)\\
        &\qquad\mathbf{S}'_3 = \mathbf{b}_9 + \mathbf{W}_9* \mathbf{S}'_2\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
    \end{split}}
\end{equation}
\TODO{Change to conv version}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Model Training and Evaluation Criteria                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Training and Evaluation Criteria}
\label{sec:3_model_training_and_evaluation_criteria}
In this section, we will be exploring the training and evaluation process of the composite model. Details including optimiser, loss function, hyperparameters, and the challenges encountered training deep models and how to mitigate it will also be discussed.

In this section, we delve into the training and evaluation process for the composite model. The section will cover key aspects such as the choice of optimizer, the loss function employed, and the hyperparameters set for the training. Additionally, the challenges commonly faced when training deep learning models, and insights into strategies and solutions to mitigate these issues will also be discussed.
\subsection{Training Details}
\label{subsec:3_training_details}
% optimiser, loss function, hyperparameters, utilisation of GPU resources
When training deep learning models, the selection of optimisation, loss computation techniques, and the hyperparameters settings play a crucial role in the effeiciency and effectiveness of the training process. In this study, we use Adam as the optimiser and cross-entropy for loss computation.

\paragraph*{Optimiser}
\label{par:3_Optimiser}
Utilising Adam as the optimiser provides a sophisticated approach by adopting an adaptive learning rate, which is particularly adept at managing sparse gradients tasks such as NLP problems. Besides, due to its efficient computation of adaptive learning rates, Adam often leads to faster convergence on training data. This can significantly reduce the time and computational resources needed to train deep models, making the process more efficient.

For the training of the composite model, the learning rate has been meticulously set to $1\mathrm{e}{-5}$. This settingn ensures a controlled and gradual adaptation of the model parameters, facilitating a smooth convergence towards the local minima. Additionally, a weight decay parameter of $1\mathrm{e}{-5}$ is employed to enhance the model's ability to navigate the optimization landscape efficiently. This careful calibration prevents the optimizer from overshooting the local minima, thereby promoting stability in the training process and improving the model's overall performance.

\paragraph*{Loss Function}
\label{par:3_loss_function}
OCR tasks involves predicting the probability distribution of possible characters for a given input image. It is a multi-class classification problem, with each character representing a unique class. Cross-entropy loss is naturally suited for multi-class settings. This makes it directly applicable to the task of classifying images into characters. In addition, unlike other loss functions that might focus solely on the accuracy of the classification, cross-entropy loss encourages the model not just to predict the correct class but to do so with high confidence. High-confidence wrong predictions are penalised more, encouraging the model to be cautious about making predictions when unsure, which is often the case with less frequent characters.
\subsection{Training Challenges and Solutions}
\label{subsec:3_training_challenges_and_solutions}
% overfitting (dropout), underfitting, learning rate, gradient vanishing/exploding, converge faster (norm), transfer learning
Training deep learning models involves navigating a series of common challenges that can significantly impact their performance and effectiveness. Common challenges include overfitting, vanishing/exploding gradients, high computational costs, and data quantity. The following discussion will delve into these challenges and the strategies used in this study to mitigate them.

\paragraph*{Overfitting}
\label{par:3_overfitting}
Overfitting occurs when a model learns the training data too well, capturing noisein the training set instead of learning the underlying patterns, which results in poor generalisation to new, unseen data. Deep learning models, by their very nature, have a large number of parameters, allowing them to model intricate patterns and relationships in the data. However, it also means they have the capacity to memorise irrelevant details in the training data, leading to overfitting. Lack of regularisation techniques, or poorly chosen learning rate and batch size can easily lead to overfitting. 

To mitigate overfitting, we implemented dropout layers\CHECK{make sure dropout is used} between both linear and convolutional layers within the composite model architecture. Dropout serves as a form of regularization that, by temporarily dropping out units from the network, prevents the model from becoming overly dependent on any single element of the training data, thereby enhancing its generalization capabilities.

Additionally, we explored the impact of batch size on model training. Smaller batch sizes result in more noise during the gradient updates, which can have a regularizing effect. However, very small batch sizes can lead to extremely noisy gradient estimates, which might make training unstable or lead to convergence on suboptimal solutions. Through iterative testing, we determined that a batch size of 8 strikes an optimal balance, offering sufficient regularization to mitigate overfitting while maintaining stable and effective training dynamics.

\paragraph*{Vanishing/Exploding Gradients}
\label{par:3_vanishing_exploding_gradients}
Training deep models often encounters exploding or vanishing gradient problems due to their complex architectures and the long chains of computations involved. If the gradients are large (greater than 1), they can exponentially increase as they propagate back through the layers, leading to exploding gradients. Conversely, if the gradients are small (less than 1), they can exponentially decrease, leading to vanishing gradients. 

Certain activation functions, like the sigmoid or tanh, squish a large input space into a small output range in a non-linear fashion. For inputs with large magnitudes, the gradients can be extremely small, leading to vanishing gradients. In addition, improper initialization of weights can exacerbate the exploding or vanishing gradient problems. For instance, large initial weights can lead to exploding gradients, while small initial weights can contribute to vanishing gradients.

To mitigate the vanishing/exploding gradients, we deploy strategies such as Xavier initialisation, Leaky ReLU activation function, gradient clipping, residual connection and batch normalisation in the composite model architecture. The details of residual connection and batch normalisation\CHECK{Make sure they are mentioned} is discussed in \hyperref[subsec:3_composite_model]{Composite Model}.

\paragraph*{High Computational Costs}
\label{par:3_high_computational_costs}
Deep models, especially those with many layers and parameters, require significant computational resources and time to train. Hardware accelerators like GPUs can reduce training times. In this study, a single A100 GPU, equipped with 80GB of RAM, was utilized to accelerate the training process. When applied to the GW dataset and the IAM dataset, the training durations were approximately 2 minutes per epoch and 30 minutes per epoch, respectively.

\paragraph*{Data Quantity}
\label{par:3_data_quantity}
Deep models typically require large datasets to effectively learn and generalize due to their complex architectures and the vast number of parameters they contain. Training these models from scratch on limited data often leads to overfitting. To mitigate this issue, we employ transfer learning. In this study, we take pre-trained CharBERT and TrOCR model, which are developed on large and comprehensive datasets, and adapt it to our specific task. 

\subsection{Evaluation}
\label{subsec:3_evaluation}
The validation set serves the purpose of hyperparameter tuning and model selection. Meanwhile, the testing set is reserved for the final evaluation. For the GW dataset, we employ a 4-fold cross-validation approach. This method divides the dataset into four equally sized segments, using each in turn for testing while the remaining three serve as the training set. The final results for the GW dataset represent the average performance across these four folds.

In this study, we focus on word error rate (WER) and character error rate (CER) as our primary evaluation metrics. These metrics are critical for assessing the model's accuracy in recognizing and reproducing text, providing the model's performance in terms of both word-level and character-level precision.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          CharBERT Training and Evaluation Criteria                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CharBERT$_{\text{SMALL}}$ Training and Evaluation Criteria}
\label{sec:3_charbert_training_and_evaluation_criteria}
The training and evaluation of CharBERT$_{\text{SMALL}}$ adhere closely to the methodology outlined by the original authors. The model is trained using the pre-training objectives, i.e., MLM and NLM, over the large text corpus. The training involves backpropagation and optimisation of weights to minimize the prediction error for both MLM and NLM tasks. While adhering to the \href{https://github.com/mawentao277/CharBERT/blob/main/shell/mlm.sh}{hyperparameters recommended by the original authors}, we have adjusted the batch size to 16, up from the suggested size of 4, to accelerate the computation process.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                    Glyph Incorporation                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Glyph Incorporation}
\label{sec:3_glyph_incorporation}
In our methodology, we enhance the training process by specifically targeting commonly misrecognized characters, such as "." and "," or "O" and "o", with the aim of reducing the likelihood of these errors in future recognitions. To achieve this, we commence by determining the probability $\mathcal{P}_{ij}$, where $i$ represents the correct character that has been erroneously recognized as character $j$. This strategy is a deviation from the CharBERT NLM training approach, which incorporates character-level errors into the text at random.

By leveraging $\mathcal{P}_{ij}$, we refine our training methodology to introduce errors in a more systematic manner, based on the observed probabilities of specific misrecognitions. This targeted approach allows us to focus the model's learning on correcting these particular errors, enhancing its accuracy and reliability in distinguishing between characters that are commonly confused.

\subsection{Get $\mathcal{P}_{ij}$}
\label{subsec:3_get_pij}

\subsection{Training CharBERT$_{\mathcal{P}_{ij}}$}
\label{subsec:3_training_charbert_pij}
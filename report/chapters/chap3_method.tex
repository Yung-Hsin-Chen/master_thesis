\newchap{Methodology}
\label{chap:3_method}
In this chapter, I will introduce the composite model of TrOCR and CharBERT by leveraging the ideas in Candidate Fusion mentioned in \href{chap:2_related_work}{Chapter 2}. First, the design concept behind the model will be outlined. Following this, I will elaborate on data collection and preprocessing, and the architectures of the models developed in this study. Finally, I will delve into the details of the training process, including the utilisation of GPU resources, the optimiser and the loss function.
\TODO{Rewrite this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                       Design Concept                                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design Concept}
\label{sec:3_design_concept}
CharBERT mitigates the problems of incomplete modelling and fragile representation by including the character encoding in addition to the subword level information. Furthermore, having NLM as the pre-training task makes CharBERT effective at correcting character level typos, which is a desired feature for OCR correcting. 

On the other hand, Candidate Fusion claims that having an interaction between the recogniser and the LM can enhance the performance of OCR. Thus, combining TrOCR and CharBERT is expected have an improvement on the OCR accuracy. 
\TODO{Rewrite this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Data Collection and processing                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Collection and Processing}
\label{sec:3_data_collection_and_processing}
This section is dedicated to detailing the data collection process for OCR task training and CharBERT$_{\text{SMALL}}$ training. We will outline the types and sources of data harnessed for this study, emphasizing the diversity and volume of the datasets to ensure comprehensive learning. Following the data collection overview, we will delve into the processing techniques applied to the collected data.
\subsection{Data for OCR} 
\label{subsec:3_data_for_ocr}
We focus on the performances of handwritten datasets on the composite model. The data used in this study is the George Washington (GW) handwritten dataset\myfootnote{The George Washington handwritten dataset can be downloaded here: \url{https://fki.tic.heia-fr.ch/databases/washington-database}} and the Joseph Hooker (JH) handwritten dataset\myfootnote{The Joseph Hooker handwritten dataset can be downloaded here: \url{https://github.com/jschaefer738b/JosephHookerHTR}}. They serve as valuable benchmarks for developing and evaluating handwriting recognition systems. Note that to ensure comparability with existing studies, this research adheres to the established train-validation-test splits of the GW and JH datasets.

Although the IAM dataset is a more contemporary collection of English handwriting samples, renowned for its diversity in handwriting styles, this study did not use it since TrOCR is pre-trained on the IAM dataset. The most significant risk of fine-tuning the composite on the same dataset is overfitting. The model may become excessively tailored to the IAM dataset, performing well on this specific dataset but poorly on unseen, real-world data or other similar datasets. In addition, if the model has already been trained to a point where it performs optimally on the IAM dataset, further training the composite model on the same data might yield diminishing returns. The model may not improve significantly and could even start to fit the noise in the dataset as mentioned above.

\paragraph*{George Washington Dataset}
\label{par:3_george_washington_dataset}
The GW dataset is a collection of historical letters and diaries handwritten by George Washington and his secretaries in 1755. This dataset comprises various types of documents, including personal correspondence, military orders, and official communications, which provide a unique window into the early American colonial period. The dataset is frequently utilized in research focused on recognizing historical handwriting, which poses unique challenges due to the use of archaic words and phrases, and the degradation of materials over time. The different handwriting styles in the dataset, ranging from Washington's neat writing to the quicker, less tidy ones, make it very useful for developing strong handwriting recognition systems. This dataset not only supports the preservation and accessibility of these important texts but also aids in advancing the technology needed to interpret and digitize aged manuscripts. An example from the GW dataset is shown in \autoref{fig:3_gw_line_image}.

\fig{images/gw_line_image.png}{fig:3_gw_line_image}{Sample of handwritten text from GW dataset. The excerpt reads: only for the publick use, unless by particu}{11.5}{GW Line Image}

\paragraph*{Joseph Hooker Dataset}
\label{par:3_joseph_hooker_dataset}
The JH dataset consists of correspondence between Joseph Dalton Hooker (1817-1911) and other leading scientists of his time. This collection offers unparalleled insights into the scientific dialogues of the era, featuring interactions with notable figures such as Charles Darwin. As such, it is an invaluable resource for historians specializing in science, botany, and the Victorian period. Hooker’s letters are diverse in content, extending beyond mere botanical discussions to include observations from his travels, his perspectives on contemporary scientific debates, his personal reflections, and his professional communications. This rich variety provides a comprehensive view of his intellectual engagement and contributions. An example from the GW dataset is shown in \autoref{fig:3_jh_line_image}.

\fig{images/jh_line_image.jpg}{fig:3_jh_line_image}{Sample of handwritten text from JH dataset. The excerpt reads: I will send them to the host to be}{11}{JH Line Image}

The Joseph Hooker and George Washington datasets, while serving distinct academic purposes, both play significant roles in the realm of handwritten text recognition (HTR) tasks. The Joseph Hooker dataset, with its extensive collection of 19th-century botanical writings and correspondence, provides a unique challenge for HTR technologies due to the scientific terminology and personal handwriting styles found in the documents. This variety enables developers to fine-tune HTR models to handle complex vocabulary and diverse manuscript formats, which is essential for digital humanities projects focusing on scientific archives.

Similarly, the George Washington dataset, consisting of an array of 18th-century materials including letters, diaries, and official documents, presents its own set of challenges for HTR. The historical significance of these documents demands a high level of accuracy in digital transcription. Washington's dataset helps in training HTR systems to recognize older forms of English script and the idiosyncrasies of historical American handwriting, which are crucial for preserving and making accessible key documents from the formative years of the United States.

In both cases, these datasets not only support the preservation and accessibility of historical texts but also advance the development of HTR systems. They enable the refinement of algorithms capable of interpreting a wide range of handwritten styles and orthographic conventions, thus broadening the utility of HTR technologies across different historical and scientific domains.

\paragraph*{Transcription Ground Truth Processing}
\label{par:3_transcription_gound_truth_processing}
The transcription ground truths of the JH dataset are stored in XML files, where parsing these files is sufficient to retrieve the transcription texts. On the other hand, the transcription ground truths of the GW dataset follow a more complex format. In this dataset, individual characters within words are separated by hyphens (\say{ - }), and words themselves are separated by vertical bars (\say{ \textbar \;}). Additionally, punctuation marks are represented by special characters, with a specific table detailing the replacements for each punctuation mark. The table for the punctuation replacement can be found in this \href{https://github.com/Yung-Hsin-Chen/master_thesis/blob/src/model/config/punctuation_list.json}{link}. For instance, consider the transcription from the GW dataset for the phrase "of the Virginia Regiment." This is encoded as 

\begin{center}
    \texttt{o-f|t-h-e|V-i-r-g-i-n-i-a|R-e-g-i-m-e-n-t-s\_pt} 
\end{center}

, where \texttt{s\_pt} is replaced with a period.

\paragraph*{Image Processing}
\label{par:3_image_processing}
In this study, images are resized and normalized prior to being input into the model. Resizing and normalizing images are standard preprocessing steps in image processing, particularly in the context of machine learning and computer vision. Each of these processes serves important purposes in preparing image data for models, enhancing model performance, and ensuring consistency. This process is illustrated in \autoref{fig:2_image_process}.

In this study, images are initially resized to 384x384 pixels to comply with the input requirements of the pretrained TrOCR model. Resizing images is a common preprocessing step in image processing pipelines, particularly for deep learning, where input dimensions need to be consistent across all data samples for the model to process them.

Following resizing, the images undergo normalization. The normalization specifies the mean for each of the three color channels (Red, Green, Blue), which are all set to 0.5. In this case, 0.5 is subtracted from each channel. This value shifts the range of pixel values from [0, 1] to [-0.5, 0.5]. The normalization also sets the standard deviation for each color channel to 0.5. Dividing by 0.5 scales the range from [-0.5, 0.5] to [-1, 1]. This practice is common for neural network inputs as it tends to make the training process more stable and helps it converge faster. Together, resizing and normalizing reposition the data so that its distribution is symmetric around zero, reducing the bias that input values might otherwise introduce into the network's computations. This method ensures that no single pixel range overly influences the network due to its scale, allowing the model to focus more on learning the patterns rather than adapting to the scale of input data.

\fig{images/image_process.png}{fig:2_image_process}{Flowchart illustrating the preprocessing steps for images used in the study.}{11}{Image Processing}

\subsection{Data for Training CharBERT$_{\text{SMALL}}$}
\label{subsec:3_data_for_training_charbert}
In the original study, the authors trained CharBERT using a 12GB dataset from Wikipedia. This training process spanned 5 days, utilizing the computational power of two NVIDIA Tesla V100 GPUs. Given constraints in time and computational resources, our approach involves testing a scaled-down version of CharBERT, which we have designated as CharBERT$_{\text{SMALL}}$. This variant was trained on a significantly smaller dataset, specifically 1.13GB of English Wikipedia data, from which sentences were randomly sampled. For example, sentences like "Tony Macrie has been president of the Seashore Lines since he formed the railroad in 1984." were randomly sampled from articles, providing a rich yet manageable training substrate. This adaptation allows us to evaluate the performance of CharBERT under more restricted conditions, ensuring our experiments are feasible within our available resources.
\TODO{Add eme}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     Model Architecture                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Architecture}
\label{sec:3_composite_model_architecture}
The end-to-end composite model described in this thesis integrates TrOCR and CharBERT to enhance text recognition accuracy. TrOCR is responsible for the initial text recognition task, while CharBERT functions as a corrector, refining the outputs from TrOCR. Specifically, the input to the TrOCR decoder is first corrected by CharBERT before it is processed by the TrOCR decoder. This step ensures that the TrOCR decoder works with embeddings that have been optimized by CharBERT, potentially improving the accuracy of the text recognition. In this section, we will delve into the architectures of both TrOCR and CharBERT, and discuss the methodology behind their integration within the composite model.

As depicted in \autoref{fig:3_composite_model}, the process begins with the image being processed by the TrOCR encoder. Subsequently, the input to the TrOCR decoder is first refined by CharBERT (highlighted in purple) before being fed into the TrOCR Decoder, following the standard transformer architecture.

\fig{images/composite_model.png}{fig:3_composite_model}{Schematic of the integrated workflow in the composite model combining TrOCR and CharBERT. }{6}{Workflow in the Composite Model}

\subsection{Notations}
\label{subsec:3_Notations}
In this report, we denote a one-dimensional tensor (vector) using boldface lowercase letters, while a multi-dimensional tensor appears as boldface uppercase letters. The TrOCR sequence IDs are denoted by $\mathbf{W}$ = {$\mathbf{w}_1, ..., \mathbf{w}_i, ..., \mathbf{w}_D$}, where each $\mathbf{w}_i$ represents a token tokenized by the TrOCR tokenizer, and $D$ signifies the length of the sequence.

\subsection{Recogniser - TrOCR}
\label{subsec:3_recogniser_trocr}
TrOCR is composed of an image encoder and a text decoder. The image encoder takes the pixel values of an image as input, and the encoder output is then fed into the decoder for text generation. This study involves adapting the decoder to integrate with CharBERT. Therefore, it will be crucial to focus on and provide a detailed explanation of its architecture. The following contents will first focus on the inputs of the decoder and the label that is used for loss calculation. Then, we will elaborate on how the inputs are processed through the decoder.

\paragraph*{Decoder Inputs and Labels}
\label{par:3_decoder_inputs_and_labels}
The text decoder takes the encoder output, decoder input IDs, and padding attention mask\myfootnote{Note that the padding attention mask should not be confused with the causal attention mask. The padding attention mask prevents the model from attending to the paddings, while the causal attention mask ensures that the prediction for a specific token in the sequence can only be influenced by previously generated tokens.} as inputs, and outputs the generated text. The decoder output is then compared to the label tensor for loss calculation. This loss is used to update the gradients of the parameters during training.

The encoder output is a set of features extracted from the pixel values of the image. These features represent a transformed version of the input image, capturing not only the patterns but also the arrangements and relationships among elements within the image.

Decoder input IDs are tensors of token IDs converted from the label texts. These include three special tokens: \texttt{<bos>}, \texttt{<eos>}, and padding. The ID \texttt{<bos>}, represented by the number 0, indicates the start of the text, while the ID \texttt{<eos>}, denoted by the number 2, signifies the end of the text. Padding, a series of the number 1, is appended after \texttt{<eos>} to ensure all input IDs are of uniform length. The decoder input IDs, generated automatically by the TrOCR tokenizer, start with \texttt{<bos>} and end with \texttt{<eos>} and padding, mimicking the inference process during training\myfootnote{During inference, the decoder output from the previous step becomes the new decoder input. However, as no output sequence has been generated at the first step, the initial decoder input comprises only the \texttt{<bos>} token. This is processed by the decoder to produce an output, which is then used to update the decoder input.}. The whole process is illustrated in \autoref{fig:3_trocr_inputs}.

The padding attention mask, another input for the decoder, determines which parts of the data the TrOCR should ignore, consisting of 0s and 1s where the 0s cover the padding portions of the decoder input IDs.

The label tensor used for loss calculation is the decoder input IDs minus the \texttt{<bos>} token. Since the TrOCR model is based on the VisionEncoderDecoderModel, it requires label paddings to be set to -100. Thus, the padding token in the label tensor is replaced with -100 instead of 1s.

\fig{images/trocr_inputs.png}{fig:3_trocr_inputs}{Processing pipeline for transforming label text into padded input IDs for the TrOCR model.}{18}{TrOCR Input Processing}

\paragraph*{Inside the Decoder}
\label{par:3_inside_the_decoder}
Let's first consider the original decoder before adapting it to integrate with CharBERT. The decoder receives three primary inputs: the decoder input IDs, the padding attention mask, and the encoder output. Initially, the decoder input IDs are converted into decoder embeddings. These embeddings are then combined with positional encoding, which preserves spatial information, before being processed by the decoder stacks. The decoder stacks include masked multi-head attention, multi-head attention, and a feedforward neural network (FFNN). The padding attention mask is applied during both the masked multi-head attention and multi-head attention stages to prevent the TrOCR from attending to the padding tokens. Finally, the output from the decoder stacks passes through a linear layer and a softmax layer to generate the final output sequence.

\subsection{Corrector - CharBERT}
\label{subsec:3_corrector_charbert}
CharBERT features a dual-channel architecture, comprising token and character channels. The token channel mirrors the structure of BERT or RoBERTa, depending on the initialization. In this study, we will specifically focus on CharBERT\textsubscript{RoBERTa}. Henceforth in this document, any reference to 'CharBERT' will pertain exclusively to CharBERT\textsubscript{RoBERTa}, unless otherwise specified. Unlike BERT and RoBERTa, which output a single embedding, CharBERT generates both token-level and character-level embeddings. Given that the token channel's architecture is identical to that of BERT/RoBERTa, the subsequent sections will concentrate on the character channel and its heterogeneous interactions.

\paragraph*{Character Channel \& Heterogeneous Interaction}
\label{par:3_character_channel_heterogeneous_interaction}
"The character channel of CharBERT begins by splitting the input text into individual characters and converting these characters into IDs via a dictionary lookup. These IDs are then embedded, followed by processing through a bidirectional GRU (Bi-GRU) layer to generate the output embeddings. These embeddings, at both the character and token levels, are subsequently passed through a transformer and undergo heterogeneous interaction, which is comprised of two parts: fusion and divide. The fusion process allows the embeddings to enrich each other through the use of a feedforward neural network (FFNN) and a convolutional neural network (CNN), enhancing their mutual characteristics. Conversely, the divide process ensures that each set of embeddings retains its unique features, facilitated by an FFNN and a residual connection. This residual connection is crucial as it helps preserve the distinct information from each embedding type. CharBERT repeats the transformer and heterogeneous interaction cycles to capture more features and information inherent in the input texts.

\subsection{Composite Model}
\label{subsec:3_composite_model}
The composite model is ingeniously designed to integrate TrOCR and CharBERT. During the inference phase, the decoder output is recycled back as input in a feedback loop. Before this recycled input is fed back into the decoder, it undergoes correction and refinement by CharBERT. Consequently, CharBERT is positioned between the decoder input and the decoder stacks to ensure that the input to the decoder is optimized in each iteration of the process. However, integrating these systems presents several challenges: 1) The TrOCR decoder accepts token IDs as input, whereas CharBERT outputs embeddings; 2) CharBERT requires textual inputs, but the input to the TrOCR decoder is a tensor; 3) The embedding representations of TrOCR do not align with those of CharBERT; 4) The input to the TrOCR decoder is a single tensor, while CharBERT produces dual-channel outputs. The subsequent sections will discuss these issues in detail and explore potential approaches to resolve them.

\paragraph*{Adapted TrOCR}
\label{par:3_adapted_trocr}
Before addressing the first problem, it is crucial to thoroughly understand the issue. The TrOCR decoder is specifically designed to accept token IDs as input, which are then mapped to embeddings. These embeddings are subsequently augmented with positional encoding before being fed into the Transformer decoder. The challenge arises from the outputs of CharBERT, which are embeddings rather than token IDs, leading to a compatibility issue. If we were to resolve this problem by converting CharBERT's embeddings back into token IDs for input into the TrOCR decoder, a new issue surfaces. Using token IDs as intermediaries between model components is problematic because token IDs are integers, while during the training process, the model weights are updated as floating-point numbers. Converting these weights to integers is not practical; while rounding could technically convert floats to integers, this approach would likely distort the learning process and could render the results meaningless for the intended task. Therefore, the only viable solution is to adapt the TrOCR decoder to accept embeddings directly.

A straightforward solution involves repositioning the embedding layer from the TrOCR decoder to precede CharBERT. This adjustment ensures that token IDs are initially converted to TrOCR embeddings, which are then input into CharBERT for correction. Consequently, the adapted TrOCR can accept embeddings directly, bypassing the need for token IDs. This modification not only ensures that the outputs from CharBERT are seamlessly integrated into the TrOCR decoder but also eliminates the use of token IDs between model components. This entire modification process is illustrated in \autoref{fig:3_adapted_trocr}. 

\fig{images/adapted_trocr.png}{fig:3_adapted_trocr}{\textbf{Comparison of the TrOCR Decoder architecture} \\On the left, the original TrOCR Decoder structure includes the Decoder Input, TrOCR Decoder Embedding Layer, and Attention Mechanisms. On the right, the adapted TrOCR Decoder structure shows the repositioning of the TrOCR Decoder Embedding Layer below CharBERT, indicating the integration of CharBERT into the decoder process to enhance input handling before the attention mechanisms.}{12}{Adapted TrOCR}

\paragraph*{Adapted CharBERT}
\label{par:3_adapted_charbert}
"The second problem addresses the mismatch between the data types of input and output. According to the modifications described in \hyperref[par:3_adapted_trocr]{Adapted TrOCR}, the TrOCR decoder input is now an embedding, which should be processed by CharBERT for correction. However, CharBERT traditionally only accepts text as input. Therefore, it becomes essential to develop an adapted version of CharBERT that can handle token and character embeddings directly as inputs. In this revised model, CharBERT no longer converts text into IDs and then into embeddings; instead, it receives pre-processed embeddings directly. This adaptation allows both token and character embeddings to be processed through their respective channels in CharBERT. A comparison between the original and the adapted CharBERT models is illustrated in \autoref{fig:3_adapted_charbert}.

\fig{images/adapted_charbert.png}{fig:3_adapted_charbert}{\textbf{Comparison of the CharBERT architecture}\\On the left, the original CharBERT model processes text into token and character embeddings, which are then fed into the attention mechanisms. On the right, the adapted CharBERT directly receives token and character embeddings as inputs, bypassing the initial text processing stage, and processes them through the same attention mechanisms.}{15}{Adapted CharBERT}

\paragraph*{Tensor Transform}
\label{par:3_tensor_transform}
The third problem presents a more complex challenge. The TrOCR decoder relies on embeddings that are unique to TrOCR. Even for identical texts, the embedding representations generated by TrOCR and CharBERT differ significantly, further complicated by CharBERT's dual-channel embeddings. Not only do the models represent the same text differently, but their embedding dimensions are also incompatible. As a result, even though CharBERT is capable of processing embeddings directly, the TrOCR decoder input cannot be directly integrated into CharBERT without modifications. To overcome this issue, an architecture comprising CNN and FFNN is utilized. This strategy is designed to adjust the dimensions of the TrOCR decoder input to match those of the CharBERT token and character embeddings. Additionally, this dimensional transformation facilitates an effective alignment of representations between TrOCR and CharBERT. To clarify the architecture of the tensor transformation, dimensions will be noted after each input and output, using the format \texttt{(batch size, embedding size)}.

\fig{images/tensor_transform.png}{fig:3_tensor_transform}{\textbf{Architectural flow of the adapted CharBERT and its integration with the TrOCR decoder through tensor transformation}\\The diagram shows how token and character embeddings from the adapted CharBERT are processed through separate tensor transformation pathways to align their dimensions and formats with the requirements of the TrOCR decoder input. Each transformation pathway involves a series of convolutional (Conv) and activation (Act) layers, interspersed with batch normalization (Batch Norm) and further refined by feedforward neural networks (FFNN) and additional activation layers, culminating in the combined decoder input.}{11}{Tensor Transform}

"The objective of the tensor transform architecture is to adjust the dimensions of the TrOCR decoder input, which are initially set at \texttt{(batch size, 512, 1024)}, to match the dimensions required by CharBERT's token embeddings \texttt{(batch size, 510, 768)} and character embeddings \texttt{(batch size, 3060, 256)}. In this architecture, the second dimension represents the sequence length, and the third dimension represents the embedding size. The transformation process is divided into two distinct stages. In the first stage, the decoder input passes through a series of CNN layers, which are interspersed with LeakyReLU activation functions and batch normalization, specifically designed to adjust the sequence dimension (dim=1). Subsequently, the output from the first stage is processed through FFNN layers, with interspersed LeakyReLU activations, in the second stage to modify the embedding dimension (dim=2).

\begin{equation} \label{eq:3_tensor_transform_cnn}
    \begin{split}
        &\mathbf{t}_{1, j} = \text{LeakyReLU}(\mathbf{b}_1 + \sum_{k=1}^{3}\mathbf{W}_{1,k}\cdot \mathbf{e}_{i+k-1})\quad;\quad \mathbf{t}'_1 = \text{Batch\_Norm}(\mathbf{t}_1)\\
        &\mathbf{t}_{2, l} = \text{LeakyReLU}(\mathbf{b}_2 + \sum_{k=1}^{3}\mathbf{W}_{2,k}\cdot \mathbf{t}'_{1,j+k-1})\quad;\quad \mathbf{t}'_2 = \text{Batch\_Norm}(\mathbf{t}_2)\\
        &\mathbf{t}_{3, p} = \text{LeakyReLU}(\mathbf{b}_3 + \sum_{k=1}^{3}\mathbf{W}_{3,k}\cdot \mathbf{t}'_{2,l+k-1})\quad\\
    \end{split}
\end{equation}
After the application of the CNN layers, the dimension of the decoder input is adjusted to (batch size, 510, 1024), aligning the sequence dimension (dim=1) with the desired dimension for the CharBERT token embedding. The CNN layers are specifically chosen for their ability to expand or contract the sequence length while preserving spatial information, making them more suitable than FFNNs for adjusting sequence dimensions. Additionally, it is important to note the inclusion of batch normalization steps between the convolutional layers. These batch normalizations stabilize the deep model and maintain healthier gradients, especially important due to the presence of activation functions\myfootnote{With activation functions in deep networks, gradients can easily explode or vanish. Batch normalization helps maintain a healthier gradient flow in the network, which can improve the efficiency of backpropagation and thereby enhance the learning process.}. Following this, the tensor transformation process moves to the second stage, which utilizes FFNN layers.
\begin{equation} \label{eq:3_tensor_transform_ffnn}
    \begin{split}
        &\mathbf{T}_{4} = \text{LeakyReLU}(\mathbf{b}_4 + \mathbf{W}_4\cdot \mathbf{T}_{3})\\
        &\mathbf{T}_{5} = \text{LeakyReLU}(\mathbf{b}_5 + \mathbf{W}_5\cdot \mathbf{T}_{4})\\
        &\mathbf{T}_{n} = \text{LeakyReLU}(\mathbf{b}_6 + \mathbf{W}_6\cdot \mathbf{T}_{5})\\
    \end{split}
\end{equation}

The resulting tensor, the CharBERT token embedding, is denoted as $\mathbf{T} = {\mathbf{t}_1, ..., \mathbf{t}_n, ..., \mathbf{t}_N}$, where $N$ represents the token sequence length and the tensor size is \texttt{(batch size, 510, 768)}.

Similarly, operations are applied to the TrOCR decoder input but adjusted for different dimension expansion to produce the CharBERT character embedding $\mathbf{C} = {\mathbf{c}_1, ..., \mathbf{c}_m, ..., \mathbf{c}_M}$, where $M$ denotes the character sequence length. It is established that $M$ is typically six times that of $N$, based on the assumption that the average word contains six characters.

The Tensor Transform layer is specifically designed to convert the TrOCR decoder input into CharBERT token and character embeddings. The CNN and FFNN layers are instrumental not only in aligning the dimensions between the tensors but also in learning to map the contextual information from TrOCR embeddings to those of CharBERT, effectively adapting the embeddings for integrated processing.

\paragraph*{Tensor Combine}
\label{par:3_tensor_combine}
The fourth problem arises because CharBERT produces two separate tensors—token and character representations—while the TrOCR decoder input requires a single tensor. To address this, the solution involves combining the two output tensors from CharBERT into a single tensor. Additionally, to stabilize the deep model, a residual connection from the original TrOCR decoder embedding can be added. This residual connection helps to reuse features from the original TrOCR decoder embedding and prevents gradient vanishing.

Among the three tensors involved, two are derived from CharBERT. Consequently, these representations are not inherently compatible with the TrOCR decoder, and their dimensions do not match. Thus, they must first undergo a transformation via the \hyperref[par:3_tensor_transform]{Tensor Transform} process before they can be effectively combined.
\begin{equation} \label{eq:3_tensor_combine_transform}
    \begin{split}
        &\mathbf{T}\in \mathbb{R}^{d_N}\to \mathbf{T}' = \text{Tensor\_Transform}(\mathbf{T})\in \mathbb{R}^{d_D}\\
        &\mathbf{C}\in \mathbb{R}^{d_M}\to \mathbf{C}' = \text{Tensor\_Transform}(\mathbf{C})\in \mathbb{R}^{d_D}\\
    \end{split}
\end{equation}
where $d_D$ = \texttt{(batch size, 512, 1024)}; $d_N$ = \texttt{(batch size, 510, 768)}; $d_M$ = \texttt{(batch size, 3060, 256)}.

After the tensor transform module is applied, the three tensors will all conform to the size \texttt{(batch size, 512, 1024)}, matching the original input size of the TrOCR decoder. These tensors can then be merged and fed into the TrOCR decoder stack. In this study, we explore four different architectures for the tensor combination module: 1) simple addition of all tensors; 2) mean pooling; 3) utilization of linear layers as an attention network; 4) implementation of convolutional layers as an attention network.

\subparagraph*{Tensor Combine 1: Adding}
\label{subpar:3_adding}
The first tensor transform module aggregates three tensors into a single tensor by performing an element-wise addition of the three input tensors. Specifically, this involves the TrOCR decoder embeddings $\mathbf{E}$, the CharBERT transformed token representation $\mathbf{T'}$, and the CharBERT transformed character representation $\mathbf{C'}$. Each of these contributes equally to the formation of the combined tensor, ensuring that the information from each input tensor is weighted equally. The operation can be mathematically expressed as follows:
\begin{equation} \label{eq:3_tensor_combine_adding}
    \begin{split}
        \mathbf{E}'_{1, ijk} = \mathbf{E}_{ijk} + \mathbf{T}'_{ijk} + \mathbf{C}'_{ijk}
    \end{split}
\end{equation}
where $i = 1, 2, ..., \text{batch size}; j = 1, 2, ..., 512; k = 1, 2, ..., 1024$.

\fig{images/combine_1.png}{fig:3_combine_1}{\textbf{Architecture of the Tensor Combine Module 1}\\This diagram illustrates the flow from the TrOCR decoder and CharBERT token and character embeddings through their respective tensor transform processes. The transformed embeddings are then combined using an element-wise addition to form the transformed decoder embeddings, which are subsequently fed back into the TrOCR decoder.}{7.5}{Tensor Combine Module 1}

\subparagraph*{Tensor Combine 2: Mean Pooling}
\label{subpar:3_mean_pooling}
The second tensor combination module is designed to dynamically allocate attention weights to each word across the three input tensors. These weights are derived by performing max pooling along the embedding axis of the three tensors, once they are stacked together. After obtaining these weights, they are applied to the stacked embeddings. It is important to note that all three tensors undergo feature-wise normalization prior to stacking. This normalization aids in faster model convergence and promotes better generalization.
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{E''}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{E}')\\
        &\mathbf{S} = \text{Stack}(\mathbf{E}', \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
        &\mathbf{P} = \frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}_{a,b,c,j}\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\\
        &\mathbf{E}'' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}

\fig{images/combine_2.png}{fig:3_combine_2}{\textbf{Architecture of the Tensor Combine Module 2}\\This module utilizes mean pooling to dynamically allocate attention weights across the three input tensors. After the tensors are stacked, attention weights are derived and applied, followed by mean pooling to merge the embeddings into a single combined embedding, which is then processed by the TrOCR decoder.}{8}{Tensor Combine Module 2}

\subparagraph*{Tensor Combine 3: Linear Layers as Attention Net}
\label{subpar:3_linear_layers_as_attention_net}
"The third tensor combination module builds upon the second module by incorporating linear layers as the attention network. Rather than merely performing mean pooling, this module employs linear layers interspersed with activation functions to more effectively capture relevant information when determining the weights. The process is described as follows:
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{P} = \text{Stack}(\mathbf{E}, \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
    \end{split}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_2}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\quad\:\:\:\: \text{\small \textcolor{RoyalBlue}{Conv Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_1 = \text{LeakyReLU}(\mathbf{b}_7 + \mathbf{W}_7\cdot \mathbf{S}')\\
        &\qquad\mathbf{S}'_2 = \text{LeakyReLU}(\mathbf{b}_8 + \mathbf{W}_8\cdot \mathbf{S}'_1)\\
        &\qquad\mathbf{S}'_3 = \mathbf{b}_9 + \mathbf{W}_9* \mathbf{S}'_2\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
        % &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{\small \textcolor{Cerulean}{Attention Net}}
    \end{split}}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_3}
    \begin{split}
        &\mathbf{P} = [\mathbf{P}_1; \mathbf{P}_2; ...; \mathbf{P}_{512}]\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\qquad\qquad\qquad\qquad\\
        &\mathbf{E}' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}
\subparagraph*{Tensor Combine 4: Convolutional Layers as Attention Net}
\label{subpar:3_convolutional_layers_as_attention_net}
"The fourth tensor combination module substitutes linear layers with convolutional layers in the attention network. The equations below illustrate this attention network, which differs from the one described in \hyperref[subpar:3_linear_layers_as_attention_net]{Tensor Transform - Linear Layers as Attention Net}.
\begin{equation} \label{eq:3_tensor_combine_conv}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\quad\:\:\: \text{\small \textcolor{RoyalBlue}{Linear Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_1 = \text{LeakyReLU}(\mathbf{b}_7 + \mathbf{W}_7\cdot \mathbf{S}')\\
        &\qquad\mathbf{S}'_2 = \text{LeakyReLU}(\mathbf{b}_8 + \mathbf{W}_8\cdot \mathbf{S}'_1)\\
        &\qquad\mathbf{S}'_3 = \mathbf{b}_9 + \mathbf{W}_9* \mathbf{S}'_2\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
    \end{split}}
\end{equation}
\TODO{Change to conv version}

\fig{images/combine_3_4.png}{fig:3_combine_3_4}{\textbf{Architecture of the Tensor Combine Module 3 and 4}\\For each input sequence, the tensors are stacked and transformed through either linear or convolutional layers (detailed in the right two panels) to generate attention weights via a softmax function. The weights are then applied to compute a weighted mean, resulting in an attention-modified combined representation, which is subsequently fed into the TrOCR decoder.}{15}{Tensor Combine Module 3 and 4}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                    Glyph Incorporation                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Glyph Incorporation}
\label{sec:3_glyph_incorporation}
In our methodology, we enhance the training process by specifically targeting commonly misrecognized characters, such as \say{.} and \say{,} or \say{O} and \say{o}, with the aim of reducing the likelihood of these errors in future recognitions. To achieve this, we commence by determining the probability $\mathcal{P}_{ij}$, where $i$ represents the correct character that has been erroneously recognized as character $j$. This strategy is a deviation from the CharBERT NLM training approach, which incorporates character-level errors into the text at random.

By leveraging $\mathcal{P}_{ij}$, we refine our training methodology to introduce errors in a more systematic manner, based on the observed probabilities of specific misrecognitions. This targeted approach allows us to focus the model's learning on correcting these particular errors, enhancing its accuracy and reliability in distinguishing between characters that are commonly confused.

\subsection{Get $\mathcal{P}_{ij}$}
\label{subsec:3_get_pij}
To obtain $\mathcal{P}_{ij}$, it is first necessary to calculate the frequency of each character misrecognized by the recognizer (TrOCR), termed as \emph{misrecognized frequency}. This process begins with employing the basic TrOCR model to process the GW and IAM datasets. After generating text outputs with TrOCR, these are compared with the corresponding text labels to determine the frequencies of characters that were misrecognized.

To align the generated outputs with the labels, we utilize the \say{Bio} package in Python, specifically referring to Biopython. This toolkit is primarily designed for biological computation, handling data such as DNA, RNA, and protein sequences, and offers extensive features for sequence analysis and alignments.

Although Biopython is not inherently designed for processing natural language texts, its tools for sequence matching and regular expressions can be adapted for text analysis applications. For instance, in our context, we can apply Biopython’s methodologies to perform precise character-by-character matching within text strings, identifying and annotating discrepancies. This approach demonstrates how Biopython’s core functionalities can be repurposed for non-biological applications, providing valuable methods for text data analysis and enhancing our understanding of text misrecognition patterns.
\FIG{Add example}

\subsection{Training CharBERT$_{\mathcal{P}_{ij}}$}
\label{subsec:3_training_charbert_pij}
To train CharBERT$_{\text{SMALL}}$, we rigorously follow the methodologies delineated in the original CharBERT study. Specifically, the text undergoes processing through the dual-channel architecture, which encompasses both token and character channels, as elaborated in \href{subsec:3_corrector_charbert}{Subsection 3.3.3}. Additionally, we adhere to the hyperparameter settings outlined in the original CharBERT publication, details of which are available at this \href{https://github.com/mawentao277/CharBERT}{link}.

This adherence ensures that our training regimen remains consistent with established benchmarks, facilitating reliable comparisons and validations of our findings. The dual-channel architecture enables CharBERT to comprehensively analyze textual information at both granular and holistic levels, enhancing its learning efficacy. By maintaining the original hyperparameters, we aim to replicate the environment under which CharBERT was initially validated, thereby ensuring that any deviations in performance or outcomes can be attributed to the differences in training data or integration nuances rather than changes in the fundamental training conditions.
\newchap{Methodology}
\label{chap:3_method}
In this chapter, we will introduce the composite model of TrOCR and CharBERT by leveraging the ideas in candidate fusion mentioned in \Cref{chap:2_related_work}. First, the design concept behind the model will be outlined. Following this, we will elaborate on data collection and preprocessing, and the architectures of the models developed in this study. Next, we will discuss the metric used to measure the performance of the models. Finally, we will delve into the details of the training process for the composite model and other relevant models derived from it.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                       Design Concept                                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Design Concept}
\label{sec:3_design_concept}
CharBERT mitigates the problems of incomplete modelling and fragile representation by including the character-level processing in addition to the subword level information. Furthermore, having noisy language modeling (NLM) as the pre-training task makes CharBERT effective at correcting typos, which is a desired feature for OCR correcting. On the other hand, the candidate fusion paper claims that having an interaction between the recognizer and the language model (LM) can enhance the performance of OCR. Thus, combining TrOCR and CharBERT is expected have an improvement on the OCR accuracy. 

This allows a dynamic feedback loop where the recognizer's outputs are continuously refined through the language model's contextual corrections, improving OCR accuracy. By combining TrOCR's efficient image-to-text conversion with CharBERT's precision in error correction, the integrated system is expected to not only addresses errors but also performs robustly across diverse textual domains, significantly enhancing both the reliability and accuracy of OCR outputs. This approach leverages the strengths of both models, making the combined system exceptionally versatile and effective in various OCR scenarios.

Building on the design concepts detailed above, we now shift our focus to empirically validating these ideas. We have formulated specific research questions for this purpose. To address the \textcolor{Blue}{research questions (RQs)}, we have designed several experiments (EXs), which will be discussed in detail in \Cref{chap:4_experiment}. The corresponding research questions and their associated experiments are outlined below:

\begin{itemize}
    \item[\textcolor{Blue}{\textbf{RQ1}}] \textcolor{Blue}{\textbf{Influence of Language Model on Decoder Output: }Does TrOCR decoder change its output with the presence of the LM?}
    \item[\textbf{EX1}] Different combinations of composite model layers will be tested to optimize performance. We will examine whether the text output of this optimized composite model differs from that produced by TrOCR alone.

    \vspace{7mm}
    \item[\textcolor{Blue}{\textbf{RQ2}}] \textcolor{Blue}{\textbf{Character-Level Information Enhancement:} In the CharBERT paper \citep{ma-etal-2020-charbert}, they claimed that, by using the character level information in addition to the subword level information, \hyperref[sec:2_charbert]{the problems of incomplete modelling and fragile 
    representation} can be solved. However, the results shown in the paper did not show significant performance improvement over RoBERTa (a strong baseline LM model). Is this statement valid? Can TrOCR combined with CharBERT achieve the claim?}
    \item[\textbf{EX2}] Since TrOCR utilizes primarily sub-token level information,  the performance of TrOCR combined CharBERT will be compared with TrOCR to ascertain if character-level information substantively enhances model effectiveness.

    \vspace{7mm}
    \item[\textcolor{Blue}{\textbf{RQ3}}] \textcolor{Blue}{\textbf{Domain Adaptation Through Model Fusion:} In the candidate fusion paper , they claimed that fusing the recognizer and the LM can make the LM adjust to the domain-specific data. Can fusing TrOCR and CharBERT achieve the same conclusion? In other words, can CharBERT adjust to historical texts even it was trained on modern texts?}
    \item[\textbf{EX3}] We will train CharBERT on a limited dataset of historical texts (hereafter referred to as historical CharBERT$_{\text{SMALL}}$) and modern English data (referred to as modern CharBERT$_{\text{SMALL}}$, or simply CharBERT$_{\text{SMALL}}$), and then combine it with TrOCR. The historical CharBERT$_{\text{SMALL}}$ will be used to test this research hypothesis. To support this claim, TrOCR-CharBERT$_{\text{SMALL}}$ should perform at least comparably to TrOCR-historical CharBERT$_{\text{SMALL}}$. Given that historical CharBERT$_{\text{SMALL}}$ is trained on a smaller dataset, it would be unfair to compare it directly with TrOCR-CharBERT, which is trained on a substantially larger dataset. Therefore, modern CharBERT$_{\text{SMALL}}$ will serve as the baseline for comparison.

    \vspace{7mm}
    \item[\textcolor{Blue}{\textbf{RQ4}}] \textcolor{Blue}{\textbf{Impact of Training on Common Errors:} According to \cite{kang2021candidate}, the language model can adapt by learning from the frequent errors generated by the OCR recognizer, thereby enhancing the overall accuracy of the system. If we intentionally integrate common errors made by TrOCR into the training process of CharBERT, do the results show improvements?}
    \item[\textbf{EX4}] We will identify common errors from TrOCR and integrate these into CharBERT's training process to evaluate whether this enhances the composite model's performance compared to training with randomly introduced errors. his specially trained CharBERT with limited data will be called CharBERT$_{\mathcal{P}{ij}}$. As with RQ3, CharBERT$_{\text{SMALL}}$ will serve as a baseline for comparison with CharBERT$_{\mathcal{P}_{ij}}$.
\end{itemize}

This structured approach ensures a thorough evaluation of how well integrated OCR systems can leverage both deep learning architectures and advanced language models to improve text recognition accuracy across diverse datasets.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                              Data Collection and processing                                %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Data Collection and Processing}
\label{sec:3_data_collection_and_processing}
This section is dedicated to detailing the data collection process for composite model training and CharBERT$_{\text{SMALL}}$ training. We will outline the types and sources of data harnessed for this study, emphasizing the diversity and volume of the datasets to ensure comprehensive learning. Following the data collection overview, we will delve into the processing techniques applied to the collected data.
\subsection{Data for OCR} 
\label{subsec:3_data_for_ocr}
We focus on the performances of the composite model on handwritten datasets. The data used in this study is the George Washington (GW) handwritten dataset\myfootnote{The George Washington handwritten dataset can be downloaded here: \url{https://fki.tic.heia-fr.ch/databases/washington-database}} and the Joseph Hooker (JH) handwritten dataset\myfootnote{The Joseph Hooker handwritten dataset can be downloaded here: \url{https://github.com/jschaefer738b/JosephHookerHTR}}. They serve as valuable benchmarks for developing and evaluating handwriting recognition systems. We selected the GW and JH datasets because they represent different periods of English with different topic domains, making them suitable for our experiments that require English from different time and knowledge domains. Note that to ensure comparability with existing studies, this research adheres to the established train-validation-test splits of the GW datasets. Detailed information about these datasets is presented in \Cref{tab:3_data_info}.

Although the IAM dataset is a more contemporary collection of English handwriting samples, renowned for its diversity in handwriting styles, this study did not use it since TrOCR is pre-trained on the IAM dataset. The most significant risk of fine-tuning the composite model on the same dataset is overfitting. The model may become excessively tailored to the IAM dataset, performing well on this specific dataset but poorly on unseen, real-world data or other similar datasets. In addition, if the model has already been trained to a point where it performs optimally on the IAM dataset, further training the composite model on the same data might yield diminishing returns. The model may not improve significantly and could even start to fit the noise in the dataset as mentioned above.

\paragraph*{George Washington Dataset}
\label{par:3_george_washington_dataset}
The GW dataset is a collection of historical letters and diaries handwritten by George Washington and his secretaries in 1755. This dataset comprises various types of documents, including personal correspondence, military orders, and official communications, which provide a unique window into the early American colonial period. The dataset is frequently utilized in research focused on recognizing historical handwriting, which poses unique challenges due to the use of archaic words and phrases. This dataset not only supports the preservation and accessibility of these important texts but also aids in advancing the technology needed to interpret and digitize aged manuscripts. An example from the GW dataset is shown in \Cref{fig:3_gw_line_image}.

\fig{images/gw_line_image.png}{fig:3_gw_line_image}{Sample of handwritten text from GW dataset. The line image reads: only for the publick use, unless by particu.}{11.5}{GW Line Image}

\paragraph*{Joseph Hooker Dataset}
\label{par:3_joseph_hooker_dataset}
The JH dataset consists of correspondence between Joseph Dalton Hooker (1817-1911) and other leading scientists of his time. This collection offers insights into the scientific dialogues of the era, featuring interactions with notable figures such as Charles Darwin. As such, it is an invaluable resource for historians specializing in science, botany, and the Victorian period. Hooker’s letters are diverse in content, extending beyond mere botanical discussions to include observations from his travels, his perspectives on contemporary scientific debates, his personal reflections, and his professional communications. This rich variety provides a comprehensive view of his intellectual engagement and contributions. An example from the JH dataset is shown in \Cref{fig:3_jh_line_image}.

\fig{images/jh_line_image.jpg}{fig:3_jh_line_image}{Sample of handwritten text from JH dataset. The line image reads: I will send them to the host to be.}{11}{JH Line Image}

\footnotetab{tab:3_data_info}{Data Information}{%
\begin{threeparttable}
\begin{tabular}{lrr}
    \toprule
    Metric	                                  & GW Dataset     & JH Dataset	\\
    \midrule
    Text Line		                          & 656            &6916	\\
    Train Data                                &329             &5532\\
    Validation Data                           &168             &691	\\
    Test Data                                 &163             &693	\\
    Unique Word instances\mytnote{3}          &1456            &8308	\\
    Unique letters\mytnote{4}                 &68              &	84\\
    Average Text Line length\mytnote{5}       &40.23           &28.45	\\
    Average Word length\mytnote{6}            &6.17            &6.89	\\
    Percentage of Non-Character\mytnote{7}    &21\%            &	22.4\%\\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
        \item[3] {\footnotesize {\fontfamily{cmus}\selectfont The count of distinct words present in the dataset.}}
        \item[4] {\footnotesize {\fontfamily{cmus}\selectfont The number of distinct characters present in the dataset \\(include punctuations).}}
        \item[5] {\footnotesize {\fontfamily{cmus}\selectfont The average number of characters (letters and spaces) per text line.}}
        \item[6] {\footnotesize {\fontfamily{cmus}\selectfont The average number of characters per word in the dataset.}}
        \item[7] {\footnotesize {\fontfamily{cmus}\selectfont The percentage of characters in the dataset that are not part of the standard alphabet. This may include punctuation marks and other non-alphabetic characters. (characters other than A-Z, a-z, 0-9)}}
    \end{tablenotes}
\end{threeparttable}
}{GW and JH Data Information}

The Joseph Hooker and George Washington datasets, while serving distinct academic purposes, both play significant roles in the realm of handwritten text recognition (HTR) tasks. The Joseph Hooker dataset, with its extensive collection of 19th-century botanical writings and correspondence, provides a unique challenge for HTR technologies due to the scientific terminology and personal handwriting styles found in the documents. This variety enables developers to fine-tune HTR models to handle complex vocabulary and diverse manuscript formats, which is essential for digital humanities projects focusing on scientific archives.

Similarly, the George Washington dataset, consisting of an array of 18th-century materials including letters, diaries, and official documents, presents its own set of challenges for HTR. GW dataset helps in training HTR systems to recognize older forms of English script and the idiosyncrasies of historical American handwriting, which are crucial for preserving and making accessible key documents from the formative years of the United States.

In both cases, these datasets not only support the preservation and accessibility of historical texts but also advance the development of HTR systems. They enable the refinement of algorithms capable of interpreting a wide range of handwritten styles, thus broadening the utility of HTR technologies across different historical and scientific domains.

\paragraph*{Transcription Ground Truth Processing}
\label{par:3_transcription_gound_truth_processing}
The transcription ground truths of the JH dataset are stored in XML files, where parsing these files is sufficient to retrieve the transcription texts. On the other hand, the transcription ground truths of the GW dataset follow a more complex format. In this dataset, individual characters within words are separated by hyphens (\say{ - }), and words themselves are separated by vertical bars (\say{ \textbar \;}). Additionally, punctuation marks are represented by special characters. The table for the punctuation replacement can be found in this \href{https://github.com/Yung-Hsin-Chen/master_thesis/blob/src/model/config/punctuation_list.json}{link}. 

For instance, consider the transcription from the GW dataset for the phrase \say{of the Virginia Regiment.} This is encoded as 

\begin{center}
    \texttt{o-f|t-h-e|V-i-r-g-i-n-i-a|R-e-g-i-m-e-n-t-s\_pt} 
\end{center}

, where \texttt{s\_pt} is replaced with a period.

\paragraph*{Image Processing}
\label{par:3_image_processing}
In this study, images are resized and normalized prior to being input into the model. Resizing and normalizing images are standard preprocessing steps in image processing, particularly in the context of machine learning and computer vision. Each of these processes serves important purposes in preparing image data for models, enhancing model performance, and ensuring consistency. This process is illustrated in \Cref{fig:2_image_process}.

In this study, images are initially resized to 384x384 pixels to comply with the input requirements of the pretrained TrOCR model. Resizing images is a common preprocessing step in image processing pipelines, particularly for deep learning, where input dimensions need to be consistent across all data samples for the model to process them.

Following resizing, the images undergo normalization. The normalization specifies the mean for each of the three color channels (Red, Green, Blue), which are all set to 0.5. In this case, 0.5 is subtracted from each channel. This value shifts the range of pixel values from [0, 1] to [-0.5, 0.5]. The normalization also sets the standard deviation for each color channel to 0.5. Dividing by 0.5 scales the range to [-1, 1]. This practice is common for neural network inputs as it tends to make the training process more stable and helps it converge faster. Together, resizing and normalizing reposition the data so that its distribution is symmetric around zero, reducing the bias that input values might otherwise introduce into the network's computations. This method ensures that no single pixel range overly influences the network due to its scale, allowing the model to focus more on learning the patterns rather than adapting to the scale of input data.

\fig{images/image_process.png}{fig:2_image_process}{Flowchart illustrating the preprocessing steps for images used in the study.}{11}{Image Processing}

\subsection{Data for Training CharBERT$_{\text{SMALL}}$}
\label{subsec:3_data_for_training_charbert}
In the original study, the authors developed CharBERT using a substantial 12GB dataset sourced from Wikipedia. This training was conducted over five days, harnessing the computational capabilities of two NVIDIA Tesla V100 GPUs. Due to constraints in time and available computational resources, our study employs a scaled-down version of CharBERT, hereafter referred to as CharBERT$_{\text{SMALL}}$. This variant has been trained on two considerably smaller datasets: 1.13GB of English Wikipedia data and 637MB of literature from the 16$^{\text{th}}$ to 19$^{\text{th}}$ centuries, respectively termed as modern CharBERT$_{\text{SMALL}}$ and historical CharBERT$_{\text{SMALL}}$.

The data from 16$^{\text{th}}$ to 19$^{\text{th}}$ centuries are downloaded from the following resources:

\begin{itemize}
    \setlength\itemsep{0em}
    % \item \href{https://llds.ling-phil.ox.ac.uk/llds/xmlui/handle/20.500.14106/2507}{A Corpus of English Dialogues 1560-1760 (CED)} \\
    \item A Corpus of English Dialogues 1560-1760 (CED) \citep{20.500.14106/2507} (\href{https://llds.ling-phil.ox.ac.uk/llds/xmlui/handle/20.500.14106/2507}{link})
    \item Pamphlets of the American Revolution \citep{20.500.14106/2021} (\href{http://hdl.handle.net/20.500.14106/2021}{link})
    \item Parsed Corpus of Early English Correspondence (PCEEC) \citep{20.500.14106/2510} (\href{http://hdl.handle.net/20.500.14106/2510}{link})
    \item Royal Society Corpus (Version 4.0) \citep{kermes2016royal} (\href{http://hdl.handle.net/21.11119/0000-0001-7E8B-6}{link})
    \item The English language of the north-west in the late Modern English period: a Corpus of late 18c Prose \citep{20.500.14106/2468} (\href{http://hdl.handle.net/20.500.14106/2468}{link})
    \item The Lampeter Corpus of Early Modern English Tracts \citep{20.500.14106/3193} (\href{http://hdl.handle.net/20.500.14106/3193}{link})
    \item The Old Bailey Corpus \citep{hubernisselpuga2016} (\href{http://hdl.handle.net/11858/00-246C-0000-0023-8CFB-2}{link})
\end{itemize}

The training process for modern CharBERT$_{\text{SMALL}}$ involved randomly sampled sentences from the Wikipedia dataset, such as \say{Tony Macrie has been president of the Seashore Lines since he formed the railroad in 1984.} This approach provided a diverse yet manageable corpus for training, ensuring a broad representation of modern English usage. This model serves as a baseline for comparisons with two other implementations of CharBERT: CharBERT$_{\mathcal{P}{ij}}$ —- which incorporates common OCR errors into the model and will be detailed later —- and historical CharBERT$_{\text{SMALL}}$. Training CharBERT with a smaller dataset allows for fair comparisons with other models also trained on limited data.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                           Metric                                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Metric}
\label{sec:5_metric}
The metrics used in this study, character error rate (CER) and word error rate (WER), are commonly employed to evaluate the performance of systems in tasks related to optical character recognition (OCR). Both metrics assess how accurately a system transcribes or recognizes text compared to a ground truth transcription. While CER measures accuracy at the character level, WER evaluates it at the word level.

The primary difference between CER and WER lies in their granularity. CER offers a more fine-grained evaluation, making it particularly useful for tasks where character-level precision is essential, such as OCR. WER, on the other hand, is better suited for scenarios where word-level inaccuracies significantly impact the interpretability of the transcribed text, such as in speech recognition tasks.

Another distinction is their sensitivity to errors. CER is highly sensitive to minor errors, which is critical in situations where a single character change can alter the meaning of a word dramatically. For example, changing \say{hat} to \say{hot} by substituting \say{a} with \say{o} completely changes the word's meaning. CER effectively captures these subtle yet crucial errors, proving indispensable for tasks requiring meticulous character-level accuracy.

In contrast, WER focuses on capturing errors that affect the reader's or listener's ability to understand the intended message at the word level. An illustrative example is the digitization of a restaurant menu where the original text, \say{Chicken with Lemon Sauce,} was misinterpreted by the OCR system as \say{Chicken with Lemon Source.} This error, which results in a WER of 20\%, does not significantly diminish the readability of the item but introduces a notable semantic error. The term \say{Sauce,} referring to a culinary liquid, is replaced by \say{Source,} which implies origin, potentially misleading readers and affecting their understanding of the menu item. Thus, WER highlights how even minor word-level errors can impact the semantic integrity of the content.

Despite these differences, both CER and WER are crucial for benchmarking and improving models in OCR, speech recognition, and similar domains. They enable developers to identify shortcomings, compare different model architectures, and track progress over time.

\subsection{Character Error Rate (CER)}
\label{subsec:5_cer}
CER is typically expressed as a percentage that represents the proportion of characters incorrectly predicted by the system. It is calculated based on the number of character-level operations required to transform the system output into the ground truth text. These operations include insertions, deletions, and substitutions of characters.

The formula for calculating CER is as follows:

\begin{equation}
    \text{CER} = \frac{I+D+S}{N}
\end{equation}

In this equation:
\begin{itemize}
    \item $I$ represents the number of insertions required,
    \item $D$ denotes the number of deletions,
    \item $S$ signifies the number of substitutions needed to achieve a match with the ground truth,
    \item $N$ is the total number of characters in the ground truth text.
\end{itemize}

This method quantifies the system's accuracy at the character level, highlighting its precision in detail.

\subsection{Word Error Rate (WER)}
\label{subsec:5_wer}
WER is calculated in a manner similar to CER but focuses on the word level. It measures the number of word-level insertions, deletions, and substitutions required to transform the system's output into the ground truth.

The formula for calculating WER is as follows:

\begin{equation}
    \text{WER} = \frac{I+D+S}{N}
\end{equation}

In this equation:
\begin{itemize}
    \item $I$ represents the number of insertions needed,
    \item $D$ denotes the number of deletions,
    \item $S$ signifies the number of substitutions necessary to match the ground truth,
    \item $N$ s the total number of words in the ground truth text.
\end{itemize}

This calculation provides a quantitative measure of the system's performance at recognizing or transcribing words accurately, offering insights into its effectiveness in understanding and processing language at the word level.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     Model Architecture                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Architecture}
\label{sec:3_composite_model_architecture}
The end-to-end composite model described in this thesis integrates TrOCR and CharBERT to enhance text recognition accuracy. TrOCR is responsible for the initial text recognition task, while CharBERT functions as a corrector, refining the outputs from TrOCR. Specifically, the input to the TrOCR decoder is first corrected by CharBERT before it is processed by the TrOCR decoder. This step ensures that the TrOCR decoder works with embeddings that have been optimized by CharBERT, potentially improving the accuracy of the text recognition. In this section, we will delve into the architectures of both TrOCR and CharBERT, and discuss the methodology behind their integration within the composite model.

As depicted in \Cref{fig:3_composite_model}, the process begins with the image being processed by the TrOCR encoder. Subsequently, the input to the TrOCR decoder is first refined by CharBERT (highlighted in purple) before being fed into the TrOCR Decoder, following the standard transformer decoder architecture.

\fig{images/composite_model.png}{fig:3_composite_model}{Schematic of the integrated workflow in the composite model combining TrOCR and CharBERT. }{6}{Workflow in the Composite Model}

\subsection{Notations}
\label{subsec:3_Notations}
In this report, we denote a one-dimensional tensor (vector) using boldface lowercase letters, while a multi-dimensional tensor appears as boldface uppercase letters. The TrOCR sequence IDs are denoted by $\mathbf{W}$ = {$\mathbf{w}_1, ..., \mathbf{w}_i, ..., \mathbf{w}_D$}, where each $\mathbf{w}_i$ represents a token representation tokenized and processed by the TrOCR processor, and $D$ signifies the length of the sequence.

\subsection{Recogniser - TrOCR}
\label{subsec:3_recogniser_trocr}
TrOCR is composed of an image encoder and a text decoder. The image encoder takes the pixel values of an image as input, and the encoder output is then fed into the decoder for text generation. This study involves adapting the decoder to integrate with CharBERT. Therefore, it will be crucial to focus on and provide a detailed explanation of its architecture. The following contents will first focus on the inputs of the decoder and the label that is used for loss calculation during training. Then, we will elaborate on how the inputs are processed through the decoder.

\paragraph*{Decoder Inputs and Labels During Training}
\label{par:3_decoder_inputs_and_labels}
The text decoder takes the encoder output, decoder input IDs, and padding attention mask\myfootnote{Note that the padding attention mask should not be confused with the causal attention mask. The padding attention mask prevents the model from attending to the paddings, while the causal attention mask ensures that the prediction for a specific token in the sequence can only be influenced by previously generated tokens.} as inputs, and outputs the generated text. The decoder output is then compared to the label tensor for loss calculation. This loss is used to update the gradients of the parameters during training.

The encoder output is a set of features extracted from the pixel values of the image. These features represent a transformed version of the input image, capturing not only the patterns but also the arrangements and relationships among elements within the image.

Decoder input IDs are tensors of token IDs converted from the label texts. These include three special tokens: \texttt{<bos>}, \texttt{<eos>}, and the padding. The ID \texttt{<bos>}, represented by the number 0, indicates the start of the text, while the ID \texttt{<eos>}, denoted by the number 2, signifies the end of the text. Padding, a series of the number 1, is appended after \texttt{<eos>} to ensure all input IDs are of uniform length. The decoder input IDs, generated automatically by the TrOCR tokenizer, start with \texttt{<bos>} and end with \texttt{<eos>} and padding, mimicking the inference process during training\myfootnote{During inference, the decoder output from the previous step becomes the new decoder input. However, as no output sequence has been generated at the first step, the initial decoder input comprises only the \texttt{<bos>} token. This is processed by the decoder to produce an output, which is then used to update the decoder input.}. 

The padding attention mask, another input for the decoder, determines which parts of the data the TrOCR should ignore, consisting of 0s and 1s where the 0s cover the padding portions of the decoder input IDs.

The label tensor used for loss calculation is the decoder input IDs minus the \texttt{<bos>} token. Since the TrOCR model is based on the VisionEncoderDecoderModel, it requires label paddings to be set to -100. Thus, the padding token in the label tensor is replaced with -100 instead of 1s in practice. The whole process is illustrated in \Cref{fig:3_trocr_inputs}.

\fig{images/trocr_inputs.png}{fig:3_trocr_inputs}{Processing pipeline for transforming label text into padded input IDs for the training of TrOCR model.}{16}{TrOCR Input Processing}

\paragraph*{Inside the Decoder}
\label{par:3_inside_the_decoder}
Let's first consider the original decoder before adapting it to integrate with CharBERT. The decoder receives three primary inputs: the decoder input IDs, the padding attention mask, and the encoder output. Initially, the decoder input IDs are converted into decoder embeddings. These embeddings are then combined with positional encoding, which preserves spatial information, before being processed by the decoder stacks. The decoder stacks include masked multi-head attention, multi-head attention, and a feedforward neural network (FFNN). The padding attention mask is applied during both the masked multi-head attention and multi-head attention stages to prevent the TrOCR from attending to the padding tokens. Finally, the output from the decoder stacks passes through a linear layer and a softmax layer to generate the final output sequence.

\subsection{Corrector - CharBERT}
\label{subsec:3_corrector_charbert}
CharBERT features a dual-channel architecture, comprising token and character channels. The token channel mirrors the configuration of BERT or RoBERTa, depending on the initialization. In this study, we will specifically focus on CharBERT\textsubscript{RoBERTa}. Henceforth in this document, any reference to \say{CharBERT} will pertain exclusively to CharBERT\textsubscript{RoBERTa}, unless otherwise specified. Unlike BERT and RoBERTa, which output a single embedding, CharBERT generates both token-level and character-level embeddings. Given that the token channel's architecture is identical to that of BERT or RoBERTa, the following contents will concentrate on the character channel and the heterogeneous interactions.

\paragraph*{Character Channel \& Heterogeneous Interaction}
\label{par:3_character_channel_heterogeneous_interaction}
The character channel of CharBERT begins by splitting the input text into individual characters and converting these characters into IDs via a dictionary lookup. These IDs are then embedded, followed by processing through a bidirectional GRU (Bi-GRU) layer to generate the output embeddings. These embeddings, at both the character and token levels, are subsequently passed through a transformer and undergo a heterogeneous interaction, which is comprised of two parts: fusion and divide. The fusion process allows the embeddings to enrich each other through the use of a FFNN and a convolutional neural network (CNN), enhancing their mutual characteristics. Conversely, the divide process ensures that each set of embeddings retains its unique features, facilitated by an FFNN and a residual connection. This residual connection is crucial as it helps preserve the distinct information from each embedding type. CharBERT repeats the transformer and heterogeneous interaction cycles to capture more features and information inherent in the input texts.

\subsection{Composite Model}
\label{subsec:3_composite_model}
The composite model is ingeniously designed to integrate TrOCR and CharBERT. During the inference phase, the decoder output is recycled back as input in a feedback loop. Before this recycled input is fed back into the decoder, it undergoes correction and refinement by CharBERT. Consequently, CharBERT is positioned between the decoder input and the decoder stacks to ensure that the input to the decoder is optimized in each iteration of the process. However, integrating these systems presents several challenges: 1) The TrOCR decoder accepts token IDs as input, whereas CharBERT outputs embeddings; 2) CharBERT requires textual inputs, but the TrOCR decoder input is a tensor; 3) The embedding representations of TrOCR do not align with those of CharBERT; 4) The input to the TrOCR decoder is a single tensor, while CharBERT produces dual-channel outputs (token and character channel outputs). These challenges are illustrated in \Cref{fig:3_composite_challenge}. The following contents will discuss these issues in detail and explore potential approaches to resolve them.

\fig{images/composite_challenge.png}{fig:3_composite_challenge}{This diagram illustrates the data flow between TrOCR decoder and CharBERT. Key integration challenges are highlighted: (1) The mismatch between the token IDs accepted by the TrOCR decoder and the character and token embeddings produced by CharBERT, (2) The requirement of CharBERT for textual inputs versus the tensor format of the TrOCR decoder input, (3) The discrepancy in embedding representations between TrOCR and CharBERT, and (4) The single tensor input of the TrOCR decoder contrasted with the dual-channel (token and character) outputs of CharBERT. }{8.5}{Challenges of Composite Model Integration}

\paragraph*{Adapted TrOCR}
\label{par:3_adapted_trocr}
Before addressing the first problem, it is crucial to thoroughly understand the issue. The TrOCR decoder is specifically designed to accept token IDs as input, which are then mapped to embeddings. These embeddings are subsequently augmented with positional encoding before being fed into the Transformer decoder. The challenge arises from the outputs of CharBERT, which are embeddings rather than token IDs, leading to a compatibility issue. If we were to resolve this problem by converting CharBERT's embeddings back into token IDs for input into the TrOCR decoder, a new issue surfaces. Using token IDs as intermediaries between model components is problematic because token IDs are integers, while during the training process, the model weights are updated as floating-point numbers. Converting these weights to integers is not practical; while rounding could technically convert floats to integers, this approach would likely distort the learning process and could render the results meaningless for the intended task. Therefore, the only viable solution is to adapt the TrOCR decoder to accept embeddings directly.

A straightforward solution involves repositioning the embedding layer from the TrOCR decoder to precede CharBERT. This adjustment ensures that token IDs are initially converted to TrOCR embeddings, which are then input into CharBERT for correction. Consequently, the adapted TrOCR can accept embeddings directly, bypassing the need for token IDs. This modification not only ensures that the outputs from CharBERT are seamlessly integrated into the TrOCR decoder but also eliminates the use of token IDs between model components. This entire modification process is illustrated in \Cref{fig:3_adapted_trocr}. 

\fig{images/adapted_trocr.png}{fig:3_adapted_trocr}{\textbf{Comparison of the TrOCR Decoder architecture} \\On the left, the original TrOCR Decoder structure includes the Decoder Input, TrOCR Decoder Embedding Layer, and Attention Mechanisms. On the right, the adapted TrOCR Decoder structure shows the repositioning of the TrOCR Decoder Embedding Layer below CharBERT, indicating the integration of CharBERT into the decoder process to enhance input handling before the attention mechanisms.}{12}{Adapted TrOCR}

\paragraph*{Adapted CharBERT}
\label{par:3_adapted_charbert}
The second problem addresses the mismatch between the data types of input and output. According to the modifications described in \hyperref[par:3_adapted_trocr]{Adapted TrOCR}, the TrOCR decoder input is now an embedding, which should be processed by CharBERT for correction. However, CharBERT traditionally only accepts text as input. Therefore, it becomes essential to develop an adapted version of CharBERT that can handle token and character embeddings directly as inputs. In this revised model, CharBERT no longer converts text into IDs and then into embeddings; instead, it receives pre-processed embeddings directly. This adaptation allows both token and character embeddings to be processed through their respective channels in CharBERT. A comparison between the original and the adapted CharBERT models is illustrated in \Cref{fig:3_adapted_charbert}.

\fig{images/adapted_charbert.png}{fig:3_adapted_charbert}{\textbf{Comparison of the CharBERT architecture}\\On the left, the original CharBERT model processes text into token and character embeddings, which are then fed into the attention mechanisms. On the right, the adapted CharBERT directly receives token and character embeddings as inputs, bypassing the initial text processing stage, and processes them through the same attention mechanisms.}{15}{Adapted CharBERT}

\paragraph*{Tensor Transform}
\label{par:3_tensor_transform}
The third problem presents a more complex challenge. The TrOCR decoder relies on embeddings that are unique to TrOCR. Even for identical texts, the embedding representations generated by TrOCR and CharBERT differ significantly, further complicated by CharBERT's dual-channel embeddings. Not only do the models represent the same text differently, but their embedding dimensions are also incompatible. As a result, even though CharBERT is capable of processing embeddings directly, the TrOCR decoder input cannot be directly integrated into CharBERT without modifications. To overcome this issue, an architecture comprising CNN and FFNN is utilized. This strategy is designed to adjust the dimensions of the TrOCR decoder input to match those of the CharBERT token and character embeddings. Additionally, this dimensional transformation facilitates an effective alignment of representations between TrOCR and CharBERT. To clarify the architecture of the tensor transformation, dimensions will be noted after each input and output, using the format \texttt{(batch size, sequence length, embedding size)}.

\fig{images/tensor_transform.png}{fig:3_tensor_transform}{\textbf{Architectural flow of the adapted CharBERT and its integration with the TrOCR decoder through tensor transformation}\\The diagram shows how token and character embeddings from the adapted CharBERT are processed through separate tensor transformation pathways to align their dimensions and formats with the requirements of the TrOCR decoder input. Each transformation pathway involves a series of convolutional (Conv) and activation (Act) layers, interspersed with batch normalization (Batch Norm) and further refined by FFNN and additional activation layers, culminating in the combined decoder input.}{11}{Tensor Transform}

The objective of the tensor transformation architecture is to adjust the dimensions of the TrOCR decoder input, which are initially set at \texttt{(batch size, 512, 1024)}, to match the dimensions required by CharBERT's token channel input \texttt{(batch size, 510, 768)} and character channel input \texttt{(batch size, 3060, 256)}. In this architecture, the second dimension represents the sequence length, and the third dimension represents the embedding size. The transformation process is divided into two distinct stages. In the first stage, the decoder input passes through a series of CNN layers, which are interspersed with LeakyReLU activation functions and batch normalization, specifically designed to adjust the sequence dimension (dim=1). Subsequently, the output from the first stage is processed through FFNN layers, with interspersed LeakyReLU activations, in the second stage to modify the embedding dimension (dim=2). Here, $e$ represents the TrOCR decoder input.

\begin{equation} \label{eq:3_tensor_transform_cnn}
    \begin{split}
        &\mathbf{t}_{1, j} = \text{LeakyReLU}(\mathbf{b}_1 + \sum_{k=1}^{3}\mathbf{W}_{1,k}\cdot \mathbf{e}_{i+k-1})\quad;\quad \mathbf{t}'_1 = \text{Batch\_Norm}(\mathbf{t}_1)\\
        &\mathbf{t}_{2, l} = \text{LeakyReLU}(\mathbf{b}_2 + \sum_{k=1}^{3}\mathbf{W}_{2,k}\cdot \mathbf{t}'_{1,j+k-1})\quad;\quad \mathbf{t}'_2 = \text{Batch\_Norm}(\mathbf{t}_2)\\
        &\mathbf{t}_{3, p} = \text{LeakyReLU}(\mathbf{b}_3 + \sum_{k=1}^{3}\mathbf{W}_{3,k}\cdot \mathbf{t}'_{2,l+k-1})\quad\\
    \end{split}
\end{equation}

After the application of the CNN layers, the dimension of the decoder input is adjusted to \texttt{(batch size, 510, 1024)}, aligning the sequence dimension (dim=1) with the desired dimension for the CharBERT token embedding. The CNN layers are specifically chosen for their ability to expand or contract the sequence length while preserving spatial information, making them more suitable than FFNNs for adjusting sequence dimensions. Additionally, it is important to note the inclusion of batch normalization steps between the convolutional layers. These batch normalizations stabilize the deep model and maintain healthier gradients, especially important due to the presence of activation functions\myfootnote{With activation functions in deep networks, gradients can easily explode or vanish. Batch normalization helps maintain a healthier gradient flow in the network, which can improve the efficiency of backpropagation and thereby enhance the learning process.}. Following this, the tensor transformation process moves to the second stage, which utilizes FFNN layers.
\begin{equation} \label{eq:3_tensor_transform_ffnn}
    \begin{split}
        &\mathbf{T}_{4} = \text{LeakyReLU}(\mathbf{b}_4 + \mathbf{W}_4\cdot \mathbf{T}_{3})\\
        &\mathbf{T}_{5} = \text{LeakyReLU}(\mathbf{b}_5 + \mathbf{W}_5\cdot \mathbf{T}_{4})\\
        &\mathbf{T}_{n} = \text{LeakyReLU}(\mathbf{b}_6 + \mathbf{W}_6\cdot \mathbf{T}_{5})\\
    \end{split}
\end{equation}

The resulting tensor, the CharBERT token embedding, is denoted as $\mathbf{T} = {\mathbf{t}_1, ..., \mathbf{t}_n, ..., \mathbf{t}_N}$, where $N$ represents the token sequence length and the size of $\mathbf{T}$ is \texttt{(batch size, 510, 768)}.

Similarly, operations are applied to the TrOCR decoder input but adjusted for different dimension expansion to produce the CharBERT character channel input $\mathbf{C} = {\mathbf{c}_1, ..., \mathbf{c}_m, ..., \mathbf{c}_M}$, where $M$ denotes the character sequence length. It is established that $M$ is typically six times that of $N$, based on the assumption that the average word contains six characters.

The tensor transformation layer is specifically designed to convert the TrOCR decoder input into CharBERT token and character embeddings. This layer also plays a critical role in the Tensor Combine Module, which we will discuss shortly. The CNN and FFNN layers are instrumental not only in aligning the dimensions between the tensors but also in learning to map the contextual information from TrOCR embeddings to those of CharBERT, effectively adapting the embeddings for integrated processing.

\paragraph*{Tensor Combine Module}
\label{par:3_tensor_combine}
The fourth problem arises because CharBERT produces two separate tensors—token and character representations—while the TrOCR decoder input requires a single tensor. To address this, the solution involves combining the two output tensors from CharBERT into a single tensor. Additionally, to stabilize the deep model, a residual connection from the original TrOCR decoder embedding can be added. This residual connection helps to reuse features from the original TrOCR decoder embedding and prevents gradient vanishing.

Among the three tensors involved, two are derived from CharBERT. Consequently, these representations are not inherently compatible with the TrOCR decoder, and their dimensions do not match. Thus, they must first undergo a transformation via the \hyperref[par:3_tensor_transform]{Tensor Transformation} process before they can be effectively combined.
\begin{equation} \label{eq:3_tensor_combine_transform}
    \begin{split}
        &\mathbf{T}\in \mathbb{R}^{d_N}\to \mathbf{T}' = \text{Tensor\_Transform}(\mathbf{T})\in \mathbb{R}^{d_D}\\
        &\mathbf{C}\in \mathbb{R}^{d_M}\to \mathbf{C}' = \text{Tensor\_Transform}(\mathbf{C})\in \mathbb{R}^{d_D}\\
    \end{split}
\end{equation}
where $d_D$ = \texttt{(batch size, 512, 1024)}; $d_N$ = \texttt{(batch size, 510, 768)}; $d_M$ = \texttt{(batch size, 3060, 256)}.

After the tensor transformation module is applied, the three tensors will all conform to the size \texttt{(batch size, 512, 1024)}, matching the original input size of the TrOCR decoder. These tensors can then be merged and fed into the TrOCR decoder stack. In this study, we explore four different architectures for the tensor combination module: 1) simple addition of all tensors; 2) mean pooling; 3) utilization of linear layers as an attention network; 4) implementation of convolutional layers as an attention network.

\subparagraph*{Tensor Combine Module 1: Adding}
\label{subpar:3_adding}
The first tensor transformation module aggregates three tensors into a single tensor by performing an element-wise addition of the three input tensors. Specifically, this involves the TrOCR decoder embeddings $\mathbf{E}$, the CharBERT transformed token representation $\mathbf{T'}$, and the CharBERT transformed character representation $\mathbf{C'}$. Each of these contributes equally to the formation of the combined tensor, ensuring that the information from each input tensor is weighted equally. The architecture is illustrated in \Cref{fig:3_combine_1}. The operation can be mathematically expressed as follows:
\begin{equation} \label{eq:3_tensor_combine_adding}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{E'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{E}')\\
        &\mathbf{E}'_{1, ijk} = \mathbf{E}_{ijk} + \mathbf{T}'_{ijk} + \mathbf{C}'_{ijk}
    \end{split}
\end{equation}
where $i = 1, 2, ..., \text{batch size}; j = 1, 2, ..., 512; k = 1, 2, ..., 1024$.

\fig{images/combine_1.png}{fig:3_combine_1}{\textbf{Architecture of the Tensor Combine Module 1}\\This diagram illustrates the flow from the TrOCR decoder and CharBERT token and character embeddings through their respective tensor transformation processes. The transformed embeddings are then combined using an element-wise addition to form the transformed decoder embeddings, which are subsequently fed back into the TrOCR decoder.}{7.5}{Tensor Combine Module 1}

\subparagraph*{Tensor Combine Module 2: Mean Pooling}
\label{subpar:3_mean_pooling}
The second tensor combination module is designed to dynamically allocate attention weights to each word across the three input tensors. These weights are derived by performing max pooling along the embedding axis of the three tensors, once they are stacked together. After obtaining these weights, they are applied to the stacked embeddings. It is important to note that all three tensors undergo feature-wise normalization prior to stacking. This normalization aids in faster model convergence and promotes better generalization. The architecture is illustrated in \Cref{fig:3_combine_2}.
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{E'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{E}')\\
        &\mathbf{S} = \text{Stack}(\mathbf{E}'_{\text{norm}}, \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
        &\mathbf{P} = \frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}_{a,b,c,j}\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\\
        &\mathbf{E}'' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}

\fig{images/combine_2.png}{fig:3_combine_2}{\textbf{Architecture of the Tensor Combine Module 2}\\This module utilizes mean pooling to dynamically allocate attention weights across the three input tensors. After the tensors are stacked, attention weights are derived and applied, followed by mean pooling to merge the embeddings into a single combined embedding, which is then processed by the TrOCR decoder.}{8}{Tensor Combine Module 2}

\subparagraph*{Tensor Combine Module 3: Linear Layers as Attention Net}
\label{subpar:3_linear_layers_as_attention_net}
The third tensor combination module builds upon the second module by incorporating linear layers as the attention network. Rather than merely performing mean pooling, this module employs linear layers interspersed with activation functions to more effectively capture relevant information when determining the weights. The architecture is illustrated in \Cref{fig:3_combine_3_4}. The process is described as follows:
\begin{equation} \label{eq:3_tensor_combine_linear_1}
    \begin{split}
        &\mathbf{T'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{T}')\\
        &\mathbf{C'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{C}')\\
        &\mathbf{E'}_{\text{norm}} = \text{Feature\_Norm}(\mathbf{E}')\\
        &\mathbf{P} = \text{Stack}(\mathbf{E'}_{\text{norm}}, \mathbf{T}'_{\text{norm}}, \mathbf{C}'_{\text{norm}}; \text{axis} = 1)\in \mathbb{R}^{{\small(\text{batch size, 3, 512, 1024})}}\\
    \end{split}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_2}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\:\:\:\: \text{\small \textcolor{RoyalBlue}{Linear Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_1 = \text{LeakyReLU}(\mathbf{b}_7 + \mathbf{W}_7\cdot \mathbf{S}')\\
        &\qquad\mathbf{S}'_2 = \text{LeakyReLU}(\mathbf{b}_8 + \mathbf{W}_8\cdot \mathbf{S}'_1)\\
        &\qquad\mathbf{S}'_3 = \mathbf{b}_9 + \mathbf{W}_9\cdot \mathbf{S}'_2\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
        % &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\text{\small \textcolor{Cerulean}{Attention Net}}
    \end{split}}
\end{equation}
\begin{equation} \label{eq:3_tensor_combine_linear_3}
    \begin{split}
        &\mathbf{P} = [\mathbf{P}_1; \mathbf{P}_2; ...; \mathbf{P}_{512}]\in \mathbb{R}^{{\small(\text{batch size, 3, 512})}}\qquad\qquad\qquad\qquad\\
        &\mathbf{E}' = \mathbf{P}\cdot \mathbf{S}
    \end{split}
\end{equation}
\subparagraph*{Tensor Combine Module 4: Convolutional Layers as Attention Net}
\label{subpar:3_convolutional_layers_as_attention_net}
The fourth tensor combination module substitutes linear layers with convolutional layers in the attention network. The equations below illustrate this attention network, which differs from the one described in \hyperref[subpar:3_linear_layers_as_attention_net]{Tensor Transformation - Linear Layers as Attention Net}. The architecture is illustrated in \Cref{fig:3_combine_3_4}. 
\begin{equation} \label{eq:3_tensor_combine_conv}
    \tcbset{fonttitle=\scriptsize}
        \tcboxmath[colback=white,colframe=RoyalBlue]{\begin{split}
        &\text{For }i = 1, 2, ..., 512:\qquad\qquad\qquad\quad\:\:\: \text{\small \textcolor{RoyalBlue}{Conv Attention Net}}\\
        &\qquad\mathbf{S}' = \mathbf{S}[:, :, i, :]\in \mathbb{R}^{{\small(\text{batch size, 3, 1024})}}\\
        &\qquad\mathbf{S}'_{1, j} = \text{LeakyReLU}(\mathbf{b}_{10} + \sum_{k=1}^{3}\mathbf{W}_{10,k}\cdot \mathbf{S}'_{i+k-1})\\
        &\qquad\mathbf{S}'_{2, l} = \text{LeakyReLU}(\mathbf{b}_{11} + \sum_{k=1}^{3}\mathbf{W}_{11,k}\cdot \mathbf{S}'_{1,j+k-1})\\
        &\qquad\mathbf{S}'_{3, p} = \mathbf{b}_{12} + \sum_{k=1}^{3}\mathbf{W}_{12,k}\cdot \mathbf{S}'_{2,l+k-1}\\
        &\qquad\mathbf{P}_i = \text{softmax}\Bigl(\frac{1}{1024}\sum_{j = 1}^{1024}\:\mathbf{S}'_{3, (a,b,j)}\Bigr)\in \mathbb{R}^{{\small(\text{batch size, 3})}}\\
    \end{split}}
\end{equation}

\fig{images/combine_3_4.png}{fig:3_combine_3_4}{\textbf{Architecture of the Tensor Combine Module 3 and 4}\\For each input sequence, the tensors are stacked and transformed through either linear or convolutional layers (detailed in the right two panels) to generate attention weights via a softmax function. The weights are then applied to compute a weighted mean, resulting in an attention-modified combined representation, which is subsequently fed into the TrOCR decoder.}{15}{Tensor Combine Module 3 and 4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          CharBERT Training and Evaluation Criteria                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training CharBERT$_{\text{SMALL}}$}
\label{sec:4_training_charbert_small}
The training of CharBERT$_{\text{SMALL}}$ closely follow the methodology outlined by the original authors. The model undergoes masked language modeling (MLM) and noisy language modeling (NLM), across a large text corpus. Training entails backpropagation and weight optimization to minimize prediction errors for both MLM and NLM tasks.

While maintaining adherence to the \href{https://github.com/mawentao277/CharBERT/blob/main/shell/mlm.sh}{hyperparameters recommended by the original authors}, we made adjustments to the batch size, increasing it from the suggested size of 4 to 16, to speed up the computation process. The training process utilized four A100 GPUs, each equipped with 80GB of RAM, and spanned six days, comprising three training epochs. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                    Glyph Incorporation                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Glyph Incorporation}
\label{sec:3_glyph_incorporation}
In our methodology, we enhance the training process by specifically targeting commonly misrecognized characters, such as \say{.} and \say{,} or \say{O} and \say{o}, with the aim of reducing the likelihood of these errors in future recognitions. To achieve this, we begin by determining the transition probability $\mathcal{P}_{ij}$, where $i$ represents the correct character that has been erroneously recognized as character $j$. This strategy is a deviation from the CharBERT NLM training approach, which incorporates character-level errors into the text during training.

By leveraging $\mathcal{P}_{ij}$, we refine our training methodology to introduce errors in a more systematic manner, based on the observed probabilities of specific misrecognitions. This targeted approach allows us to focus the model's learning on correcting these particular errors, enhancing its accuracy and reliability in distinguishing between characters that are commonly confused.

\subsection{Get $\mathcal{P}_{ij}$}
\label{subsec:3_get_pij}
To obtain $\mathcal{P}_{ij}$, it is first necessary to calculate the frequency of each character misrecognized by the recognizer (TrOCR), termed as \emph{misrecognized frequency}. This process begins with employing the basic TrOCR model to process the GW and JH datasets. After generating text outputs with TrOCR, these are compared with the corresponding text labels to determine the frequencies of characters that were misrecognized.

To align the generated outputs with the labels, we utilize the \say{Bio} package in Python, specifically referring to Biopython. This toolkit is primarily designed for biological computation, handling data such as DNA, RNA, and protein sequences, and offers extensive features for sequence analysis and alignments. It also provides extensive features for sequence analysis and alignments, making it highly suitable for our needs. Below, we present an example of how the Bio package aligns OCR output with a label. For the OCR output \say{Company} and the label \say{company}, the toolkit not only matches each corresponding character but also identifies missing or misrecognised ones with dashes. This clear visualization allows us to easily determine which characters are frequently misrecognised by our recognition model. This example is particularly useful for understanding common recognition errors and adjusting the model accordingly.

\begin{center}
    \begin{minipage}{0.3\textwidth} 
    \begin{verbatim}
    0 *C-ompany*  9
    0 |--||||||| 10
    0 *-company*  9
    \end{verbatim}
    \end{minipage}
\end{center}


Although Biopython is not inherently designed for processing natural language texts, its tools for sequence matching and regular expressions can be adapted for text analysis applications. For instance, in our context, we can apply Biopython's methodologies to perform precise character-by-character matching within text strings, identifying and annotating discrepancies. This approach demonstrates how Biopython's core functionalities can be repurposed for non-biological applications, providing valuable methods for text data analysis and enhancing our understanding of text misrecognition patterns.

\subsection{Training CharBERT$_{\mathcal{P}_{ij}}$}
\label{subsec:3_training_charbert_pij}
To train CharBERT$_{\mathcal{P}_{ij}}$, we adhere to the methodologies outlined in the original CharBERT study similar to our approach for training CharBERT$_{\text{SMALL}}$. This involves processing text through the model's dual-channel architecture, which includes both token and character channels, as detailed in \Cref{subsec:3_corrector_charbert}. We also follow the hyperparameter settings recommended in the CharBERT publication, which are documented extensively at this \href{https://github.com/mawentao277/CharBERT}{link}.

In the original CharBERT training, the model is pre-trained by randomly adding, deleting, or swapping characters within the input text to simulate typical errors, thereby training CharBERT to correct them. For our specific application focusing on OCR corrections, we have modified this approach by replacing the random swapping of characters with targeted replacements. These replacements are guided by the misrecognition probabilities $\mathcal{P}{ij}$, which were derived from the analysis of character frequencies in misrecognized words by the TrOCR system, as discussed earlier. This modification reflects the error patterns more commonly observed in OCR outputs, ensuring that our training process closely mimics real-world OCR challenges and optimizes CharBERT$_{\text{SMALL}}$ for more accurate error correction in this context.

% \newpage
% \begin{landscape}
%     \twofig{images/combine_1.png}{fig:3_combine_1}{7.5}{images/combine_2.png}{fig:3_combine_2}{8}{fig:3_combine_1_2}{\textbf{Architecture of the Tensor Combine Module 2}\\This module utilizes mean pooling to dynamically allocate attention weights across the three input tensors. After the tensors are stacked, attention weights are derived and applied, followed by mean pooling to merge the embeddings into a single combined embedding, which is then processed by the TrOCR decoder.\textbf{Architecture of the Tensor Combine Module 1}\\This diagram illustrates the flow from the TrOCR decoder and CharBERT token and character embeddings through their respective tensor transform processes. The transformed embeddings are then combined using an element-wise addition to form the transformed decoder embeddings, which are subsequently fed back into the TrOCR decoder.}{Tensor Combine Module 1 and 2}
% \end{landscape}
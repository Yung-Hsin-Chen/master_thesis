\newchap{Related Work}
\label{chap:2_related_work}
\section{TrOCR}
\label{sec:2_trocr}
OCR tasks typically involves text detection and text recognition. TrOCR is a Transformer based model which focuses on text recognition part of the OCR task, which convert images to texts. Therefore, the input should be sliced into line images, each with a single line of transcription on it. Unlike former progress in text recognition, TrOCR uses pre-trained image Transformer and text Transformer instead of CNN backbones and CTC. It eliminates the usage of external large LMs and can be extended for multilingual purposes by leveraging pre-trained LMs of different languages. In addition, unlike traditional OCR systems that rely on feature engineering and pre-/post-processing, TrOCR allows contextual learning, leading to SOTA results in challenging scenarios.

The encoder of TrOCR is initialised by ViT-style models such as DeiT or BEiT, while the decoder initialisation uses BERT-style models such as RoBERTa or MiniLM. Both of them are Transformer encoder stuctures. Therefore, the missing encoder-decoder attention in TrOCR decoder is initialised randomly for training. Due to the fixed input length of Transformers, the image is first resized into 384$\times$384, and then split into 16$\times$16 patches before inputting the Transformer. Unlike CNN, Transformers don't have spatial information of the input data. Thus, positional encodings is added to the patches to preserve the spatial structures of the input images. 

TrOCR is pre-trained on synthetic data and SROIE, IAM datasets sequentially. TrOCR\textsubscript{LARGE}(total parameters=558M), initialised by BEiT\textsubscript{LARGE} and RoBERTa\textsubscript{LARGE}, reached CER 2.89 by pre-training on the synthetic data and the IAM dataset.

So far, TrOCR is considered a SOTA model for OCR on both printed and handwritten tasks. Although its design and capabilities represent a significant advancement in the field of OCR, deficiencies still exist. In scenarios where the text layout are curved or vertical, TrOCR's performance is compromised. Note that these limitations are not unique to TrOCR, but are rather common challenges for most OCR models. In this study, TrOCR will serve as the baseline. This foundational benchmark will be improved through integration with LMs to refine and correct the output results.

\section{CharBERT}
\label{sec:2_charbert}
CharBERT \citep{ma2020charbert} is an enhancement of BERT, which aims to address problems in Byte-Pair Encoding (BPE) used by pre-trained language models like BERT, RoBERTa. It has the same model structure and configuration as BERT and RoBERTa depending on the initialisation. During inference, it takes a text as input and outputs a representative embedding of the text.

Pre-trained language models like BERT, RoBERTa have achieved outstanding results in NLP tasks. Both models use BPE to encode input data. BPE is capable of encoding almost all vocabularies including OOV words. It breaks down OOV words into subwords until they are in its vocabulary dictionary. In addition, it allows efficiency in vocabulary space. For instance, BPE can represent different forms of a word (e.g., "run", "running", "runs") with their common subwords, which results in a smaller set of total tokens. However, BPE has the problems of incomplete modeling and fragile representation. 

Incomplete modeling refers to the inability of BPE to fully encapsulate the complete aspects of a word. BPE splits words into subword units. While these small pieces are helpful for understanding parts of the word, they may not entirely convey the meaning or nuances of the whole word. For instance, the word "understand", which will be broken down into "under" and "stand" by BPE, means comprehending a concept. However, its meaning is not merely a combination of "under" and "stand". 

Fragile representation highlighs BPE's sensitivity to minor typos in a word. In other words, a small spelling mistake can result in drastic changes in the set of subwords. For instance, BPE processes "cat" as a known subword. However, if an error occurs, resulting in "cta", BPE will break down the word into individual characters: "c" ,"t" and "a". In this example, a minor rearrangement of letters in "cat" causes significant changes in how BPE interprets the word.

CharBERT aims to address these two problems with two tricks in the pre-training stage: 1) employ a dual-channel architectural approach for the subword and character; 2) Utilise noisy language modeling (NLM) and masked language modeling (MLM) for unsupervised character representation learning. The first trick processes both subword and character-level information and fuse them, ensuing a more robust representation in case of typos. The second trick involves introducing character-level noise into words and and training the model to correct these errors. This approach enhances the model's ability to handle real-world text with variations and typos. Besides, it also masked 10\% of words and train the model to predict them. This enables CharBERT to the synthetic information on a token level.

CharBERT is fine-tuned on several downstream tasks, including question answering, text classification and sequence labeling. CharBERT outperforms BERT across all tasks. However, it encounters some challenging competition from RoBERTa, which the authors acknowledge as a robust baseline. 
% TODO: Do I include table of results? Or should I let people look for the table in the paper by themselves?

Although the performance of CharBERT does not exceed RoBERTa significantly, the model architecture of CharBERT is intriguing. Since models are usually better at the task they are pre-trained on, the NLM pre-training task makes CharBERT a potentially good corrector, which can be paired with the recognisor and compensate its language deficiencies. In addition, we can further pre-trained the NLM with common OCR mistakes instead of randomly introduced errors. This makes the corrector more familiar to OCR specific errors.

\section{Candidate Fusion}
\label{sec:2_candidate_fusion}
Candidate fusion \citep{kang2021candidate} is a technique involves integrating the LM within the recogniser, i.e., let the LM and the recogniser interact. Most SOTA word recognition models and LMs are trained separately. However, with candidate fusion, several advantages can be achieved. First of all, the recogniser is able to integrate insights from both its own processing and the input from the LM, resulting in a more comprehensive understanding. Secondly, the system is designed to assess the information supplied by the LM, allowing the recogniser to selectively weigh its importance. Lastly, the LM can learn from frequent errors generated by the recogniser, improving overall accuracy.

The models utilised for demonstrating candidate fusion is an encoder-decoder structure, where the encoder extracts features from input images, and the decoder converts the image features into text. The encoder is a CNN followed by a GRU, and the decoder is an undirectional multi-layerd GRU. Both structures in the encoder provide spatial information about the input, allowing the decoder to follow the proper order.

In the paper, they employ two-step training. In the first step, the LM is pre-trained on a large corpus for it to understand general language and grammar. In the second step, the LM is further trained alongside on handwritten datasets with the recogniser. This allows the LM to consider both its own knowledge and what the recogniser predicts when outputting the result text. The LM will adjust its predictions by taking into the recogniser's decision into account.

The model is being evaluated on IAM, GW and Rimes. Among the datasets, the model shows significant improvement on GW over a strong baseline \citep{8395102}. However, the performance of this model beyond the GW dataset remains questionable, as the improvements are relatively small. Additionally, these other datasets encompass more writers, contributing to a greater diversity in handwriting styles. Consequently, the model's marked improvement on the GW dataset might indicate potential limitations in its adaptability to diverse handwriting styles.

Despite the potential deficiency in the model, the idea of fusing the recogniser and the LM and allowing both of them to learn from each other is a quality design worth exploring. This technique provides new opportunities for achieving more context-aware text recognition. In this study, TrOCR and CharBERT will serve as the recogniser and the LM respectively, and will be combined. The composite model aims to explore the potential enhancements or effects they can bring when used together.
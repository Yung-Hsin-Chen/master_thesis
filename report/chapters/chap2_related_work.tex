\newchap{Related Work}
\label{chap:2_related_work}
This chapter explores the realm of optical character recognition (OCR), tracing the evolution of this technology and the development of post-OCR correction techniques. A detailed exploration of TrOCR, a state-of-the-art (SOTA) OCR model, alongside CharBERT, a large language model (LLM), forms a significant portion of this chapter, with an emphasis on their architectures and training methodologies. These two models are central to this study due to their innovative integration potential. Additionally, the concept of candidate fusion is discussed, which leverages the strengths of both OCR and LMs to improve text recognition accuracy. The chapter also examines the advantages of incorporating glyph analysis into model training, further refining the recognition process. Overall, this chapter reviews the current advancements in OCR technology and sets the stage for the introduction of a novel composite model designed to combine these technologies.

\section{Optical Character Recognition (OCR)}
\label{sec:2_ocr}
According to \cite{philips2020historical}, the digitization of historical documents involves several critical phases. The process begins with image acquisition, where physical documents are converted into digital images through scanning or photography. Following this, preprocessing steps such as binarization, using methods like Otsu's algorithm \citep{liu2009otsu} to simplify images into binary formats, and layout analysis to determine structural elements like text blocks, are essential for preparing images for text recognition. Text recognition is then performed using technologies like OCR and handwritten text recognition (HTR). Modern approaches often employ convolutional neural networks (CNNs) \citep{zhang2017cnn} and long short-term memory networks (LSTMs) \citep{breuel2013high} to enhance accuracy. This comprehensive process is depicted in \Cref{fig:2_ocr_process}.

\fig{images/ocr_process.png}{fig:2_ocr_process}{Digitization Process of Historical Documents}{15}{Digitization Process of Historical Documents}

The paper by \cite{karthick2019steps} elaborates on the OCR process: 1) preprocessing; 2) feature extraction; 3) recognition. Initially, during preprocessing, the image quality is enhanced by removing noise, adjusting contrast, and segmenting the image to prepare it for analysis. This stage sets the foundation for feature extraction, where unique characteristics of the text are identified, which are crucial for accurate recognition. Before feature extraction, additional steps such as further segmentation are applied to enhance the text structure in the images. Following feature extraction, the recognition stage classifies these features into textual data. This text is then optionally refined during the post-processing stage, where errors are corrected, and text clarity is improved. The process is illustrated in \Cref{fig:2_ocr}.

\fig{images/ocr.png}{fig:2_ocr}{Stages of the OCR Process}{15}{Stages of the Optical Character Recognition (OCR) Process}

While traditional models typically handle the processes of preprocessing, feature extraction, and recognition in separate, discrete stages, there are modern OCR models that operate on an end-to-end basis \citep{huang2021multiplexed}\citep{neudecker2019ocr}\citep{belay2020amharic}. These end-to-end models streamline the text recognition process by integrating all these stages into a single continuous workflow. This approach leverages advanced machine learning techniques, particularly deep learning, to directly process raw images of text into editable text outputs. This not only simplifies the OCR process by eliminating the need for manual feature engineering and separate processing stages, but also enhances the system's ability to handle varied and complex text.

\paragraph*{SOTA OCR Models}
OCR technology has made significant strides in recent years. Notably, transformer models, originally developed for natural language processing (NLP) tasks, have been successfully adapted for OCR applications. Models such as TrOCR \citep{li2023trocr}, which combines vision transformer (ViT) \citep{dosovitskiy2021an}\citep{vaswani2017attention} with pre-trained LMs like RoBERTa \citep{liu2019roberta}, have demonstrated remarkable effectiveness in extracting text from images. Additionally, the incorporation of attention mechanisms in OCR models has enabled the network to sequentially focus on specific parts of an image, mimicking the human reading process such as Attention-based Scene Text Recognizer (ASTER) \citep{shi2018aster} and STAR-Net \citep{liu2016star}. Both of them have substantially enhanced OCR accuracy. Furthermore, the latest version of Tesseract \citep{smith2007overview}, an open-source OCR engine, has been augmented with LSTM \citep{hochreiter1997long} networks. This enhancement not only supports a greater range of languages but also improves recognition capabilities, particularly when integrated with customized pre-processing and training techniques. These advancements underscore the dynamic evolution of OCR technology, making it more versatile and effective in processing complex textual data from images.

\paragraph*{Relevence to Our Study}
In our research, we aim to explore potential enhancements in OCR models. We will select a SOTA OCR model as our baseline to evaluate whether modifications made to this model can surpass its original performance. Ideally, we will utilize an end-to-end model, eliminating the need to individually manage steps such as image processing and feature extraction.

\section{Post-OCR correction}
\label{sec:2_post-ocr_correction}
Post-OCR correction refers to the methods and processes used to correct errors in text after it has been converted from images (like scanned documents or photos) to editable and searchable text data. This stage is crucial because OCR technology, despite its advancements, often makes mistakes due to various challenges such as poor image quality, complex layouts, unusual fonts, or challenging handwritings.

Post-OCR correction is an effective tool but comes with its own set of challenges, such as distorted outputs from OCR processes or domain-specific terminology within datasets. For example, \cite{karthikeyan2021ocr} utilize the RoBERTa model, employing a self-supervised pre-training approach to predict masked sections of medical texts. Their goal is to bridge the gaps in OCR transcriptions, particularly when the technology fails to accurately capture parts of documents. The authors discuss and tackle the inherent challenges posed by the varying accuracy of OCR technology, especially in recognizing texts that contain medical terminologies and are often scanned from physical documents where text may be skewed or obscured.

\cite{mokhtar2018ocr} points out that the structured and predictable nature of OCR errors makes them suitable for correction using machine learning models. They proposed two deep learning models: a word-based sequence-to-sequence model and a character-based model, allowing them to correct individual characters and handle words not seen during training. The findings show that while the word-based model struggles with unseen words, the character-based model performs well across different datasets. This insight aligns with the findings from \cite{kang2021candidate}, which suggest that character-level processing can significantly improve the overall performance.

\paragraph*{SOTA Post-OCR Models}
Post-OCR correction models extensively utilize advanced machine learning techniques, particularly deep learning, to refine OCR outputs effectively. Transformers, renowned for their sequence handling capabilities and scalability, dominate this field. Models like Bidirectional Encoder Representations from Transformers (BERT) \citep{devlin-etal-2019-bert} and its variants, including RoBERTa \citep{liu2019roberta} and GPT \citep{radford2018improving}, are effectively fine-tuned for post-OCR text correction. This fine-tuning process involves training on datasets composed of erroneous OCR outputs paired with their correct versions. 

\paragraph*{Relevence to Our Study}
\label{par:2_post_ocr_relevance}
\hypertarget{model_needs}{In this study, we aim to leverage post-OCR correction to address the inherent limitations of OCR technologies. Drawing from the insights gained in earlier sections, we plan to employ a BERT-like architecture that incorporates character-level processing capabilities. This approach is expected to enhance the accuracy and effectiveness of OCR by refining the outputs, particularly when dealing with complex text characteristics.}

\section{TrOCR}
\label{sec:2_trocr}
OCR tasks typically involve text detection and text recognition. TrOCR is a transformer-based model which focuses on the text recognition part of the OCR task, converting images to texts. Therefore, the input should be sliced into line images, each with a single line of transcription on it. Unlike previous advancements in text recognition, TrOCR uses a pre-trained ViT for image feature extraction and a text transformer \citep{vaswani2017attention} for sequence-to-sequence learning instead of CNN backbones \citep{wang2020cspnet} and Connectionist Temporal Classification (CTC) \citep{graves2006connectionist}. It eliminates the usage of external LLMs and can be extended for multilingual purposes by leveraging pre-trained LMs of different languages. In addition, unlike traditional OCR systems that rely on feature engineering and pre-/post-processing, TrOCR allows contextual learning, leading to SOTA results in challenging scenarios.

The encoder of TrOCR is initialized by pre-trained ViT-style models such as Data-Efficient Image Transformer (DeiT) \citep{touvron2021training} or Bidirectional Encoder representation from Image Transformers (BEiT) \citep{bao2022beit}, while the decoder initialization uses pre-trained BERT-style models such as RoBERTa \citep{liu2019roberta} or MiniLM \citep{wang2020minilm}. Both of them are transformer encoder structures. Therefore, the missing encoder-decoder attention in the TrOCR decoder is initialized randomly during training. Due to the fixed input length of transformers, the image is first resized to 384$\times$384, and then split into 16$\times$16 patches before inputting into the transformer. Unlike CNNs, transformers do not have spatial information of the input data. Thus, positional encodings are added to the patches to preserve the spatial structures of the input images.

TrOCR is pre-trained on synthetic data and SROIE \citep{huang2019icdar2019}, IAM datasets \citep{marti1999full} sequentially. TrOCR\textsubscript{LARGE} (total parameters=558M), initialized by BEiT\textsubscript{LARGE} and RoBERTa\textsubscript{LARGE}, reached a character error rate (CER) \citep{klakow2002testing}\citep{wang2003word} of 2.89 by pre-training on the synthetic data and the IAM dataset.

So far, TrOCR is considered a SOTA model for OCR on both printed and handwritten tasks. Although its design and capabilities represent a significant advancement in the field of OCR, deficiencies still exist. In scenarios where the text layout is curved as shown in \Cref{fig:5_curved_text}, TrOCR's performance is compromised. Note that these limitations are not unique to TrOCR but are rather common challenges for most OCR models.

\paragraph*{Relevence to Our Study}
The TrOCR handwritten model has been selected as our primary OCR model for text recognition, owing to its SOTA performance. As an end-to-end model, TrOCR simplifies the processing pipeline by eliminating the need for separate image processing and feature extraction steps. This allows it to be easily fine-tuned in conjunction with CharBERT. In this study, TrOCR will serve as the baseline. This foundational benchmark will be improved through integration with LMs to refine and correct the output results.

\twofig{images/curved_text_1.png}{fig:5_curved_text_1}{0.8}{images/curved_text_2.png}{fig:5_curved_text_2}{0.6}{fig:5_curved_text}{\textbf{Curved Text Layout}\\TrOCR struggles with curved text recognition. For example, Figure (a) is erroneously recognized as \say{MASIAS ACURVED TEH} and Figure (b) as \say{THIS SECRET.COM\/} by TrOCR\textsubscript{LARGE} instead of the correct \say{THIS IS A CURVED TEXT.}}{Curved Text Layout}

\section{CharBERT}
\label{sec:2_charbert}
CharBERT \citep{ma-etal-2020-charbert} is an enhancement of BERT, which aims to address problems in Byte-Pair Encoding (BPE) \citep{sennrich2015neural} used by pre-trained LMs like BERT and RoBERTa. During inference, CharBERT takes text as input and outputs two representative embeddings of the text: token embeddings and character embeddings.

Pre-trained LMs like BERT and RoBERTa have achieved outstanding results in NLP tasks. Both models use BPE to encode input data. BPE is capable of encoding almost all vocabularies, including out-of-vocabulary (OOV) words. It breaks down OOV words into subwords until they are in its vocabulary dictionary. In addition, it allows efficiency in vocabulary space. For instance, BPE can represent different forms of a word (e.g., \say{run}, \say{running}, \say{runs}) with their common subwords, which results in a smaller set of total tokens. However, BPE has the problems of incomplete modeling and fragile representation.

Incomplete modeling refers to the inability of BPE to fully encapsulate the complete aspects of a word. BPE splits words into subword units. While these small pieces are helpful for understanding parts of the word, they may not entirely convey the meaning or nuances of the whole word. For instance, the word \say{understand}, which will be broken down into \say{under} and \say{stand} by BPE, means comprehending a concept. However, its meaning is not a combination of \say{under} and \say{stand}.

Fragile representation highlights BPE's sensitivity to minor typos in a word. In other words, a small spelling mistake can result in drastic changes in the set of subwords. Consider the word \say{backhand,} which BPE correctly splits into \say{back} and \say{hand,} effectively capturing the word's meaning. However, if there is a typo resulting in \say{bachand} (missing \say{k}), BPE breaks the word down into \say{b}, \say{ach}, and \say{and.} This segmentation fails to convey the intended meaning and highlights the model's vulnerability to errors.

To illustrate further, let's examine character-level tokenization. For \say{backhand,} the results of the character-level tokenization would be \say{b,} \say{a,} \say{c,} \say{k,} \say{h,} \say{a,} \say{n,} \say{d.} In the case of the typo \say{bachand,} the tokens change slightly to \say{b,} \say{a,} \say{c,} \say{h,} \say{a,} \say{n,} \say{d.} Despite the typo, the character-level tokenization remains relatively consistent, showing a minor deviation from the original. This example demonstrates that while BPE can drastically misinterpret a word due to a small error, character-level tokenization maintains a closer representation to the original, even with typos. This example is illustrated in \Cref{fig:2_bpe}.

\fig{images/bpe.png}{fig:2_bpe}{\textbf{Fragile Representation Example}\\BPE is sensitive to typos, drastically altering subwords: \say{backhand} splits into \say{back} and \say{hand,} but \say{bachand} (missing \say{k}) becomes \say{b}, \say{ach}, and \say{and,} losing the original meaning. In contrast, character-level tokenization maintains consistency: \say{backhand} is tokenized as \say{b,} \say{a,} \say{c,} \say{k,} \say{h,} \say{a,} \say{n,} \say{d,} and "bachand" slightly alters to \say{b,} \say{a,} \say{c,} \say{h,} \say{a,} \say{n,} \say{d.}}{12}{Fragile Representation Example}

CharBERT aims to address these two problems with two tricks in the pre-training stage: 1) employ a dual-channel architectural approach for the subword and the character; 2) utilize noisy language modeling (NLM) and masked language modeling (MLM) for unsupervised representation learning. The first trick processes both subword and character-level information and fuses them, ensuring a more robust representation in case of typos. The second trick involves introducing character-level noise into words or masking words, and trains the model to correct these errors. This approach enhances the model's ability to handle real-world text with variations and typos. 

CharBERT is fine-tuned on several downstream tasks, including question answering, text classification, and sequence labeling. CharBERT outperforms BERT across all tasks. However, it encounters some challenging competition from RoBERTa, which the authors acknowledge as a robust baseline as shown in \Cref{fig:2_charbert_result}.

\fig{images/charbert_result.png}{fig:2_charbert_result}{Experimental Results of CharBERT and Strong Pre-trained Models\\Image source: \cite{ma-etal-2020-charbert}}{15}{Experimental Results of CharBERT and Strong Pre-trained Models}

Although the performance of CharBERT does not significantly exceed that of RoBERTa, the model architecture of CharBERT is intriguing. Since models are usually better at the tasks they are pre-trained on, the NLM pre-training task makes CharBERT a potentially good corrector, which can be paired with the recognizer and compensate for its language deficiencies in our study. In addition, we can further pre-train the NLM with common OCR mistakes instead of randomly introduced errors. This could make the corrector more familiar with OCR-specific errors.

\paragraph*{Relevence to Our Study}
CharBERT will function as the corrector in our post-OCR correction system. It meets \hyperlink{model_needs}{our requirements}, featuring character-level processing and a BERT-like architecture. Additionally, given its pre-training tasks, CharBERT is particularly effective at correction tasks, making it highly suitable for our post-OCR correction needs. 

\section{Candidate Fusion}
\label{sec:2_candidate_fusion}
Candidate fusion \citep{kang2021candidate} is a technique that involves integrating the LM within the recognizer in an end-to-end manner, i.e., letting the LM and the recognizer interact. Most SOTA word recognition models and LMs are trained separately. However, with candidate fusion, several advantages can be achieved. First, the recognizer is able to integrate insights from both its own processing and the input from the LM, resulting in a more comprehensive understanding. Second, the system is designed to assess the information supplied by the LM, allowing the recognizer to selectively weigh its importance. Lastly, the LM can learn from frequent errors generated by the recognizer, improving overall accuracy.

The model utilized for demonstrating candidate fusion have an encoder-decoder structure, where the encoder extracts features from input images, and the decoder converts the image features into text. The encoder is a CNN followed by a gated recurrent unit (GRU) \citep{dey2017gate}, and the decoder is an unidirectional multi-layered GRU. Both structures in the encoder provide spatial information about the input, allowing the decoder to follow the proper order.

In the paper, they employ \hypertarget{2_two_step}{two-step training}. In the first step, the LM is pre-trained on a large corpus for it to understand general language and grammar. In the second step, the LM is further trained alongside handwritten datasets with the recognizer. This allows the LM to consider both its own knowledge and what the recognizer predicts when outputting the result text. The LM will adjust its predictions by taking into account the recognizer's decisions.

The model is being evaluated on IAM, GW \citep{fischer2012lexicon}, and Rimes \citep{grosicki2011icdar}. Among the datasets, the model shows significant improvement on GW over a strong baseline, \cite{dutta2018improving}. However, the performance of this model beyond the GW dataset remains questionable, as the improvements are relatively small [\Cref{fig:2_candidate_fusion_result}]. Additionally, these other datasets encompass more writers, contributing to a greater diversity in handwriting styles. Consequently, the model's marked improvement on the GW dataset might indicate potential limitations in its adaptability to diverse handwriting styles.

Despite the potential deficiency in the model, the idea of fusing the recognizer and the LM and allowing both of them to learn from each other is a quality design worth exploring. This technique provides new opportunities for achieving more context-aware text recognition. In this study, TrOCR and CharBERT will serve as the recognizer and the LM, respectively, and will be combined. The composite model aims to explore the potential enhancements or effects they can bring when used together.

\fig{images/candidate_fusion_result.png}{fig:2_candidate_fusion_result}{Experimental Results of Candidate Fusion\\Image source: \cite{kang2021candidate}}{13}{Experimental Results of Candidate Fusion}

\paragraph*{Relevence to Our Study}
To thoroughly leverage the benefits of candidate fusion, this study will integrate TrOCR and CharBERT as the recognizer and LM, respectively, within the candidate fusion framework. Following the \hyperlink{2_two_step}{two-step training} methodology outlined in the original research, we aim to enable both the recognizer and the language model to interact and learn from each other effectively. This integration seeks to enhance the overall effectiveness of our OCR system, particularly in handling diverse and complex handwriting styles and adapt to texts from various domains.

\section{Glyph/Common Error Integration}
\label{sec:2_glyph_embedding}
Integrating the concept of glyphs into OCR is a promising approach, especially for the digitization of historical texts. This method focuses on glyphs—the visual forms of characters—which is crucial for handling the varied typographic styles found in historical documents. Glyph-based OCR is expected to be skilled at adapting to these styles and is particularly effective at recognizing characters in texts that are in poor condition, such as those with faded ink or smudges. By understanding the visual aspects of glyphs, such systems are anticipated to significantly reduce OCR errors, minimizing the need for labor-intensive post-correction. Additionally, since this method is based on visual characteristics rather than language, it is expected to be applicable across different languages and scripts, making it highly versatile.

Although the idea is promising, it has not yet been widely proven. \cite{amrhein2018supervised} explore how neural machine translation (NMT), when applied at the character level, can be particularly effective for correcting glyph-based errors. This approach leverages the flexibility of NMT to adapt to the nuances of character shapes and their common misinterpretations, thereby improving the accuracy of OCR text post-correction. However, the integration of glyph analysis into OCR workflows, as discussed in this paper, shows only potential improvements in OCR text post-correction accuracy.

Despite the lack of extensive experimental validation, this idea remains an intriguing avenue worth exploring. Continued research and development in this area could further uncover its potential and practical applications.

\paragraph*{Relevence to Our Study}
While our study primarily focuses on handwritten images, rather than the printed letters that are often the subject of glyph-based OCR research, we can adaptively incorporate glyph analysis into our approach. Specifically, we plan to integrate commonly misrecognized characters into the training process of the LM based on their occurrence frequencies. This strategy will enable the LM to become familiar with frequent OCR errors and to distinguish between similar character glyphs, thereby enhancing its ability to correct these mistakes effectively and improve the overall accuracy of our OCR system for handwritten documents.
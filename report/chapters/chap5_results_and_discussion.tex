
\newchap{Results and Discussion}
\label{chap:5_results_and_discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                           Results                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{$\mathcal{P}_{ij}$ Matrix}
\label{sec:5_pij_matrix}
As detailed in \hyperref[subsec:3_get_pij]{Subsection 3.4.1}, the $\mathcal{P}_{ij}$ matrix encapsulates the probabilities of a correct character $i$ being misrecognized as another character $j$. This matrix is a pivotal component in our analysis, providing a quantitative measure of the likelihood of error between different character transitions within the OCR process. Below, we present a subset of the matrix, highlighting the top 14 most commonly misrecognized characters. These entries have been selected due to their higher frequency of occurrence, which suggests significant areas where the CharBERT model may benefit from further training. The visual representation of these misrecognition probabilities, as shown in \autoref{fig:5_pij_matrix}, provides a clear and immediate insight into the error dynamics of the model. 

\fig{images/pij_matrix.png}{fig:5_pij_matrix}{\textbf{Heatmap of Character Misrecognitions $\mathcal{P}_{ij}$}\\This visual representation illustrates the frequency of common misrecognitions between various characters, as outlined in our study. The x-axis represents the correct characters, and the y-axis shows the characters as they were misrecognized by the OCR system. The color gradient, ranging from light yellow to dark brown, indicates the relative frequency of each misrecognition event, with darker shades representing higher frequencies. This analysis helps identify specific characters that are prone to misrecognition, facilitating targeted improvements in the OCR model.}{15}{$\mathcal{P}_{ij}$ Matrix}

\section{Component Analysis and Benchmarking Results}
\label{sec:5_component_analysis_and_benchmarking_results}
As outlined in \hyperref[subsec:4_component_analysis_and_benchmarking]{Subsection 4.4.1}, several analytical experiments were conducted to optimize the model's performance. These experiments included comparing the effects of freezing specific layers to determine their impact on learning, integrating dropout mechanisms to enhance generalization, evaluating different combinations of tensor modules for optimal processing, assessing various loss functions to improve training efficiency, and benchmarking the model against GPT-4 to gauge its competitive standing.

\subsection{Baseline Models}
\label{subsec:5_baseline_models}
For the GW dataset, fine-tuning significantly improved model accuracy. When fine-tuning was not applied, the WER and CER were relatively high at 37.76\% and 15.40\%, respectively. However, after fine-tuning, these rates dropped to 14.44\% for WER and 4.78\% for CER, showcasing a substantial enhancement in model performance.

Similarly, for the JH dataset, the benefits of fine-tuning were pronounced. Without fine-tuning, the model performed poorly, with a WER of 91.31\% and a CER of 58.57\%. Post fine-tuning, these figures improved to 36.97\% for WER and 20.28\% for CER, indicating a marked improvement in the model's ability to accurately transcribe text.

These results clearly demonstrate that fine-tuning has a significant positive effect on reducing error rates and thereby enhancing the overall accuracy of the model across different datasets.

\tab{tab:5_res}{Baseline Model Results}{%
    \begin{tabular}{ll|rrl}
        \toprule
        Dataset	& Fine-Tune                                 & WER	& CER	& File Name	\\
        \midrule
        GW		& False	                                	& 37.76	& 15.40	& gw\\
        GW		& True	                        	        & 14.44 & 4.78	& gw\_ft\\
        \midrule
        JH		& False		                                & 91.31 & 58.57 & iam\\
        JH		& True 		                                & 36.97 & 20.28 & iam\_ft\\
        \bottomrule
    \end{tabular}
}{Baseline Model Results}

\paragraph*{Output Texts Analysis - Over-Correction} 
Upon examining the GW dataset TrOCR outputs without fine-tuning, it becomes apparent that TrOCR tends to over-correct the text. As illustrated below, TrOCR autocorrects \say{Expamples} (an original misspelling by George Washington) to \say{Examples,} the correct form of the word. Additionally, it completes the truncated word \say{gene} as \say{general} presuming the word was inadvertently cut short. 

\begin{center}
    \begin{minipage}{1.0\textwidth}
    \begin{Verbatim}[commandchars=\\\{\}]
        {\small\textcolor{Gray}{Test on GW Dataset Without Fine-Tuning}}
        label:  have a very good effect. The Commonalty in gene
        output: have a very good effect. Then commonly in general

        {\small\textcolor{Gray}{Test on GW Dataset Without Fine-Tuning}}
        label:  est occasion for Expamples, will be morally im 
        output: cut occasion for Examples, will be morally in-
    \end{Verbatim}
    \end{minipage}
\end{center}

This tendency to over-correct is particularly noticeable at the end of sentences where the last word is truncated, a frequent occurrence in the George Washington (GW) dataset. TrOCR often attempts to complete these cut-off words, or it may substitute them with a different word that, while seemingly appropriate, is irrelevant to the original token image. In some cases, TrOCR even transforms the incomplete word into a completely fabricated term.

Notably, this tendency to over-correct persists even after fine-tuning, indicating that TrOCR consistently aims to standardize text regardless of the context. In the case of the George Washington (GW) testing dataset, which includes 30 labels ending with unfinished words, the fine-tuned TrOCR correctly resolved only 5 of these instances. Often, it either attempted to complete them or altered them into entirely different words. In contrast, the best-performing integration of TrOCR with CharBERT correctly handled 10 of these cases, attempting to complete the word only once, compared to the eight times observed with the fine-tuned TrOCR. This improvement demonstrates that adding a language model (LM) to the end-to-end OCR system provides valuable access to both language and image information, enhancing the model's ability to process and preserve the textual integrity of historical documents. The distribution of these outcomes is depicted in Figure \ref{fig:2_complete}.

\fig{images/complete.png}{fig:2_complete}{\textbf{Comparison of Outcomes for TrOCR Fine-Tuned vs. Best Performed TrOCR-CharBERT}\\This figure illustrates the percentage of outcomes for unfinished word scenarios within the GW dataset, comparing the fine-tuned TrOCR and the TrOCR-CharBERT. The categories \say{Complete word,} \say{Other word,} and \say{Not a word} indicate whether the model attempted to complete the unfinished words, substituted them with a different word it deemed fit, or transformed them into non-words, respectively. The pie charts reveal that the integration with CharBERT significantly reduces the instances of attempting to complete words erroneously, demonstrating its ability to more accurately preserve the original text integrity.}{17}{TrOCR Output Pie Chart}

The findings from the TrOCR outputs highlight its inherent ability to correct and complete words based on its learned understanding from training data. While this functionality showcases TrOCR's robustness in rectifying spelling errors and incomplete entries, it also introduces potential inaccuracies when dealing with OCR tasks where original spellings and phrasings are crucial for authenticity. The model's tendency to standardize text could be a drawback in contexts where the preservation of original language nuances is necessary.

\paragraph*{Output Texts Analysis - Poor Performance on JH Dataset}
The performance on the JH dataset is notably poor. This may be attributed, firstly, to Joseph Hooker's tendency to write in an ascending manner, which often results in cropping that inadvertently includes lines from adjacent text, as depicted in \autoref{fig:5_ascend}. Such cropping may distract the TrOCR from accurately recognizing the text. Secondly, the elegance of Joseph Hooker's handwriting, while aesthetically pleasing, presents readability challenges due to its cursive style and unique letter formations. Additionally, a few images (fewer than 10) contain printed rather than handwritten letters, as illustrated in \autoref{fig:5_the_camp}. In this example, TrOCR recognized \say{THE Camp.} instead of the correct label \say{THE CAMP.} as shown below. Despite the fact that TrOCR's handwritten model can recognize printed letters, it struggles with correct capitalization. Although these instances are relatively rare, they still contribute to the overall error rate of the task.

\begin{center}
    \begin{minipage}{0.7\textwidth}
    \begin{Verbatim}[commandchars=\\\{\}]
        {\small\textcolor{Gray}{Test on JH Dataset Without Fine-Tuning}}
        label:  THE CAMP.
        output: THE Camp.
    \end{Verbatim}
    \end{minipage}
\end{center}

\twofig{images/ascend.jpg}{fig:5_ascend}{1.3}{images/the_camp.jpg}{fig:5_the_camp}{0.5}{fig:5_jh_poor_image}{\textbf{JH Dataset Poor Image Examples}\\The subpar performance on the JH dataset is largely due to Joseph Hooker's ascending handwriting style, which often leads to line cropping and text overlap, as shown in Figure a. This can interfere with accurate text recognition. Moreover, Hooker's cursive handwriting, while visually appealing, is challenging to read due to unique letter formations. A few instances of printed text, rather than handwritten, are also present, shown in Figure b, contributing to the overall error rate.}{JH Dataset Poor Image Examples}

\subsection{Freezing Specific Layers}
\label{subsec:5_freezing_specific_layers}
For the GW dataset, results showed significant variation depending on the configuration of frozen components. The lowest WER and CER were observed when both the TrOCR encoder and decoder were frozen. This suggests that the model performs optimally when only the components connecting CharBERT and TrOCR, as well as the CharBERT components themselves, are trainable. Other configurations involving freezing combinations did not yield as favorable outcomes.

Conversely, the JH dataset exhibited less variability in response to component freezing. Although freezing both the TrOCR encoder and decoder also yielded marginally better results compared to other configurations, the improvements were not as pronounced as those observed in the GW dataset. This indicates a dataset-specific response to the freezing of model components, highlighting the need for tailored strategies depending on the characteristics of the data being processed.

This experience underscore a significant variation in the performance of the TrOCR model across two datasets, GW and JH, in response to different component freezing configurations. For the GW dataset, the best performance—indicated by the lowest WER and CER—was achieved when both the TrOCR encoder and decoder were frozen, suggesting optimal functionality when only the CharBERT-related components are active. In contrast, the JH dataset demonstrated less sensitivity to component freezing, with marginal improvements observed when both the encoder and decoder were frozen, compared to other configurations. These results highlight the importance of customizing model training strategies to accommodate the specific attributes and requirements of different datasets.

\tab{tab:5_res_freeze}{Freezing Specific Layers Results}{%
\begin{tabular}{ll|rrl}
    \toprule
    Dataset	& Freeze                                  	& WER	& CER	& File Name	\\
    \midrule
    GW		& None	                                	& 17.13	& 10.17	& gw\_none\\
    GW		& TrOCR encoder	                        	& 25.09	& 15.39	& gw\_encoder\\
    GW	    & TrOCR decoder	                            & 17.46	& 10.49	& gw\_decoder\\
    GW	    & TrOCR encoder, decoder	                & 12.84	& 5.88	& gw\_01\\
    \midrule
    JH		& None		                                & 36.06 & 22.11 & iam\_none\\
    JH		& TrOCR encoder		                        & 34.74 & 22.32 & iam\_encoder\\
    JH	    & TrOCR decoder                          	& 35.17 & 21.95 & iam\_decoder\\
    JH	    & TrOCR encoder, decoder	                & 35.63	& 21.90	& iam\_encoder\_decoder\\ 
    \bottomrule
    \end{tabular}
}{Freezing Specific Layers Results}

\paragraph*{Output Texts Analysis - TrOCR combined with CharBERT}
When TrOCR is combined with CharBERT, the enhanced model achieves more precise post-corrections. Unlike TrOCR alone, which may over-correct or erroneously complete words, this hybrid approach maintains the authenticity of the original images. For example, as discussed in \hyperref[subsec:5_baseline_models]{Subsection 5.3.1}, TrOCR incorrectly extends \say{gene} to \say{general} and correct \say{Expamples} to \say{Examples.} By integrating CharBERT, the model leverages both visual information and linguistic knowledge, enabling it to make more informed decisions about when to amend text and when to preserve the original input. This is particularly effective in the JH dataset, where the combined model correctly recognizes \say{THE CAMP.,} accurately handling printed letters without the capitalization errors seen with TrOCR alone.

\begin{center}
    \begin{minipage}{1.0\textwidth}
    \begin{Verbatim}[commandchars=\\\{\}]
        {\small\textcolor{Gray}{Test on GW Dataset with Encoder, Decoder Freezed}}
        label:  have a very good effect. The Commonalty in gene
        output: have a very good effect. The Commonalty in gene

        {\small\textcolor{Gray}{Test on GW Dataset with Encoder, Decoder Freezed}}
        label:  est occasion for Expamples, will be morally im
        output: est occasion for Expamples, will be morally im 

        {\small\textcolor{Gray}{Test on JH Dataset with Encoder, Decoder Freezed}}
        label:  THE CAMP.
        output: THE CAMP.  
    \end{Verbatim}
    \end{minipage}
\end{center}

\paragraph*{Output Texts Analysis - Effects on Freezing Different Layers}
Although the integration of TrOCR with CharBERT significantly enhances OCR and post-OCR correction tasks, these benefits are primarily observed when the encoder and decoder are frozen. Models with other component configurations perform suboptimally. They do not over-correct or complete sentences as TrOCR alone does, yet the accuracy of their output remains questionable, as demonstrated by the following examples:

\begin{center}
    \begin{minipage}{1.0\textwidth}
    \begin{Verbatim}[commandchars=\\\{\}]
        {\small\textcolor{Gray}{Test on GW Dataset with None Freezed}}
        label:  have a very good effect. The Commonalty in gene
        output: have a very good effect. The Commonwealthalty in get

        {\small\textcolor{Gray}{Test on GW Dataset with Encoder Freezed}}
        label:  have a very good effect. The Commonalty in gene
        output: have a very good effect. The Commally in g

        {\small\textcolor{Gray}{Test on JH Dataset with None Freezed}}
        label:  & education has extended to
        output: & observations has returned to
    \end{Verbatim}
    \end{minipage}
\end{center}

This discrepancy may stem from several factors:

\subparagraph*{Feature Stability}
Freezing the encoder and decoder in TrOCR maintains the stability of the features extracted from images. As a transformer-based OCR model, TrOCR is adept at effectively extracting text. By freezing these components, the pre-trained weights, which are proficient at capturing crucial image features, remain unaltered during additional training with CharBERT. This preservation is essential for accurate OCR.

\subparagraph*{Avoiding Catastrophic Forgetting}
In fine-tuning deep learning models for new tasks, there is a risk of catastrophic forgetting, where the model loses its previously learned knowledge—here, its OCR capabilities. Freezing the encoder and decoder helps prevent this loss.

\subparagraph*{Focus on Language Understanding}
With the OCR components frozen, CharBERT primarily enhances language understanding and error correction, based on the context it detects. CharBERT, designed to handle textual inaccuracies and contextual understanding, thus focuses on refining the text output.

\subparagraph*{Reducing Overfitting}
Allowing the TrOCR encoder and decoder to be trainable may lead to overfitting, where the model becomes excessively specialized to the training data nuances, compromising its ability to generalize. Freezing mitigates this risk.

\subparagraph*{Balanced Training Dynamics}
Activating both components could cause CharBERT’s gradient during backpropagation to negatively impact TrOCR’s feature extraction, degrading core OCR functionality. Freezing focuses optimization on linguistic corrections without undermining OCR capabilities.

\subsection{Integrating Dropout Mechanisms}
\label{subsec:5_integrating_dropout_mechanisms}
For both datasets, varying the dropout rates demonstrated minimal impact on the model's performance. However, the GW dataset exhibited a more pronounced response to changes in dropout rates compared to the JH dataset. This suggests that the influence of dropout is dataset-specific, highlighting the necessity for tailored dropout configurations that are carefully calibrated based on the characteristics of the dataset and the desired outcomes of the model. This approach ensures optimized performance while addressing the unique demands of each dataset.

\tab{tab:5_res_dropout}{Integrating Dropout Mechanisms Results}{
\begin{tabular}{ll|rrl}
    \toprule
    Dataset	& Dropout Rate	& WER	& CER	& File Name	\\
    \midrule
    GW		& -		        & 13.11	& 5.78	& gw\_00 \\
    GW		& 0.1	    	& 12.84 & 5.88  & gw\_01 \\
    GW		& 0.2	        & 12.88 & 6.26	& gw\_02 \\
    GW		& 0.5	        & 13.80 & 6.39	& gw\_05 \\
    \midrule
    JH		& -		        & 35.30 & 21.33 & iam\_00 \\
    JH		& 0.1		    & 35.63 & 21.90 & iam\_encoder\_decoder \\
    JH		& 0.2	        & 35.13 & 22.76 & iam\_02 \\
    JH		& 0.5	        & 35.30 & 22.77 & iam\_05 \\
    \bottomrule
    \end{tabular}
}{Integrating Dropout Mechanisms Results}

\paragraph*{Output Texts Analysis - Effects of Different Dropout Rates}
The impact of varying dropout rates on the output texts is evident, although it is challenging to determine which rate enhances language understanding the most. Each rate results in some correct and some incorrect recognitions, suggesting a nuanced effect on model performance. This observation may be attributed to the model's architecture. Both TrOCR and CharBERT, being based on transformer architecture and designed for error correction, respectively, likely exhibit a high degree of inherent robustness against overfitting due to their sophisticated designs and pre-trained weights. As dropout is primarily a tool to prevent overfitting, its contribution may be less significant if the model already generalizes well. Moreover, given that TrOCR and CharBERT are tasked with interpreting complex dependencies and contextual nuances, the random deactivation of neurons through dropout could disrupt more than assist. This disruption is particularly problematic in tasks requiring the interaction of all neurons to capture complex data patterns. Notably, lower dropout rates (such as 0 or 0.1) tend to perform better compared to higher rates (0.2 and 0.5), possibly due to these factors.

\subsection{Comparing Combined Tensor Modules}
\label{subsec:5_comparing_combined_tensor_modules}
For both datasets, \hyperref[subpar:3_linear_layers_as_attention_net]{Module 3} performs sligthly better than \hyperref[subpar:3_adding]{Module 1} and \hyperref[subpar:3_convolutional_layers_as_attention_net]{Module 4}, but significantly better than \hyperref[subpar:3_mean_pooling]{Module 2}. 

These results indicate that the effectiveness of specific tensor modules varies significantly between the two datasets, with certain configurations like linear layers as attention mechanisms consistently outperforming others such as mean pooling. This variability underscores the importance of selecting tensor modules based on dataset-specific characteristics to optimize model performance. The study clearly illustrates the need for meticulous configuration and testing of different components within OCR models to achieve optimal results.

\tab{tab:5_res_combine_tensor}{Comparing Combined Tensor Modules Results}{
\begin{tabular}{ll|rrl}
    \toprule
    Dataset	& Combined Tensor Modules                                                 & WER	   & CER   & File Name	\\
    \midrule
    GW		& \hyperref[subpar:3_adding]{Module 1}	                                  & 13.63  & 6.61  & gw\_c1 \\
    GW		& \hyperref[subpar:3_mean_pooling]{Module 2}	                          & 30.66  & 18.85 & gw\_c2 \\
    GW		& \hyperref[subpar:3_linear_layers_as_attention_net]{Module 3}	          & 12.84  & 5.88  & gw\_01 \\
    GW		& \hyperref[subpar:3_convolutional_layers_as_attention_net]{Module 4}	  & 13.31  & 6.29  & gw\_c4 \\
    \midrule
    JH		& \hyperref[subpar:3_adding]{Module 1}                                    & 34.52  & 20.61 & iam\_c1 \\
    JH		& \hyperref[subpar:3_mean_pooling]{Module 2}	                          & 44.99  & 30.14 & iam\_c2 \\
    JH		& \hyperref[subpar:3_linear_layers_as_attention_net]{Module 3}	          & 35.63  & 21.90 & iam\_encoder\_decoder \\
    JH		& \hyperref[subpar:3_convolutional_layers_as_attention_net]{Module 4}	  & 43.00  & 25.24 & iam\_c4 \\
    \bottomrule
\end{tabular}
}{Comparing Combined Tensor Modules Results}

\paragraph*{Output Texts Analysis - Effects of Different Combined Tensor Modules}
In the comparative analysis of tensor combine modules applied to the GW and JH datasets, it is evident that Module 2 consistently underperforms. This module, which utilizes mean pooling to dynamically allocate attention weights across the embedding dimensions of stacked tensors, simplifies the model by focusing on average features. However, this approach likely leads to an oversimplification of the representation, obscuring critical nuances and distinct handwriting traits that are essential for accurate character recognition. The inherent simplicity of mean pooling fails to capture the complex variations and subtleties in historical handwritten texts, resulting in suboptimal performance.

Conversely, Module 3, which employs linear layers for attention-based feature weighting, excels with the GW dataset. This module's sophisticated attention mechanism adaptively enhances and focuses on relevant features, which proves particularly effective for the GW dataset. The dataset, characterized by subtle variations in handwriting style from multiple authors, benefits from the nuanced differentiation that the linear layers provide, thereby improving both accuracy and robustness.

On the other hand, the JH dataset shows the best performance with Module 1, which uses a simple element-wise addition. This outcome can be attributed to the inherent variability and less consistent text layouts within the JH dataset. The straightforward additive approach of Module 1, though simpler, may inadvertently offer robustness by avoiding over-emphasis on any particular feature set, thus maintaining a general representation that accommodates the diverse handwriting styles found in the dataset.

Module 4, which replaces linear layers with convolutional layers, aims to exploit spatial correlations within the data. Although this strategy is typically effective for input types where spatial relationships are key, such as images, its benefits may not translate as effectively to tensor combinations derived from text embeddings.

This analysis highlights the critical importance of tailoring the tensor combining strategy to match the specific characteristics and challenges of each dataset. Module 3's adaptive capabilities are ideally suited to datasets like GW, which require nuanced differentiation, while the simpler, more robust approach of Module 1 is more appropriate for handling the varied and unpredictable features of the JH dataset. This strategic alignment is essential for optimizing model performance across different handwriting analysis tasks.

\subsection{Comparing Loss Functions}
\label{subsec:5_comparing_loss_functions}

\textcolor{red}{The table titled "Comparing Loss Functions Results" provides a detailed analysis of how three different loss functions impact the performance of OCR systems across two distinct datasets: the George Washington (GW) and Joseph Hooker (JH) datasets. For each dataset, the effectiveness of each loss function is evaluated based on the Word Error Rate (WER) and Character Error Rate (CER). Notably, Loss Function 1 appears to yield the lowest WER and CER in the GW dataset, suggesting it may be the most effective of the three for this particular set of data. However, the performance varies significantly in the JH dataset, where the error rates are generally higher, indicating a greater challenge in character recognition or possibly the complexity of the dataset itself. This comparison is crucial for understanding which loss functions might provide the best balance between word and character recognition accuracy in different OCR applications.}

\tab{tab:5_loss}{Comparing Loss Functions Results}{
\begin{tabular}{ll|rrl}
    \toprule
    Dataset	& Loss Function	    & WER	& CER	& File Name	\\
    \midrule
    GW		& \:\:1	            & 12.84	& 5.88  & gw\_01	\\
    GW		& \:\:2		        & 13.30 & 5.81  & gw\_loss2\\
    GW		& \:\:3	            & 14.19 & 6.85  & gw\_loss3\\
    \midrule
    JH		& \:\:1		        & 35.63 & 21.90 & iam\_encoder\_decoder \\
    JH		& \:\:2		        & 44.71 & 26.20	& iam\_loss2\\
    JH		& \:\:3		        & 40.72 & 27.92 & iam\_loss3\\
    \bottomrule
\end{tabular}
}{Comparing Loss Functions Results}

\subsection{Benchmarking Against GPT-4}
\label{subsec:5_benchmarking_against_gpt-4}

\tab{tab:5_gp4}{Benchmarking Against GPT-4 Results}{
\begin{tabular}{l|rr}
    \toprule
    Model	& WER	& CER		\\
    \midrule
    TrOCR-CharBERT		& 	& 	\\
    GPT-4 Post Correction	&  & 	\\
    GPT-4 Image Recognition		&  & 	\\
    \bottomrule
    \end{tabular}
}{Benchmarking Against GPT-4 Results}

\section{Model Cross-Dataset Performance Results}
\label{sec:5_model_adaptability_results}
The model's performance when trained on the George Washington (GW) dataset and tested on the Joseph Hooker (JH) dataset highlights a significant domain shift that the model struggles to manage effectively. The limited variability or the non-generalizable nature of the features learned from the GW dataset results in poor adaptability to the JH dataset. Conversely, the model trained on the JH dataset, although not excelling, performs better when tested on the GW dataset. These results suggest that the JH dataset likely offers a richer and more varied range of handwriting styles and complexities, making models trained on it more robust when applied to different datasets.

This inference aligns with earlier discussions in \autoref{subsec:5_comparing_combined_tensor_modules}, suggesting that the JH dataset's complexity and diversity might explain why Module 3 do not perform as well on it. In contrast, the GW dataset appears to be more homogeneous and specific in style, which hampers its generalization capabilities when confronted with the varied and complex data characteristics of the JH dataset. This analysis underscores the need for OCR models that can adapt across significantly diverse handwriting datasets and highlights the importance of incorporating a broad spectrum of data characteristics during the training phase to enhance model robustness and versatility.

\tab{tab:5_cross_dataset}{Model Cross-Dataset Performance Results}{
\begin{tabular}{ll|rrl}
    \toprule
    Training Dataset	& Testing Dataset	& WER	& CER	& File Name	\\
    \midrule
    GW		            & JH                & 96.96	& 82.64 & gw\_adapt	\\
    JH		            & GW 	            & 40.30 & 27.23 & iam\_adapt\\
    \bottomrule
\end{tabular}
}{Model Cross-Dataset Performance Results}

\section{Model Domain Adaptability Results}
\label{sec:5_model_domain_adaptability_results}

\tab{tab:5_adapt}{Model Domain Adaptability Results}{
    \begin{tabular}{lllrrl}
        \toprule
        Training Dataset & LM Training Data                                     & WER   & CER   & File Name  \\
        \midrule
        GW               & Contemporary English                                 & 13.88 & 6.51  & gw\_small  \\
        GW               & 15$^{\text{th}}$ -- 18$^{\text{th}}$ Century English &  &   & gw\_pij \\
        \midrule
        JH               & Contemporary English                                 &       &       & iam\_small \\
        JH               & 15$^{\text{th}}$ -- 18$^{\text{th}}$ Century English &       &       & iam\_pij \\
        \bottomrule
    \end{tabular}
}{Model Domain Adaptability Results}

\section{TrOCR-CharBERT$_{\mathcal{P}_{ij}}$ Results}
\label{sec:5_trocr_charbert_pij_results}

\tab{tab:5_res_pij}{TrOCR-CharBERT\ensuremath{_{\mathcal{P}_{ij}}} Results}{
\begin{tabular}{lllrrl}
    \toprule
    Training Dataset    & Model                                & $\mathcal{P}_{ij}$    & WER   & CER   & File Name  \\
    \midrule
    GW                  & TrOCR\_CharBERT$_{\text{SMALL}}$     & False                 & 13.88 & 6.51  & gw\_small  \\
    GW                  & TrOCR\_CharBERT$_{\mathcal{P}_{ij}}$ & True                  & 12.94 & 6.03  & gw\_pij \\
    \midrule
    JH                  & TrOCR\_CharBERT$_{\text{SMALL}}$     & False                 &       &       & iam\_small \\
    JH                  & TrOCR\_CharBERT$_{\mathcal{P}_{ij}}$ & True                  &       &       & iam\_pij \\
    \bottomrule
\end{tabular}
}{TrOCR-CharBERT\ensuremath{_{\mathcal{P}_{ij}}} Results}
\newchap{Experiment}
\label{chap:4_experiment}
Experiments are essential for answering research questions because they provide empirical evidence to support or disprove hypotheses. This chapter presents a detailed examination of the experimental setup designed to address the research questions and the training setups. First, we will introduce our baseline model for comparison purposes. Next, we will design and conduct an experiment for benchmarking, examining its outputs and performance to answer research questions 1 and 2. Following this, we will test on TrOCR-historical CharBERT$_{\text{SMALL}}$ and TrOCR-CharBERT$_{\mathcal{P}{ij}}$ to answer research questions 3 and 4, respectively.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                       Baseline Model                                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Baseline Model}
\label{sec:4_baseline_model}
In this study, we consider both the pre-trained and fine-tuned versions of the TrOCR model as baseline models for comparison. The rationale behind using the fine-tuned TrOCR model alongside its pre-trained counterpart is to ascertain the performance benchmark. If the fine-tuned TrOCR model outperform the composite model, it would suggest that further fine-tuning of the TrOCR model is a more efficient approach than employing the larger, more complex composite model. This comparison allows us to evaluate the efficiency of the fine-tuning process relative to the integration of additional model components.
\paragraph*{Pre-trained TrOCR}
\label{par:4_pre-trained_trocr}
We use the pre-trained \href{https://huggingface.co/microsoft/trocr-large-handwritten}{handwritten large TrOCR} as the baseline model. The model is trained on synthetic data and IAM handwritten dataset.
\paragraph*{Fine-tuned TrOCR}
\label{par:4_fine_tuned_trocr}
Another stronger baseline of ours is the fine-tuned TrOCR on George Washington (GW) and Joseph Hooker (JH) handwritten datasets. We further include this baseline model for the following reasons: 

Fine-tuning TrOCR on a specific dataset can enhance its performance significantly. Camparing the composite model with both the original and fine-tuned TrOCR allows us to assess the added value of integrating CharBERT. In other words, demonstrating that the composite model outperforms not just the original but also a fine-tuned version of TrOCR helps justify this added complexity. 

In addition, fine-tuning allows the model to adapt to the specific characteristics and distribution of your data, which might be significantly different from the data TrOCR was originally trained on. By evaluating against a fine-tuned baseline, we ensure that comparisons take into account the model's ability to handle data-specific challenges.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Model Training and Evaluation Criteria                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Training Setups}
\label{sec:4_model_training_process}
In this section, we delve into the training and evaluation process for the composite model. The section will cover key aspects such as the choice of optimizer, the loss function employed, and the hyperparameters set for the training. Additionally, the challenges commonly faced when training deep learning models, and insights into strategies and solutions to mitigate these issues will also be discussed.
\subsection{Training Details}
\label{subsec:4_training_details}
% optimiser, loss function, hyperparameters, utilisation of GPU resources
When training deep learning models, the selection of optimisation, loss computation techniques, and the hyperparameters settings play a crucial role in the effeiciency and effectiveness of the training process. In this study, we use Adam \citep{Kingma2014AdamAM} as the optimiser and cross-entropy for loss computation.

\paragraph*{Optimiser}
\label{par:4_Optimiser}
Utilizing Adam as the optimizer provides a sophisticated approach by adopting an adaptive learning rate, which is particularly effective at managing sparse gradients in tasks such as NLP problems. Besides, due to its efficient computation of adaptive learning rates, Adam often leads to faster convergence on training data. This can significantly reduce the time and computational resources needed to train deep models, making the process more efficient. Additionally, Adam is less likely to get stuck at local minima, further enhancing its effectiveness in training deep learning models.

For the training of the composite model, the learning rate has been set to $1\mathrm{e}{-5}$. This setting ensures a smooth convergence towards the local minima. Additionally, a weight decay parameter of $1\mathrm{e}{-5}$ is employed to enhance the model's ability to navigate the optimization landscape efficiently. This careful calibration prevents the optimizer from overshooting the local minima, thereby promoting stability in the training process and improving the model's overall performance.

\paragraph*{Loss Function}
\label{par:4_loss_function}
OCR tasks involve predicting the probability distribution of possible characters for a given input image. It is a multi-class classification problem, with each character representing a unique class. Cross-entropy loss is naturally suited for multi-class settings. This makes it directly applicable to the task of classifying images into characters. In addition, unlike other loss functions that might focus solely on the accuracy of the classification, cross-entropy loss encourages the model not just to predict the correct class but to do so with high confidence. High-confidence wrong predictions are penalized more, encouraging the model to be cautious about making predictions when unsure, which is often the case with less frequent characters.

In this study, we experiment with three combinations of loss functions. The first loss function is the loss between the final TrOCR decoder output and the TrOCR generated label. The illustration is shown in Figure \hyperref[fig:4_loss_1]{20a}. The second combination includes the first loss plus the loss from both the token and character channel CharBERT outputs compared to their labels. The second combination is depicted in \hyperref[fig:4_loss_2]{20b}. We observed that the sum of the losses from CharBERT is usually one-fourth of the TrOCR decoder output loss as shown in \Cref{fig:4_loss}. To balance the weights from TrOCR and CharBERT, we test a third combination, where we divide the TrOCR output loss by one-fourth.

\twofig{images/loss_1.png}{fig:4_loss_1}{0.7}{images/loss_2.png}{fig:4_loss_2}{1.0}{fig:4_loss_architec}{Figure a shows the architecture of Loss function 1, while Figure b shows the architecture of Loss function 2.}{Loss Function 1 and 2}

\fig{images/loss.png}{fig:4_loss}{The \say{TrOCR Loss} (blue line) represents the loss between the final TrOCR decoder output and the TrOCR generated label. The \say{TrOCR Loss/4} (cyan line) shows the TrOCR output loss divided by one-fourth to balance the weights with CharBERT. The \say{CharBERT Token Loss} (red line) and \say{CharBERT Character Loss} (orange line) represent the losses from the token and character channels of CharBERT, respectively. The plot demonstrates how the sum of the losses from CharBERT is generally one-fourth of the TrOCR decoder output loss, justifying the division of the TrOCR output loss by one-fourth for balanced weight contribution.}{14}{Comparison of Loss of CharBERT and TrOCR}

\subsection{Training Challenges and Solutions}
\label{subsec:4_training_challenges_and_solutions}
% overfitting (dropout), underfitting, learning rate, gradient vanishing/exploding, converge faster (norm), transfer learning
Training deep learning models involves navigating a series of common challenges that can significantly impact their performance and effectiveness. Common challenges include overfitting, vanishing/exploding gradients, high computational costs, and data quantity. The following discussion will delve into these challenges and the strategies used in this study to mitigate them.

\paragraph*{Overfitting}
\label{par:4_overfitting}
Overfitting occurs when a model learns the training data too well, capturing noise in the training set instead of learning the underlying patterns, which results in poor generalization to new, unseen data. Deep learning models, by their very nature, have a large number of parameters, allowing them to model complex patterns and relationships in the data. However, it also means they have the capacity to memorize irrelevant details in the training data, leading to overfitting. Lack of regularization techniques, or poorly chosen learning rate and batch size, can easily lead to overfitting.

To mitigate overfitting, we implemented dropout layers between both linear and convolutional layers within the composite model architecture. Dropout serves as a form of regularization that, by temporarily dropping out units from the network, prevents the model from becoming overly dependent on any single element of the training data, thereby enhancing its generalization capabilities.

Additionally, we explored the impact of batch size on model training. Smaller batch sizes result in more noise during the gradient updates, which can have a regularizing effect. However, very small batch sizes can lead to extremely noisy gradient estimates, which might make training unstable or lead to convergence on suboptimal solutions. Conversely, very large batch sizes can lead to smoother gradient updates but may also lead to overfitting as the model might capture too much of the training data's specifics. Through iterative testing, we determined that a batch size of 8 strikes an optimal balance, offering sufficient regularization to mitigate overfitting while maintaining stable and effective training dynamics.

\paragraph*{Vanishing/Exploding Gradients}
\label{par:4_vanishing_exploding_gradients}
Training deep models often encounters exploding or vanishing gradient problems due to their complex architectures and the long chains of computations involved. If the gradients are large (greater than 1), they can exponentially increase as they propagate back through the layers, leading to exploding gradients. Conversely, if the gradients are small (less than 1), they can exponentially decrease, leading to vanishing gradients.

Certain activation functions, like the sigmoid or tanh, squish a large input space into a small output range in a non-linear fashion. For inputs with large magnitudes, the gradients can be extremely small, leading to vanishing gradients. In addition, improper initialization of weights can worsen the exploding or vanishing gradient problems. For instance, large initial weights can lead to exploding gradients, while small initial weights can contribute to vanishing gradients.

To mitigate the vanishing/exploding gradients, we deploy strategies such as Xavier initialization \citep{glorot2010understanding}, Leaky ReLU activation function, gradient clipping, residual connections, and batch normalization in the composite model architecture. The details of how the residual connections and batch normalization are used in the composite model are discussed in \Cref{subsec:3_composite_model}.

\paragraph*{High Computational Costs}
\label{par:4_high_computational_costs}
Deep models, especially those with many layers and parameters, require significant computational resources and time to train. Hardware accelerators like GPUs can reduce training times. In this study, a single A100 GPU, equipped with 80GB of RAM, was utilized to accelerate the training process. When applied to the GW dataset and the JH dataset, the training durations were approximately 2 minutes per epoch and 30 minutes per epoch, respectively.

\paragraph*{Data Quantity}
\label{par:4_data_quantity}
Deep models typically require large datasets to effectively learn and generalize due to their complex architectures and the vast number of parameters they contain. Training these models from scratch on limited data often leads to overfitting. To mitigate this issue, we employ transfer learning. In this study, we take pre-trained CharBERT and TrOCR model, which are developed on large and comprehensive datasets, and adapt it to our specific task. 

\subsection{Evaluation}
\label{subsec:4_evaluation}
The validation set serves the purpose of hyperparameter tuning and model selection. Meanwhile, the testing set is reserved for the final evaluation. For the GW dataset, we employ a 4-fold cross-validation approach. This method divides the dataset into four equally sized segments, using each in turn for testing while the remaining three serve as the training set. The final results for the GW dataset represent the average performance across these four folds.

In this study, we focus on word error rate (WER) and character error rate (CER) as our primary evaluation metrics. These metrics are critical for assessing the model's accuracy in recognizing and reproducing text, providing the model's performance in terms of both word-level and character-level precision.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          Analysis                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Analysis}
\label{sec:4_composite_model_analysis}
In this section, we conduct an in-depth analysis of our composite model, aiming to understand their performance, robustness, and adaptability across various tasks and datasets. Through a series of component analyses and benchmarking exercises, we investigate the power of different model configurations and techniques, such as freezing specific layers, integrating dropout mechanisms, comparing tensor modules, and benchmarking against state-of-the-art models like GPT-4o. Additionally, we delve into methods for validating model adaptability, exploring strategies for testing the model's performance on diverse-domain datasets. Finally, we discuss approaches for assessing the impact of common OCR errors on the performance of the composite model. By examining the interplay between different components and techniques within composite models, we aim to provide comprehensive insights into their effectiveness and address the research questions.

\subsection{Component Analysis and Benchmarking}
\label{subsec:4_component_analysis_and_benchmarking}
In this section, we delve into various strategies aimed at enhancing the performance and robustness of our composite model. In this section, we delve into various strategies aimed at enhancing the performance and robustness of our composite model. This experiment addresses research question 1 by comparing the model outputs to the baseline model and research question 2 by examining the effect of incorporating character information into the model. These goals are achieved by assessing the model's performance with different combinations of components. To understand the contribution of specific components or features of a model to its overall performance, we conducted a comprehensive analysis. This analysis encompassed strategies such as 1) freezing specific layers; 2) integrating dropout mechanisms; 3) comparing combined tensor modules; 4) benchmarking against GPT-4o. Each of these approaches was carefully implemented to isolate and understand the impact of various model elements on its efficiency. By comparing different combinations of tensor modules and loss functions, we gain valuable insights into the architecture's effectiveness and identify optimal configurations. Moreover, benchmarking our models against industry standards like GPT-4o provides a comprehensive evaluation of their capabilities, shedding light on areas of improvement and innovation.

\paragraph*{Freezing Specific Layers}
\label{par:4_freezing_specific_layers}
When a layer is frozen, its weights remain unchanged during the training updates. This is often done to understand the contribution of specific layers to the model's learning and performance. By freezing specific layers and training the remaining layers, we can assess how changes in certain parts of the model affect its overall performance. This helps identify which layers are critical for learning the task at hand and which might be redundant or less influential.

\paragraph*{Integrating Dropout Mechanisms}
\label{par:4_integrating_dropout_mechanisms} 
Dropout is a regularization technique used to prevent overfitting in neural networks. By integrating dropout mechanisms into the model, we can analyze its effect on reducing overfitting and improving generalization. This approach helps promote robustness and adaptability in diverse data scenarios.

\paragraph*{Comparing Tensor Combine Modules}
\label{par:4_comparing_combined_tensor_modules}
This involves experimenting with \hyperref[par:3_tensor_combine]{different Tensor Combine Modules} within the model. By comparing how different combinations of these modules affect performance, we can understand how the architecture of the model contribute to its success or limitations.

\paragraph*{Comparing Loss Functions}
\label{par:4_comparing_loss_functions}
Given the involvement of two models in our study, we explored \hyperref[par:4_loss_function]{various combinations of loss functions} derived from the outputs of both models. This method not only allow us to assess the individual contributions of each model's output to the overall task, but also identify the most effective loss functions. 

\paragraph*{Benchmarking Against GPT-4o}
\label{par:4_benchmarking_against_gpt-4}
Benchmarking against a model like GPT-4o provides a high standard for performance and innovation. It allows us to examine how far our model has come in terms of understanding and generating text compared to leading models in the field. This comparison can highlight the strengths and weaknesses of our approach, offering insights into what components or features are contributing most to its performance. Although GPT-4o is not particularly good at correction or OCR tasks, comparing it to our model can reveal if having image information provides an advantage for the language model (LM). In other words, by comparing GPT-4o's correction of TrOCR outputs, we aim to determine if an end-to-end model truly benefits from incorporating image information.

\subsection{Validating Model Cross-Dataset Performance}
\label{subsec:5_validating_model_cross_dataset_performance}
In this subsection, we explore methods to validate the adaptability of our composite model. Through cross-training, we investigate how our model can effectively learn from and adapt to varied data characteristics, ultimately ensuring its reliability.

The primary purpose of this analysis is to determine how well the model can adapt to different datasets and to identify any potential limitations in its training. By testing the model on both the dataset it was trained on, it's possible to examine how effectively the model can transfer its learned patterns and predictions to new, unseen data.

This model evaluation strategy involves a cross-training and testing approach using two separate datasets, GW dataset and JH dataset. In this step, the model is first trained on GW dataset and then tested on the test set of JH dataset. This process is then repeated by training the model on JH dataset and similarly testing it on the GW dataset. This approach help assess both the model's dataset-specific performance and its generalization capabilities across varied data types.

This method provides a comprehensive understanding of the model's performance and robustness, offering insights into how training on one dataset might influence performance on another. This method also highlights the model's ability to generalize, which is critical for deploying machine learning models in environments where they will encounter varied types of data. Additionally, this strategy helps identify specific weaknesses or overfitting issues, guiding further refinements to enhance its accuracy and adaptability. Overall, this ensures that the model not only performs well in a controlled, homogeneous dataset environment but is also effective and reliable when faced with diverse and dynamic data conditions.

\subsection{Validating Model Domain Adaptability}
\label{subsec:5_validating_model_domain_adaptability}
The candidate fusion paper claims that the integration of a recognizer with an LM not only enhances recognition accuracy but also enables the LM to adapt to domain-specific data, especially in the time-domain. To examine whether a similar enhancement and adaptability can be achieved by integrating TrOCR with CharBERT, we embarked on an experimental setup involving two distinct versions of CharBERT: one trained on a dataset of historical English texts and the other on modern English texts, termed historical CharBERT$_{\text{SMALL}}$ and modern CharBERT$_{\text{SMALL}}$, respectively. These specialized models were then each combined with TrOCR to form composite systems designed to tackle the challenges of domain-specific text recognition. This analysis addresses research question 3.

Our primary objective is to evaluate how well these composite models perform when exposed to the GW dataset, which consists of historical documents with unique linguistic features that differ markedly from contemporary text forms. By assessing the performance of both TrOCR-historical CharBERT$_{\text{SMALL}}$ and TrOCR-modern CharBERT$_{\text{SMALL}}$ on this dataset, we aim to determine the extent of each model's ability to adapt to the domain-specific characteristics inherent in historical documents.

If both versions of the composite model—those incorporating historical and modern training—show comparable effectiveness on the GW dataset, it would support the adaptability claims made in the candidate fusion paper. Such an outcome could reduce the need for annotated OCR data, as the LM can be trained on specific time-domain language, and the combined recognizer and LM can then adapt to different time-domain images.

\subsection{TrOCR-CharBERT$_{\mathcal{P}_{ij}}$ Analysis}
\label{subsec:5_trocr_charbert_pij_analysis}
This analysis addresses research question 4. The primary objective of this analysis is to evaluate whether integrating common errors identified in TrOCR outputs into the training of CharBERT can improve the performance of the combined model. For this purpose, a new variant of CharBERT, referred to as CharBERT$_{\mathcal{P}{ij}}$, is trained specifically to address these identified inaccuracies.

To ensure a fair comparison of the enhanced capabilities of TrOCR-CharBERT$_{\mathcal{P}{ij}}$, it is inappropriate to compare it directly against a composite model using the pre-trained standard CharBERT, which is trained on a large amount of data. Instead, we compare TrOCR-CharBERT$_{\mathcal{P}{ij}}$ to TrOCR-CharBERT$_{\text{SMALL}}$. This comparison aims to isolate the impact of the targeted error correction training in CharBERT$_{\mathcal{P}_{ij}}$ and to demonstrate the potential improvements in OCR accuracy stemming from this training approach.
\newchap{Experiment}
\label{chap:4_experiment}
This chapter presents a detailed examination of the experimental setup and evaluation criteria in our study. Beginning with the data collection and processing section, we outline the sources and types of data utilised to benchmark the performance of our composite model and the training of CharBERT${_\text{SMALL}}$. This is followed by a discussion on the baseline model, where we compare both the pre-trained and fine-tuned versions of the TrOCR model to establish performance benchmarks. The subsequent sections delve into the training and evaluation criteria for the composite model and CharBERT${_\text{SMALL}}$. They highlights the strategic choices made in terms of optimizers, loss functions, and hyperparameters. In addition, we address common training challenges and our solutions. Finally, the analysis section explores the impact of specific model components and features on overall performance. This comprehensive approach not only enables a deeper understanding of the models' capabilities and limitations but also guides the refinement of OCR technologies for enhanced performance and efficiency.
\TODO{Rewrite this}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                       Baseline Model                                       %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Baseline Model}
\label{sec:4_baseline_model}
In this study, we consider both the pre-trained and fine-tuned versions of the TrOCR model as baseline models for comparison. The rationale behind using the fine-tuned TrOCR model alongside its pre-trained counterpart is to ascertain the performance benchmark. If the fine-tuned TrOCR model outperform the composite model, it would suggest that further fine-tuning of the TrOCR model is a more efficient approach than employing the larger, more complex composite model. This comparison allows us to evaluate the efficiency of the fine-tuning process relative to the integration of additional model components.
\paragraph*{Pre-trained TrOCR}
\label{par:4_pre-trained_trocr}
We use the pre-trained \href{https://huggingface.co/microsoft/trocr-large-handwritten}{handwritten large TrOCR} as the baseline model. The model is trained on synthetic data and IAM handwritten dataset, and evaluated on IAM handwritten dataset. To make fair comparisons across different models or approaches in their paper, the output text predicted by TrOCR is filtered to include only characters from the 36-character set, which consists of 26 lowercase letters (a-z) and 10 digits (0-9). 
\TODO{Mention generolisation}
\paragraph*{Fine-tuned TrOCR}
\label{par:4_fine_tuned_trocr}
Another stronger baseline of ours is the fine-tuned TrOCR on GW and IAM handwritten datasets. We further include this baseline model for the following reasons: 

First of all, fine-tuning TrOCR on a specific dataset can enhance its performance significantly. Camparing the composite model with both the original and fine-tuned TrOCR allows us to assess the added value of integrating CharBERT. In other words, demonstrating that the composite model outperforms not just the original but also a fine-tuned version of TrOCR helps justify this added complexity. 

In addition, including both the original and fine-tuned versions of TrOCR as baselines provides a more comprehensive view of how the composite model performs across different stages of model optimization. This comparison is crucial for understanding whether the improvements come from fine-tuning, integrating CharBERT, or both. 

Lastly, fine-tuning allows the model to adapt to the specific characteristics and distribution of your data, which might be significantly different from the data TrOCR was originally trained on. By evaluating against a fine-tuned baseline, we ensure that comparisons take into account the model's ability to handle data-specific challenges.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                           Model Training and Evaluation Criteria                           %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Training Process}
\label{sec:4_model_training_process}
In this section, we delve into the training and evaluation process for the composite model. The section will cover key aspects such as the choice of optimizer, the loss function employed, and the hyperparameters set for the training. Additionally, the challenges commonly faced when training deep learning models, and insights into strategies and solutions to mitigate these issues will also be discussed.
\subsection{Training Details}
\label{subsec:4_training_details}
% optimiser, loss function, hyperparameters, utilisation of GPU resources
When training deep learning models, the selection of optimisation, loss computation techniques, and the hyperparameters settings play a crucial role in the effeiciency and effectiveness of the training process. In this study, we use Adam as the optimiser and cross-entropy for loss computation.

\paragraph*{Optimiser}
\label{par:4_Optimiser}
Utilising Adam as the optimiser provides a sophisticated approach by adopting an adaptive learning rate, which is particularly adept at managing sparse gradients tasks such as NLP problems. Besides, due to its efficient computation of adaptive learning rates, Adam often leads to faster convergence on training data. This can significantly reduce the time and computational resources needed to train deep models, making the process more efficient.

For the training of the composite model, the learning rate has been meticulously set to $1\mathrm{e}{-5}$. This settingn ensures a controlled and gradual adaptation of the model parameters, facilitating a smooth convergence towards the local minima. Additionally, a weight decay parameter of $1\mathrm{e}{-5}$ is employed to enhance the model's ability to navigate the optimization landscape efficiently. This careful calibration prevents the optimizer from overshooting the local minima, thereby promoting stability in the training process and improving the model's overall performance.

\paragraph*{Loss Function}
\label{par:4_loss_function}
OCR tasks involves predicting the probability distribution of possible characters for a given input image. It is a multi-class classification problem, with each character representing a unique class. Cross-entropy loss is naturally suited for multi-class settings. This makes it directly applicable to the task of classifying images into characters. In addition, unlike other loss functions that might focus solely on the accuracy of the classification, cross-entropy loss encourages the model not just to predict the correct class but to do so with high confidence. High-confidence wrong predictions are penalised more, encouraging the model to be cautious about making predictions when unsure, which is often the case with less frequent characters.

\subsection{Training Challenges and Solutions}
\label{subsec:4_training_challenges_and_solutions}
% overfitting (dropout), underfitting, learning rate, gradient vanishing/exploding, converge faster (norm), transfer learning
Training deep learning models involves navigating a series of common challenges that can significantly impact their performance and effectiveness. Common challenges include overfitting, vanishing/exploding gradients, high computational costs, and data quantity. The following discussion will delve into these challenges and the strategies used in this study to mitigate them.

\paragraph*{Overfitting}
\label{par:4_overfitting}
Overfitting occurs when a model learns the training data too well, capturing noisein the training set instead of learning the underlying patterns, which results in poor generalisation to new, unseen data. Deep learning models, by their very nature, have a large number of parameters, allowing them to model intricate patterns and relationships in the data. However, it also means they have the capacity to memorise irrelevant details in the training data, leading to overfitting. Lack of regularisation techniques, or poorly chosen learning rate and batch size can easily lead to overfitting. 

To mitigate overfitting, we implemented dropout layers between both linear and convolutional layers within the composite model architecture. Dropout serves as a form of regularization that, by temporarily dropping out units from the network, prevents the model from becoming overly dependent on any single element of the training data, thereby enhancing its generalization capabilities.

Additionally, we explored the impact of batch size on model training. Smaller batch sizes result in more noise during the gradient updates, which can have a regularizing effect. However, very small batch sizes can lead to extremely noisy gradient estimates, which might make training unstable or lead to convergence on suboptimal solutions. Through iterative testing, we determined that a batch size of 8 strikes an optimal balance, offering sufficient regularization to mitigate overfitting while maintaining stable and effective training dynamics.

\paragraph*{Vanishing/Exploding Gradients}
\label{par:4_vanishing_exploding_gradients}
Training deep models often encounters exploding or vanishing gradient problems due to their complex architectures and the long chains of computations involved. If the gradients are large (greater than 1), they can exponentially increase as they propagate back through the layers, leading to exploding gradients. Conversely, if the gradients are small (less than 1), they can exponentially decrease, leading to vanishing gradients. 

Certain activation functions, like the sigmoid or tanh, squish a large input space into a small output range in a non-linear fashion. For inputs with large magnitudes, the gradients can be extremely small, leading to vanishing gradients. In addition, improper initialization of weights can exacerbate the exploding or vanishing gradient problems. For instance, large initial weights can lead to exploding gradients, while small initial weights can contribute to vanishing gradients.

To mitigate the vanishing/exploding gradients, we deploy strategies such as Xavier initialisation, Leaky ReLU activation function, gradient clipping, residual connection and batch normalization in the composite model architecture. The details of residual connection and batch normalisation\CHECK{Make sure they are mentioned} is discussed in \hyperref[subsec:3_composite_model]{Composite Model}.

\paragraph*{High Computational Costs}
\label{par:4_high_computational_costs}
Deep models, especially those with many layers and parameters, require significant computational resources and time to train. Hardware accelerators like GPUs can reduce training times. In this study, a single A100 GPU, equipped with 80GB of RAM, was utilized to accelerate the training process. When applied to the GW dataset and the IAM dataset, the training durations were approximately 2 minutes per epoch and 30 minutes per epoch, respectively.

\paragraph*{Data Quantity}
\label{par:4_data_quantity}
Deep models typically require large datasets to effectively learn and generalize due to their complex architectures and the vast number of parameters they contain. Training these models from scratch on limited data often leads to overfitting. To mitigate this issue, we employ transfer learning. In this study, we take pre-trained CharBERT and TrOCR model, which are developed on large and comprehensive datasets, and adapt it to our specific task. 

\subsection{Evaluation}
\label{subsec:4_evaluation}
The validation set serves the purpose of hyperparameter tuning and model selection. Meanwhile, the testing set is reserved for the final evaluation. For the GW dataset, we employ a 4-fold cross-validation approach. This method divides the dataset into four equally sized segments, using each in turn for testing while the remaining three serve as the training set. The final results for the GW dataset represent the average performance across these four folds.

In this study, we focus on word error rate (WER) and character error rate (CER) as our primary evaluation metrics. These metrics are critical for assessing the model's accuracy in recognizing and reproducing text, providing the model's performance in terms of both word-level and character-level precision.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                          CharBERT Training and Evaluation Criteria                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CharBERT$_{\text{SMALL}}$ Training Process}
\label{sec:4_charbert_training_process}
The training and evaluation of CharBERT$_{\text{SMALL}}$ closely follow the methodology outlined by the original authors. The model undergoes training using pre-training objectives, specifically MLM (Masked Language Modeling) and NLM (Next Sentence Prediction), across a large text corpus. Training entails backpropagation and weight optimization to minimize prediction errors for both MLM and NLM tasks.

While maintaining adherence to the \href{https://github.com/mawentao277/CharBERT/blob/main/shell/mlm.sh}{hyperparameters recommended by the original authors}, we made adjustments to the batch size, increasing it from the suggested size of 4 to 16, to expedite the computation process. The training process utilized five A100 GPUs, each equipped with 80GB of RAM, and spanned six days, comprising five training epochs.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          Analysis                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Composite Model Analysis}
\label{sec:4_composite_model_analysis}
In this section, we conduct an in-depth analysis of composite neural network models, aiming to understand their performance, robustness, and adaptability across various tasks and datasets. Through a series of component analyses and benchmarking exercises, we investigate the efficacy of different model configurations and techniques, such as freezing specific layers, integrating dropout mechanisms, comparing tensor modules, and benchmarking against state-of-the-art models like GPT-4. Additionally, we delve into methods for validating model generalization and adaptability, exploring strategies for testing the model's performance on diverse datasets and assessing its ability to transfer learned patterns to new, unseen data environments. By examining the interplay between different components and techniques within composite models, we aim to provide comprehensive insights into their effectiveness and suitability for real-world applications.

\subsection{Component Analysis and Benchmarking}
\label{subsec:4_component_analysis_and_benchmarking}
\TODO{Think a better name}
In this section, we delve into various strategies aimed at enhancing the performance and robustness of our neural network models. To understand the contribution of specific components or features of a model to its overall performance, we conducted a comprehensive analysis. This analysis encompassed strategies such as 1) freezing specific layers; 2) integrating dropout mechanisms; 3) comparing combined tensor modules; 4) benchmarking against GPT-4. Each of these approaches was carefully implemented to isolate and understand the impact of various model elements on its efficiency. By comparing different combinations of tensor modules and loss functions, we gain valuable insights into the architecture's effectiveness and identify optimal configurations. Moreover, benchmarking our models against industry standards like GPT-4 provides a comprehensive evaluation of their capabilities, shedding light on areas of improvement and innovation.

\paragraph*{Freezing Specific Layers}
\label{par:4_freezing_specific_layers}
When a layer is frozen, its weights remain unchanged during the training updates. This is often done to understand the contribution of specific layers to the model's learning and performance. By freezing specific layers and training the remaining layers, we can assess how changes in certain parts of the model affect its overall performance. This helps identify which layers are critical for learning the task at hand and which might be redundant or less influential.

\paragraph*{Integrating Dropout Mechanisms}
\label{par:4_integrating_dropout_mechanisms} 
Dropout is a regularization technique used to prevent overfitting in neural networks. By integrating dropout mechanisms into different parts of the model, we can analyze its effect on reducing overfitting and improving generalization. This helps to understand the importance of each component in achieving a balance between model complexity and its ability to perform well on unseen data.

\paragraph*{Comparing Combined Tensor Modules}
\label{par:4_comparing_combined_tensor_modules}
This involves experimenting with different tensor combination modules within the model. By comparing how different combinations of these modules affect performance, we can understand how the architecture and data flow within the model contribute to its success or limitations.

\paragraph*{Integrating Batch Normalization}
\label{par:4_integrating_batch_normalization}
Batch normalization is a technique used in deep learning to improve the stability and speed of training neural networks. It helps to stabilize the learning process by normalizing the inputs to each layer. This reduces the likelihood of vanishing or exploding gradients, which can occur when training deep neural networks, especially those with many layers. In addition, by normalizing the inputs, batch normalization allows for faster convergence during training. This means that the network can reach satisfactory performance levels with fewer training iterations, speeding up the overall training process. 

\paragraph*{Comparing Loss Functions}
\label{par:4_comparing_loss_functions}
Given the involvement of two models in our study, we explored various combinations of loss functions derived from the outputs of both models. This method not only allow us to assess the individual contributions of each model's output to the overall task, but also identify the most effective loss functions for the 

\paragraph*{Benchmarking Against GPT-4}
\label{par:4_benchmarking_against_gpt-4}
Benchmarking against a model like GPT-4 provides a high standard for performance and innovation. It allows us to gauge how far your model has come in terms of understanding and generating text compared to leading models in the field. This comparison can highlight the strengths and weaknesses of our approach, offering insights into what components or features are contributing most to its performance. 

\subsection{Validating Model Generalization and Adapability}
\label{subsec:5_validating_model_generalization_capability}
In this subsection, we explore methods to validate and enhance the robustness and adaptability of our machine learning model. By testing its generalization capabilities through exposure to diverse datasets and assessing its adaptability to new data environments, we aim to develop a model that performs reliably across a wide range of scenarios. Through a combination of dataset amalgamation and cross-training approaches, we investigate how our model can effectively learn from and adapt to varied data characteristics, ultimately ensuring its reliability and versatility in real-world applications.

\paragraph*{Testing Model Generalization}
\label{par:4_testing_model_generalization}
The purpose of this method is to enhance the model's generalization capabilities. By exposing the model to a broader array of data characteristics and complexities during the training phase, it is better equipped to handle new and varied types of data that it might encounter in real-world applications. This approach aims to reduce the model's potential biases towards any single dataset's idiosyncrasies and promotes a more robust performance across different environments.

To test the model generalization capability, we combine the training datasets from both GW dataset and JH dataset and use this amalgamated dataset to train the model. This method is implemented by merging all the data points from both datasets into a single training set, which is then utilized to develop a model that is expected to capture and learn from the diversity and range of features presented by both datasets.

The advantages of this step are manifold. Firstly, it increases the robustness of the model by training it across a diverse set of data, which helps mitigate overfitting to specific dataset features. Secondly, this strategy enhances the model’s adaptability, making it capable of performing well in a wider range of scenarios, thereby improving its practical applicability. Finally, combining datasets can provide a richer and more comprehensive set of data for the model to learn from, which can lead to the discovery of more generalized patterns and relationships within the data, contributing to better overall performance in tasks requiring high levels of data interpretation and analysis. This step is crucial for developing a versatile model that not only performs well in controlled test conditions but also thrives in diverse operational environments.

\paragraph*{Testing Model Adapability}
\label{par:4_testing_model_adapability}
The primary purpose of this method is to determine how well the model can adapt to different datasets and to identify any potential biases or limitations in its training. By testing the model on both the dataset it was trained on and an unrelated dataset, it's possible to gauge how effectively the model can transfer its learned patterns and predictions to new, unseen data, which is crucial for real-world applications.

This model evaluation strategy involves a cross-training and testing approach using two separate datasets, GW dataset and JH dataset. In this step, the model is first trained on GW dataset and then tested on both the test set of GW dataset and the test set of JH dataset. This process is then repeated by training the model on JH dataset and similarly testing it on the test sets of both datasets. This dual-training approach allows the model to be evaluated under two different learning conditions, which helps in assessing both its dataset-specific performance and its generalization capabilities across varied data types.

This method provides a comprehensive understanding of the model’s performance and robustness, offering insights into how training on one dataset might influence performance on another. This method also highlights the model's ability to generalize, which is critical for deploying machine learning models in environments where they will encounter varied types of data. Additionally, this strategy helps in fine-tuning the model by identifying specific weaknesses or overfitting issues, guiding further refinements to enhance its accuracy and adaptability. Overall, Step 2 ensures that the model not only performs well in a controlled, homogeneous dataset environment but is also effective and reliable when faced with diverse and dynamic data conditions.

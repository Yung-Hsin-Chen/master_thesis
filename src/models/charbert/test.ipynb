{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install nltk\n",
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling.modeling_charbert import CharBertModel\n",
    "from modeling.modeling_roberta import RobertaModel\n",
    "from run_lm_finetuning import load_and_cache_examples, TextDataset\n",
    "from modeling.configuration_bert import BertConfig\n",
    "from modeling.configuration_roberta import RobertaConfig\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, CharBertModel, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaModel, RobertaTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys = {\n",
    "#     \"output_debug\": False,\n",
    "#     \"adv_probability\": 0.1,\n",
    "#     \"model_type\": model_type,\n",
    "#     \"mlm_probability\": 0.1,\n",
    "#     \"block_size\": -1,\n",
    "#     \"char_maxlen_for_word\": 6,\n",
    "#     \"file_path\": \"./train_data.txt\",\n",
    "#     \"char_vocab\": \"./data/dict/roberta_char_vocab\",\n",
    "#     \"term_vocab\": \"./data/dict/term_vocab\",\n",
    "#     \"data_version\": \"1\",\n",
    "#     \"seed\": 42,\n",
    "#     \"input_nraws\": 10000\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    output_debug = False\n",
    "    adv_probability = 0.1\n",
    "    model_type = \"roberta\"\n",
    "    mlm_probability = 0.1\n",
    "    block_size = -1\n",
    "    char_maxlen_for_word = 6\n",
    "    char_vocab = \"./data/dict/roberta_char_vocab\"\n",
    "    term_vocab = \"./data/dict/term_vocab\"\n",
    "    data_version = \"1\"\n",
    "    seed = 42\n",
    "    input_nraws = 10000\n",
    "    config_name = \"roberta-large\"\n",
    "    model_name_or_path = \"roberta-large\"\n",
    "    tokenizer_name = \"roberta-large\"\n",
    "\n",
    "my_args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "     parser = argparse.ArgumentParser()\n",
    "\n",
    "     ## Required parameters\n",
    "     parser.add_argument(\"--train_data_file\", default=\"./train_data.txt\", type=str,\n",
    "                         help=\"The input training data file (a text file).\")\n",
    "     parser.add_argument(\"--output_dir\", default=\"./results/\", type=str,\n",
    "                         help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "\n",
    "     ## Other parameters\n",
    "     parser.add_argument(\"--eval_data_file\", default=None, type=str,\n",
    "                         help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
    "\n",
    "     parser.add_argument(\"--char_vocab\", default=\"./data/dict/bert_char_vocab\", type=str,\n",
    "                         help=\"char vocab for charBert\")\n",
    "     parser.add_argument(\"--term_vocab\", default=\"./data/dict/term_vocab\", type=str,\n",
    "                         help=\"term vocab for charBert\")\n",
    "\n",
    "     parser.add_argument(\"--model_type\", default=\"roberta\", type=str,\n",
    "                         help=\"The model architecture to be fine-tuned.\")\n",
    "     parser.add_argument(\"--model_name_or_path\", default=\"roberta-large\", type=str,\n",
    "                         help=\"The model checkpoint for weights initialization.\")\n",
    "\n",
    "     parser.add_argument(\"--mlm\", action='store_true',\n",
    "                         help=\"Train with masked-language modeling loss instead of language modeling.\")\n",
    "     parser.add_argument(\"--mlm_probability\", type=float, default=0.10,\n",
    "                         help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
    "     parser.add_argument(\"--adv_probability\", type=float, default=0.10,\n",
    "                         help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
    "\n",
    "     parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                         help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "     parser.add_argument(\"--data_version\", default=\"\", type=str,\n",
    "                         help=\"training data version for cached file\")\n",
    "     parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                         help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "     parser.add_argument(\"--cache_dir\", default=\"\", type=str,\n",
    "                         help=\"Optional directory to store the pre-trained models downloaded from s3 (instread of the default one)\")\n",
    "     parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "                         help=\"Optional input sequence length after tokenization.\"\n",
    "                              \"The training dataset will be truncated in block of this size for training.\"\n",
    "                              \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "     parser.add_argument(\"--do_train\", action='store_true',\n",
    "                         help=\"Whether to run training.\")\n",
    "     parser.add_argument(\"--do_eval\", action='store_true',\n",
    "                         help=\"Whether to run eval on the dev set.\")\n",
    "     parser.add_argument(\"--output_debug\", action='store_true',\n",
    "                         help=\"Whether to output the debug information.\")\n",
    "     parser.add_argument(\"--evaluate_during_training\", action='store_true',\n",
    "                         help=\"Run evaluation during training at each logging step.\")\n",
    "     parser.add_argument(\"--do_lower_case\", action='store_true',\n",
    "                         help=\"Set this flag if you are using an uncased model.\")\n",
    "     parser.add_argument(\"--char_maxlen_for_word\", default=6, type=int,\n",
    "                         help=\"Max number of char for each word.\")\n",
    "\n",
    "     parser.add_argument(\"--per_gpu_train_batch_size\", default=2, type=int,\n",
    "                         help=\"Batch size per GPU/CPU for training.\")\n",
    "     parser.add_argument(\"--per_gpu_eval_batch_size\", default=2, type=int,\n",
    "                         help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "     parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
    "                         help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
    "     parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                         help=\"The initial learning rate for Adam.\")\n",
    "     parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                         help=\"Weight decay if we apply some.\")\n",
    "     parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                         help=\"Epsilon for Adam optimizer.\")\n",
    "     parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                         help=\"Max gradient norm.\")\n",
    "     parser.add_argument(\"--num_train_epochs\", default=1.0, type=float,\n",
    "                         help=\"Total number of training epochs to perform.\")\n",
    "     parser.add_argument(\"--max_steps\", default=-1, type=int,\n",
    "                         help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
    "     parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
    "                         help=\"Linear warmup over warmup_steps.\")\n",
    "\n",
    "     parser.add_argument('--logging_steps', type=int, default=50,\n",
    "                         help=\"Log every X updates steps.\")\n",
    "     parser.add_argument('--save_steps', type=int, default=50,\n",
    "                         help=\"Save checkpoint every X updates steps.\")\n",
    "     parser.add_argument(\"--input_nraws\", default=10000, type=int,\n",
    "                         help=\"number of lines when read the input data each time.\")\n",
    "     parser.add_argument('--save_total_limit', type=int, default=None,\n",
    "                         help='Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default')\n",
    "     parser.add_argument(\"--eval_all_checkpoints\", action='store_true',\n",
    "                         help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\")\n",
    "     parser.add_argument(\"--no_cuda\", action='store_true',\n",
    "                         help=\"Avoid using CUDA when available\")\n",
    "     parser.add_argument('--overwrite_output_dir', action='store_true',\n",
    "                         help=\"Overwrite the content of the output directory\")\n",
    "     parser.add_argument('--overwrite_cache', action='store_true',\n",
    "                         help=\"Overwrite the cached training and evaluation sets\")\n",
    "     parser.add_argument('--seed', type=int, default=42,\n",
    "                         help=\"random seed for initialization\")\n",
    "\n",
    "     # parser.add_argument('--fp16', action='store_true',\n",
    "     #                     help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\")\n",
    "     # parser.add_argument('--fp16_opt_level', type=str, default='O1',\n",
    "     #                     help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "     #                          \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "     parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
    "                         help=\"For distributed training: local_rank\")\n",
    "     parser.add_argument('--server_ip', type=str, default='', help=\"For distant debugging.\")\n",
    "     parser.add_argument('--server_port', type=str, default='', help=\"For distant debugging.\")\n",
    "     args, unknown = parser.parse_known_args()\n",
    "\n",
    "     if args.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not args.mlm:\n",
    "          raise ValueError(\"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
    "                              \"flag (masked language modeling).\")\n",
    "     if args.eval_data_file is None and args.do_eval:\n",
    "          raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "                              \"or remove the --do_eval argument.\")\n",
    "\n",
    "     if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "          raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
    "\n",
    "     # Setup distant debugging if needed\n",
    "     if args.server_ip and args.server_port:\n",
    "          # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "          import ptvsd\n",
    "          print(\"Waiting for debugger attach\")\n",
    "          ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "          ptvsd.wait_for_attach()\n",
    "\n",
    "     # Setup CUDA, GPU & distributed training\n",
    "     \"\"\"\n",
    "     if args.local_rank == -1 or args.no_cuda:\n",
    "          device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "          args.n_gpu = torch.cuda.device_count()\n",
    "     else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "          torch.cuda.set_device(args.local_rank)\n",
    "          device = torch.device(\"cuda\", args.local_rank)\n",
    "          torch.distributed.init_process_group(backend='nccl')\n",
    "          args.n_gpu = 1\n",
    "     args.device = device\n",
    "     \"\"\"\n",
    "     device = torch.device(\"cpu\")\n",
    "     args.n_gpu = 0  # Since we're not using GPU\n",
    "     args.device = device\n",
    "\n",
    "     # Setup logging\n",
    "     # logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "     #                     datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "     #                     level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "     # logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "     #                     args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
    "\n",
    "     # Set seed\n",
    "     set_seed(args)\n",
    "\n",
    "     # Load pretrained model and tokenizer\n",
    "     if args.local_rank not in [-1, 0]:\n",
    "          torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "     config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "     config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n",
    "                                             cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "     tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n",
    "                                                  do_lower_case=args.do_lower_case,\n",
    "                                                  cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "     if args.block_size <= 0:\n",
    "          args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
    "     args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
    "     model = model_class.from_pretrained(args.model_name_or_path,\n",
    "                                             from_tf=bool('.ckpt' in args.model_name_or_path),\n",
    "                                             config=config,\n",
    "                                             cache_dir=args.cache_dir if args.cache_dir else None)\n",
    "     model.to(args.device)\n",
    "\n",
    "     if args.local_rank == 0:\n",
    "          torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "     # logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "     # Training\n",
    "     if args.do_train:\n",
    "          if args.local_rank not in [-1, 0]:\n",
    "               torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "          train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
    "\n",
    "          if args.local_rank == 0:\n",
    "               torch.distributed.barrier()\n",
    "\n",
    "          global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "          logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "\n",
    "     # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "     if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "          # Create output directory if needed\n",
    "          if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "               os.makedirs(args.output_dir)\n",
    "\n",
    "          logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "          # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "          # They can then be reloaded using `from_pretrained()`\n",
    "          model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "          model_to_save.save_pretrained(args.output_dir)\n",
    "          tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "          # Good practice: save your training arguments together with the trained model\n",
    "          torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "          # Load a trained model and vocabulary that you have fine-tuned\n",
    "          model = model_class.from_pretrained(args.output_dir)\n",
    "          tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "          model.to(args.device)\n",
    "\n",
    "\n",
    "     # Evaluation\n",
    "     results = {}\n",
    "     if args.do_eval and args.local_rank in [-1, 0]:\n",
    "          checkpoints = [args.output_dir]\n",
    "          if args.eval_all_checkpoints:\n",
    "               checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "               logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "          logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "          for checkpoint in checkpoints:\n",
    "               global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "               prefix = checkpoint.split('/')[-1] if checkpoint.find('checkpoint') != -1 else \"\"\n",
    "               \n",
    "               model = model_class.from_pretrained(checkpoint)\n",
    "               model.to(args.device)\n",
    "               result = evaluate(args, model, tokenizer, prefix=prefix)\n",
    "               result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "               results.update(result)\n",
    "\n",
    "     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm flag (masked language modeling).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 108\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m args, unknown \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmlm:\n\u001b[0;32m--> 108\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflag (masked language modeling).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39meval_data_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdo_eval:\n\u001b[1;32m    111\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor remove the --do_eval argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm flag (masked language modeling)."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#      parser = argparse.ArgumentParser()\n",
    "#      parser.add_argument(\"--model_type\", default=\"roberta\", type=str,\n",
    "#                          help=\"The model architecture to be fine-tuned.\")\n",
    "#      parser.add_argument(\"--model_name_or_path\", default=\"roberta-large\", type=str,\n",
    "#                          help=\"The model checkpoint for weights initialization.\")\n",
    "#      parser.add_argument(\"--config_name\", default=\"roberta-large\", type=str,\n",
    "#                          help=\"Optional pretrained config name or path if not the same as model_name_or_path\")\n",
    "#      parser.add_argument(\"--tokenizer_name\", default=\"roberta-large\", type=str,\n",
    "#                          help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path\")\n",
    "#      parser.add_argument(\"--output_debug\", default=False, action='store_true',\n",
    "#                          help=\"Whether to output the debug information.\")\n",
    "#      parser.add_argument(\"--adv_probability\", type=float, default=0.10,\n",
    "#                          help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
    "#      parser.add_argument(\"--mlm_probability\", type=float, default=0.10,\n",
    "#                          help=\"Ratio of tokens to mask for masked language modeling loss\")\n",
    "#      parser.add_argument(\"--block_size\", default=-1, type=int,\n",
    "#                          help=\"Optional input sequence length after tokenization.\"\n",
    "#                               \"The training dataset will be truncated in block of this size for training.\"\n",
    "#                               \"Default to the model max input length for single sentence inputs (take into account special tokens).\")\n",
    "#      parser.add_argument(\"--char_maxlen_for_word\", default=6, type=int,\n",
    "#                          help=\"Max number of char for each word.\")\n",
    "#      parser.add_argument(\"--char_vocab\", default=\"./data/dict/bert_char_vocab\", type=str,\n",
    "#                          help=\"char vocab for charBert\")\n",
    "#      parser.add_argument(\"--term_vocab\", default=\"./data/dict/term_vocab\", type=str,\n",
    "#                          help=\"term vocab for charBert\")\n",
    "#      parser.add_argument(\"--data_version\", default=\"\", type=str,\n",
    "#                          help=\"training data version for cached file\")\n",
    "#      parser.add_argument('--seed', type=int, default=42,\n",
    "#                          help=\"random seed for initialization\")\n",
    "#      parser.add_argument(\"--input_nraws\", default=10000, type=int,\n",
    "#                          help=\"number of lines when read the input data each time.\")\n",
    "#      parser.add_argument(\"--train_data_file\", default=\"./train_data.txt\", type=str,\n",
    "#                          help=\"The input training data file (a text file).\")\n",
    "#      parser.add_argument(\"--eval_data_file\", default=\"./train_data.txt\", type=str,\n",
    "#                          help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\")\n",
    "     \n",
    "#      args, unknown = parser.parse_known_args()\n",
    "\n",
    "#      config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "#      config = config_class.from_pretrained(args.config_name, cache_dir=None)\n",
    "#      model = model_class.from_pretrained(args.model_name_or_path,\n",
    "#                                         from_tf=False,\n",
    "#                                         config=config,\n",
    "#                                         cache_dir=None)\n",
    "\n",
    "#      tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name)\n",
    "#      train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
    "     \n",
    "#      return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls: <class 'modeling.configuration_roberta.RobertaConfig'>\n",
      "pretrained_model_name_or_path: roberta-large\n",
      "pretrained_model_name_or_path: roberta-large\n",
      "cls.pretrained_config_archive_map: {'roberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json', 'roberta-large': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json', 'roberta-large-mnli': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-config.json', 'distilroberta-base': 'https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-config.json', 'roberta-base-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-openai-detector-config.json', 'roberta-large-openai-detector': 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-openai-detector-config.json'}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, keys, evaluate=False):\n",
    "    dataset = TextDataset(tokenizer, **keys)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'output_debug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(tokenizer, keys, evaluate)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset\u001b[39m(tokenizer, keys, evaluate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'output_debug'"
     ]
    }
   ],
   "source": [
    "train_dataset = get_dataset(tokenizer, keys, evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = self.bert(char_input_ids,\n",
    "                            start_ids,\n",
    "                            end_ids,\n",
    "                            input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds,\n",
    "                            encoder_hidden_states=encoder_hidden_states,\n",
    "                            encoder_attention_mask=encoder_attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
